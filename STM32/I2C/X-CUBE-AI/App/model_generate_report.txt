Neural Network Tools for STM32 family v1.7.0 (stm.ai v8.1.0-19520)
Created date          : 2024-02-04 19:23:28
Parameters            : generate --name model -m C:/Project/ThermalCamera/h5/model.h5 --type keras --compression none --verbosity 1 --workspace C:\Users\TUKORE~1\AppData\Local\Temp\mxAI_workspace1691005497070027427999144256054 --output C:\Users\TUKorea707\.stm32cubemx\network_output --allocate-inputs --series stm32f7 -O ram --allocate-outputs

Exec/report summary (generate)
------------------------------------------------------------------------------------------------------------
model file         :   C:\Project\ThermalCamera\h5\model.h5                                                 
type               :   keras                                                                                
c_name             :   model                                                                                
compression        :   none                                                                                 
options            :   allocate-inputs, allocate-outputs                                                    
optimization       :   ram                                                                                  
target/series      :   stm32f7                                                                              
workspace dir      :   C:\Users\TUKORE~1\AppData\Local\Temp\mxAI_workspace1691005497070027427999144256054   
output dir         :   C:\Users\TUKorea707\.stm32cubemx\network_output                                      
model_fmt          :   float                                                                                
model_name         :   model                                                                                
model_hash         :   942d55d93af718ec6f2c019a46bf37f6                                                     
params #           :   51,861 items (202.58 KiB)                                                            
------------------------------------------------------------------------------------------------------------
input 1/1          :   'input_1' (domain:activations/**default**)                                           
                   :   768 items, 3.00 KiB, ai_float, float, (1,24,32,1)                                    
output 1/1         :   'number' (domain:activations/**default**)                                            
                   :   5 items, 20 B, ai_float, float, (1,1,1,5)                                            
macc               :   4,682,512                                                                            
weights (ro)       :   205,140 B (200.33 KiB) (1 segment) / -2,304(-1.1%) vs float model                    
activations (rw)   :   57,536 B (56.19 KiB) (1 segment) *                                                   
ram (total)        :   57,536 B (56.19 KiB) = 57,536 + 0 + 0                                                
------------------------------------------------------------------------------------------------------------
(*) 'input'/'output' buffers can be used from the activations buffer

Model name - model ['input_1'] ['number']
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
m_id   layer (type,original)                                     oshape                         param/size          macc               connected to   | c_size               c_macc                c_type                          
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
0      input_1 (Input, InputLayer)                               [b:None,h:24,h:24,w:32,c:1]                                                          |                                            
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
1      block_1_conv2d (Conv2D, Conv2D)                           [b:None,h:24,h:24,w:32,c:16]   160/640          110,608                    input_1   | -640(-100.0%)        -110,608(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
2      batch_normalization (ScaleBias, BatchNormalization)       [b:None,h:24,h:24,w:32,c:16]   32/128            24,576             block_1_conv2d   | +512(+400.0%)        +86,032(+350.1%)      conv2d_of32[0]                  
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
3      activation (Nonlinearity, Activation)                     [b:None,h:24,h:24,w:32,c:16]                     12,288        batch_normalization   |                                            nl_of32[1]                      
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
5      block_1_pad (Pad, ZeroPadding2D)                          [b:None,h:25,h:25,w:33,c:16]                                            activation   | +18,560(+100.0%)     +884,768(+100.0%)     conv2d_of32[2]                  
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
6      block_1_2_conv2d (Conv2D, Conv2D)                         [b:None,h:12,h:12,w:16,c:32]   4,640/18,560     884,768                block_1_pad   | -18,560(-100.0%)     -884,768(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
7      batch_normalization_1 (ScaleBias, BatchNormalization)     [b:None,h:12,h:12,w:16,c:32]   64/256            12,288           block_1_2_conv2d   | -256(-100.0%)        -12,288(-100.0%)      
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
8      block_2_conv2d (Conv2D, Conv2D)                           [b:None,h:12,h:12,w:16,c:16]   4,624/18,496     884,752      batch_normalization_1   | -18,496(-100.0%)     -884,752(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
9      batch_normalization_2 (ScaleBias, BatchNormalization)     [b:None,h:12,h:12,w:16,c:16]   32/128             6,144             block_2_conv2d   | +18,368(+14350.0%)   +878,608(+14300.3%)   conv2d_of32[3]                  
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
10     activation_1 (Nonlinearity, Activation)                   [b:None,h:12,h:12,w:16,c:16]                      3,072      batch_normalization_2   |                                            nl_of32[4]                      
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
11     block_2_2_conv2d (Conv2D, Conv2D)                         [b:None,h:12,h:12,w:16,c:32]   4,640/18,560     884,768               activation_1   | -18,560(-100.0%)     -884,768(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
13     batch_normalization_3 (ScaleBias, BatchNormalization)     [b:None,h:12,h:12,w:16,c:32]   64/256            12,288           block_2_2_conv2d   | +18,304(+7150.0%)    +872,480(+7100.3%)    conv2d_of32[5]                  
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
14     block_2_output (Eltwise, Add)                             [b:None,h:12,h:12,w:16,c:32]                      6,144      batch_normalization_1   |                                            eltwise/sum_of32[6]             
                                                                                                                              batch_normalization_3   | 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
15     block_3_conv2d (Conv2D, Conv2D)                           [b:None,h:12,h:12,w:16,c:16]   4,624/18,496     884,752             block_2_output   | -18,496(-100.0%)     -884,752(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
16     batch_normalization_4 (ScaleBias, BatchNormalization)     [b:None,h:12,h:12,w:16,c:16]   32/128             6,144             block_3_conv2d   | +18,368(+14350.0%)   +878,608(+14300.3%)   conv2d_of32[7]                  
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
17     activation_2 (Nonlinearity, Activation)                   [b:None,h:12,h:12,w:16,c:16]                      3,072      batch_normalization_4   |                                            nl_of32[8]                      
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
19     block_3_pad (Pad, ZeroPadding2D)                          [b:None,h:13,h:13,w:17,c:16]                                          activation_2   | +18,560(+100.0%)     +221,216(+100.0%)     conv2d_of32[9]                  
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
20     block_3_2_conv2d (Conv2D, Conv2D)                         [b:None,h:6,h:6,w:8,c:32]      4,640/18,560     221,216                block_3_pad   | -18,560(-100.0%)     -221,216(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
21     batch_normalization_5 (ScaleBias, BatchNormalization)     [b:None,h:6,h:6,w:8,c:32]      64/256             3,072           block_3_2_conv2d   | -256(-100.0%)        -3,072(-100.0%)       
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
22     block_4_conv2d (Conv2D, Conv2D)                           [b:None,h:6,h:6,w:8,c:16]      4,624/18,496     221,200      batch_normalization_5   | -18,496(-100.0%)     -221,200(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
23     batch_normalization_6 (ScaleBias, BatchNormalization)     [b:None,h:6,h:6,w:8,c:16]      32/128             1,536             block_4_conv2d   | +18,368(+14350.0%)   +219,664(+14301.0%)   conv2d_of32[10]                 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
24     activation_3 (Nonlinearity, Activation)                   [b:None,h:6,h:6,w:8,c:16]                           768      batch_normalization_6   |                                            nl_of32[11]                     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
25     block_4_2_conv2d (Conv2D, Conv2D)                         [b:None,h:6,h:6,w:8,c:32]      4,640/18,560     221,216               activation_3   | -18,560(-100.0%)     -221,216(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
27     batch_normalization_7 (ScaleBias, BatchNormalization)     [b:None,h:6,h:6,w:8,c:32]      64/256             3,072           block_4_2_conv2d   | +18,304(+7150.0%)    +218,144(+7101.0%)    conv2d_of32[12]                 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
28     block_4_output (Eltwise, Add)                             [b:None,h:6,h:6,w:8,c:32]                         1,536      batch_normalization_5   |                                            eltwise/sum_of32[13]            
                                                                                                                              batch_normalization_7   | 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
29     block_5_conv2d (Conv2D, Conv2D)                           [b:None,h:6,h:6,w:8,c:16]      4,624/18,496     221,200             block_4_output   | -18,496(-100.0%)     -221,200(-100.0%)     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
30     batch_normalization_8 (ScaleBias, BatchNormalization)     [b:None,h:6,h:6,w:8,c:16]      32/128             1,536             block_5_conv2d   | +18,368(+14350.0%)   +219,664(+14301.0%)   conv2d_of32[14]                 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
31     activation_4 (Nonlinearity, Activation)                   [b:None,h:6,h:6,w:8,c:16]                           768      batch_normalization_8   |                                            nl_of32[15]                     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
33     block_5_pad (Pad, ZeroPadding2D)                          [b:None,h:7,h:7,w:9,c:16]                                             activation_4   | +18,560(+100.0%)     +55,328(+100.0%)      conv2d_of32[16]                 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
34     block_5_2_conv2d (Conv2D, Conv2D)                         [b:None,h:3,h:3,w:4,c:32]      4,640/18,560      55,328                block_5_pad   | -18,560(-100.0%)     -55,328(-100.0%)      
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
35     batch_normalization_9 (ScaleBias, BatchNormalization)     [b:None,h:3,h:3,w:4,c:32]      64/256               768           block_5_2_conv2d   | -256(-100.0%)        -768(-100.0%)         
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
36     block_6_conv2d (Conv2D, Conv2D)                           [b:None,h:3,h:3,w:4,c:16]      4,624/18,496      55,312      batch_normalization_9   | -18,496(-100.0%)     -55,312(-100.0%)      
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
37     batch_normalization_10 (ScaleBias, BatchNormalization)    [b:None,h:3,h:3,w:4,c:16]      32/128               384             block_6_conv2d   | +18,368(+14350.0%)   +54,928(+14304.2%)    conv2d_of32[17]                 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
38     activation_5 (Nonlinearity, Activation)                   [b:None,h:3,h:3,w:4,c:16]                           192     batch_normalization_10   |                                            nl_of32[18]                     
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
40     block_6_pad (Pad, ZeroPadding2D)                          [b:None,h:4,h:4,w:5,c:16]                                             activation_5   | +18,560(+100.0%)     +9,248(+100.0%)       conv2d_of32[19]                 
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
41     block_6_2_conv2d (Conv2D, Conv2D)                         [b:None,h:1,h:1,w:2,c:32]      4,640/18,560       9,248                block_6_pad   | -18,560(-100.0%)     -9,248(-100.0%)       
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
42     batch_normalization_11 (ScaleBias, BatchNormalization)    [b:None,h:1,h:1,w:2,c:32]      64/256               128           block_6_2_conv2d   | -256(-100.0%)        -128(-100.0%)         
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
43     global_average_pooling2d (Pool, GlobalAveragePooling2D)   [b:None,h:1,h:1,w:1,c:32]                            64     batch_normalization_11   |                                            pool_of32[20]                   
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
44     number_output_dense (Dense, Dense)                        [b:None,h:1,h:1,w:1,c:5]       165/660              165   global_average_pooling2d   |                      +75(+45.5%)           dense_of32/nl_of32[o][21, 22]   
       number_output (Nonlinearity, Dense)                       [b:None,h:1,h:1,w:1,c:5]                             75        number_output_dense   |                      -75(-100.0%)          
------ --------------------------------------------------------- ------------------------------ -------------- --------- -------------------------- --- -------------------- --------------------- ------------------------------- 
model/c-model: macc=4,754,448/4,682,512 -71,936(-1.5%) weights=207,444/205,140 -2,304(-1.1%) activations=--/57,536 io=--/0



Generated C-graph summary
------------------------------------------------------------------------------------------------------------------------
model name            : model
c-name                : model
c-node #              : 23
c-array #             : 50
activations size      : 57536 (1 segment)
weights size          : 205140 (1 segment)
macc                  : 4682512
inputs                : ['input_1_output']
outputs               : ['number_output_output']

C-Arrays (50)
------ --------------------------------- ------------- ------------------------- ------------- --------- --------- 
c_id   name (*_array)                    item/size     domain/mem-pool           c-type        fmt       comment   
------ --------------------------------- ------------- ------------------------- ------------- --------- --------- 
0      activation_1_output               3072/12288    activations/**default**   float         float32             
1      block_2_2_conv2d_output           6144/24576    activations/**default**   float         float32             
2      block_2_output_output             6144/24576    activations/**default**   float         float32             
3      block_3_conv2d_output             3072/12288    activations/**default**   float         float32             
4      activation_2_output               3072/12288    activations/**default**   float         float32             
5      block_3_2_conv2d_output           1536/6144     activations/**default**   float         float32             
6      block_4_conv2d_output             768/3072      activations/**default**   float         float32             
7      activation_3_output               768/3072      activations/**default**   float         float32             
8      block_4_2_conv2d_output           1536/6144     activations/**default**   float         float32             
9      block_4_output_output             1536/6144     activations/**default**   float         float32             
10     block_5_conv2d_output             768/3072      activations/**default**   float         float32             
11     activation_4_output               768/3072      activations/**default**   float         float32             
12     block_5_2_conv2d_output           384/1536      activations/**default**   float         float32             
13     block_6_conv2d_output             192/768       activations/**default**   float         float32             
14     activation_5_output               192/768       activations/**default**   float         float32             
15     block_6_2_conv2d_output           64/256        activations/**default**   float         float32             
16     global_average_pooling2d_output   32/128        activations/**default**   float         float32             
17     number_output_dense_output        5/20          activations/**default**   float         float32             
18     number_output_output              5/20          activations/**default**   float         float32   /output   
19     block_1_conv2d_weights            144/576       weights/weights           const float   float32             
20     block_1_conv2d_bias               16/64         weights/weights           const float   float32             
21     block_1_2_conv2d_weights          4608/18432    weights/weights           const float   float32             
22     block_1_2_conv2d_bias             32/128        weights/weights           const float   float32             
23     block_2_conv2d_weights            4608/18432    weights/weights           const float   float32             
24     block_2_conv2d_bias               16/64         weights/weights           const float   float32             
25     block_2_2_conv2d_weights          4608/18432    weights/weights           const float   float32             
26     block_2_2_conv2d_bias             32/128        weights/weights           const float   float32             
27     block_3_conv2d_weights            4608/18432    weights/weights           const float   float32             
28     block_3_conv2d_bias               16/64         weights/weights           const float   float32             
29     block_3_2_conv2d_weights          4608/18432    weights/weights           const float   float32             
30     block_3_2_conv2d_bias             32/128        weights/weights           const float   float32             
31     block_4_conv2d_weights            4608/18432    weights/weights           const float   float32             
32     block_4_conv2d_bias               16/64         weights/weights           const float   float32             
33     block_4_2_conv2d_weights          4608/18432    weights/weights           const float   float32             
34     block_4_2_conv2d_bias             32/128        weights/weights           const float   float32             
35     block_5_conv2d_weights            4608/18432    weights/weights           const float   float32             
36     block_5_conv2d_bias               16/64         weights/weights           const float   float32             
37     block_5_2_conv2d_weights          4608/18432    weights/weights           const float   float32             
38     block_5_2_conv2d_bias             32/128        weights/weights           const float   float32             
39     block_6_conv2d_weights            4608/18432    weights/weights           const float   float32             
40     block_6_conv2d_bias               16/64         weights/weights           const float   float32             
41     block_6_2_conv2d_weights          4608/18432    weights/weights           const float   float32             
42     block_6_2_conv2d_bias             32/128        weights/weights           const float   float32             
43     number_output_dense_weights       160/640       weights/weights           const float   float32             
44     number_output_dense_bias          5/20          weights/weights           const float   float32             
45     input_1_output                    768/3072      activations/**default**   float         float32   /input    
46     block_1_conv2d_output             12288/49152   activations/**default**   float         float32             
47     activation_output                 12288/49152   activations/**default**   float         float32             
48     block_1_2_conv2d_output           6144/24576    activations/**default**   float         float32             
49     block_2_conv2d_output             3072/12288    activations/**default**   float         float32             
------ --------------------------------- ------------- ------------------------- ------------- --------- --------- 

C-Layers (23)
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
c_id   name (*_layer)             id   layer_type     macc     rom     tensors                              shape (array id)    
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
0      block_1_conv2d             2    conv2d         110608   640     I: input_1_output                    (1,24,32,1) (45)    
                                                                       W: block_1_conv2d_weights            (1,3,3,16) (19)     
                                                                       W: block_1_conv2d_bias               (16,) (20)          
                                                                       O: block_1_conv2d_output             (1,24,32,16) (46)   
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
1      activation                 3    nl             12288    0       I: block_1_conv2d_output             (1,24,32,16) (46)   
                                                                       O: activation_output                 (1,24,32,16) (47)   
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
2      block_1_2_conv2d           5    conv2d         884768   18560   I: activation_output                 (1,24,32,16) (47)   
                                                                       W: block_1_2_conv2d_weights          (16,3,3,32) (21)    
                                                                       W: block_1_2_conv2d_bias             (32,) (22)          
                                                                       O: block_1_2_conv2d_output           (1,12,16,32) (48)   
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
3      block_2_conv2d             9    conv2d         884752   18496   I: block_1_2_conv2d_output           (1,12,16,32) (48)   
                                                                       W: block_2_conv2d_weights            (32,3,3,16) (23)    
                                                                       W: block_2_conv2d_bias               (16,) (24)          
                                                                       O: block_2_conv2d_output             (1,12,16,16) (49)   
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
4      activation_1               10   nl             3072     0       I: block_2_conv2d_output             (1,12,16,16) (49)   
                                                                       O: activation_1_output               (1,12,16,16) (0)    
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
5      block_2_2_conv2d           13   conv2d         884768   18560   I: activation_1_output               (1,12,16,16) (0)    
                                                                       W: block_2_2_conv2d_weights          (16,3,3,32) (25)    
                                                                       W: block_2_2_conv2d_bias             (32,) (26)          
                                                                       O: block_2_2_conv2d_output           (1,12,16,32) (1)    
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
6      block_2_output             14   eltwise/sum    6144     0       I: block_1_2_conv2d_output           (1,12,16,32) (48)   
                                                                       I: block_2_2_conv2d_output           (1,12,16,32) (1)    
                                                                       O: block_2_output_output             (1,12,16,32) (2)    
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
7      block_3_conv2d             16   conv2d         884752   18496   I: block_2_output_output             (1,12,16,32) (2)    
                                                                       W: block_3_conv2d_weights            (32,3,3,16) (27)    
                                                                       W: block_3_conv2d_bias               (16,) (28)          
                                                                       O: block_3_conv2d_output             (1,12,16,16) (3)    
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
8      activation_2               17   nl             3072     0       I: block_3_conv2d_output             (1,12,16,16) (3)    
                                                                       O: activation_2_output               (1,12,16,16) (4)    
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
9      block_3_2_conv2d           19   conv2d         221216   18560   I: activation_2_output               (1,12,16,16) (4)    
                                                                       W: block_3_2_conv2d_weights          (16,3,3,32) (29)    
                                                                       W: block_3_2_conv2d_bias             (32,) (30)          
                                                                       O: block_3_2_conv2d_output           (1,6,8,32) (5)      
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
10     block_4_conv2d             23   conv2d         221200   18496   I: block_3_2_conv2d_output           (1,6,8,32) (5)      
                                                                       W: block_4_conv2d_weights            (32,3,3,16) (31)    
                                                                       W: block_4_conv2d_bias               (16,) (32)          
                                                                       O: block_4_conv2d_output             (1,6,8,16) (6)      
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
11     activation_3               24   nl             768      0       I: block_4_conv2d_output             (1,6,8,16) (6)      
                                                                       O: activation_3_output               (1,6,8,16) (7)      
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
12     block_4_2_conv2d           27   conv2d         221216   18560   I: activation_3_output               (1,6,8,16) (7)      
                                                                       W: block_4_2_conv2d_weights          (16,3,3,32) (33)    
                                                                       W: block_4_2_conv2d_bias             (32,) (34)          
                                                                       O: block_4_2_conv2d_output           (1,6,8,32) (8)      
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
13     block_4_output             28   eltwise/sum    1536     0       I: block_3_2_conv2d_output           (1,6,8,32) (5)      
                                                                       I: block_4_2_conv2d_output           (1,6,8,32) (8)      
                                                                       O: block_4_output_output             (1,6,8,32) (9)      
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
14     block_5_conv2d             30   conv2d         221200   18496   I: block_4_output_output             (1,6,8,32) (9)      
                                                                       W: block_5_conv2d_weights            (32,3,3,16) (35)    
                                                                       W: block_5_conv2d_bias               (16,) (36)          
                                                                       O: block_5_conv2d_output             (1,6,8,16) (10)     
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
15     activation_4               31   nl             768      0       I: block_5_conv2d_output             (1,6,8,16) (10)     
                                                                       O: activation_4_output               (1,6,8,16) (11)     
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
16     block_5_2_conv2d           33   conv2d         55328    18560   I: activation_4_output               (1,6,8,16) (11)     
                                                                       W: block_5_2_conv2d_weights          (16,3,3,32) (37)    
                                                                       W: block_5_2_conv2d_bias             (32,) (38)          
                                                                       O: block_5_2_conv2d_output           (1,3,4,32) (12)     
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
17     block_6_conv2d             37   conv2d         55312    18496   I: block_5_2_conv2d_output           (1,3,4,32) (12)     
                                                                       W: block_6_conv2d_weights            (32,3,3,16) (39)    
                                                                       W: block_6_conv2d_bias               (16,) (40)          
                                                                       O: block_6_conv2d_output             (1,3,4,16) (13)     
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
18     activation_5               38   nl             192      0       I: block_6_conv2d_output             (1,3,4,16) (13)     
                                                                       O: activation_5_output               (1,3,4,16) (14)     
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
19     block_6_2_conv2d           40   conv2d         9248     18560   I: activation_5_output               (1,3,4,16) (14)     
                                                                       W: block_6_2_conv2d_weights          (16,3,3,32) (41)    
                                                                       W: block_6_2_conv2d_bias             (32,) (42)          
                                                                       O: block_6_2_conv2d_output           (1,1,2,32) (15)     
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
20     global_average_pooling2d   43   pool           64       0       I: block_6_2_conv2d_output           (1,1,2,32) (15)     
                                                                       O: global_average_pooling2d_output   (1,1,1,32) (16)     
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
21     number_output_dense        44   dense          165      660     I: global_average_pooling2d_output   (1,1,1,32) (16)     
                                                                       W: number_output_dense_weights       (32,5) (43)         
                                                                       W: number_output_dense_bias          (5,) (44)           
                                                                       O: number_output_dense_output        (1,1,1,5) (17)      
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 
22     number_output              44   nl             75       0       I: number_output_dense_output        (1,1,1,5) (17)      
                                                                       O: number_output_output              (1,1,1,5) (18)      
------ -------------------------- ---- -------------- -------- ------- ------------------------------------ ------------------- 



Number of operations per c-layer
------- ------ --------------------------------- ----------- -------------- -------- ---------- 
c_id    m_id   name (type)                               #op           type   #param   sparsity 
------- ------ --------------------------------- ----------- -------------- -------- ---------- 
0       2      block_1_conv2d (conv2d)               110,608   smul_f32_f32      160     0.0000 
1       3      activation (nl)                        12,288     op_f32_f32          
2       5      block_1_2_conv2d (conv2d)             884,768   smul_f32_f32    4,640     0.0000 
3       9      block_2_conv2d (conv2d)               884,752   smul_f32_f32    4,624     0.0000 
4       10     activation_1 (nl)                       3,072     op_f32_f32          
5       13     block_2_2_conv2d (conv2d)             884,768   smul_f32_f32    4,640     0.0000 
6       14     block_2_output (eltwise/sum)            6,144     op_f32_f32          
7       16     block_3_conv2d (conv2d)               884,752   smul_f32_f32    4,624     0.0000 
8       17     activation_2 (nl)                       3,072     op_f32_f32          
9       19     block_3_2_conv2d (conv2d)             221,216   smul_f32_f32    4,640     0.0000 
10      23     block_4_conv2d (conv2d)               221,200   smul_f32_f32    4,624     0.0000 
11      24     activation_3 (nl)                         768     op_f32_f32          
12      27     block_4_2_conv2d (conv2d)             221,216   smul_f32_f32    4,640     0.0000 
13      28     block_4_output (eltwise/sum)            1,536     op_f32_f32          
14      30     block_5_conv2d (conv2d)               221,200   smul_f32_f32    4,624     0.0000 
15      31     activation_4 (nl)                         768     op_f32_f32          
16      33     block_5_2_conv2d (conv2d)              55,328   smul_f32_f32    4,640     0.0000 
17      37     block_6_conv2d (conv2d)                55,312   smul_f32_f32    4,624     0.0000 
18      38     activation_5 (nl)                         192     op_f32_f32          
19      40     block_6_2_conv2d (conv2d)               9,248   smul_f32_f32    4,640     0.0000 
20      43     global_average_pooling2d (pool)            64     op_f32_f32          
21      44     number_output_dense (dense)               165   smul_f32_f32      165     0.0000 
22      44     number_output (nl)                         75     op_f32_f32          
------- ------ --------------------------------- ----------- -------------- -------- ---------- 
total                                              4,682,512                  51,285     0.0000 

Number of operation types
---------------- ----------- ----------- 
operation type             #           % 
---------------- ----------- ----------- 
smul_f32_f32       4,654,533       99.4% 
op_f32_f32            27,979        0.6% 

Complexity report (model)
------ -------------------------- ------------------------- ------------------------- ---------- 
m_id   name                       c_macc                    c_rom                     c_id       
------ -------------------------- ------------------------- ------------------------- ---------- 
2      batch_normalization        ||                 2.4%   |                  0.3%   [0]        
3      activation                 |                  0.3%   |                  0.0%   [1]        
5      block_1_pad                ||||||||||||||||  18.9%   ||||||||||||||||   9.0%   [2]        
9      batch_normalization_2      |||||||||||||||   18.9%   |||||||||||||||    9.0%   [3]        
10     activation_1               |                  0.1%   |                  0.0%   [4]        
13     batch_normalization_3      ||||||||||||||||  18.9%   ||||||||||||||||   9.0%   [5]        
14     block_2_output             |                  0.1%   |                  0.0%   [6]        
16     batch_normalization_4      |||||||||||||||   18.9%   |||||||||||||||    9.0%   [7]        
17     activation_2               |                  0.1%   |                  0.0%   [8]        
19     block_3_pad                ||||               4.7%   ||||||||||||||||   9.0%   [9]        
23     batch_normalization_6      ||||               4.7%   |||||||||||||||    9.0%   [10]       
24     activation_3               |                  0.0%   |                  0.0%   [11]       
27     batch_normalization_7      ||||               4.7%   ||||||||||||||||   9.0%   [12]       
28     block_4_output             |                  0.0%   |                  0.0%   [13]       
30     batch_normalization_8      ||||               4.7%   |||||||||||||||    9.0%   [14]       
31     activation_4               |                  0.0%   |                  0.0%   [15]       
33     block_5_pad                |                  1.2%   ||||||||||||||||   9.0%   [16]       
37     batch_normalization_10     |                  1.2%   |||||||||||||||    9.0%   [17]       
38     activation_5               |                  0.0%   |                  0.0%   [18]       
40     block_6_pad                |                  0.2%   ||||||||||||||||   9.0%   [19]       
43     global_average_pooling2d   |                  0.0%   |                  0.0%   [20]       
44     number_output_dense        |                  0.0%   |                  0.3%   [21, 22]   
------ -------------------------- ------------------------- ------------------------- ---------- 
macc=4,682,512 weights=205,140 act=57,536 ram_io=0

Generated files (7)
--------------------------------------------------------------------- 
C:\Users\TUKorea707\.stm32cubemx\network_output\model_config.h        
C:\Users\TUKorea707\.stm32cubemx\network_output\model.h               
C:\Users\TUKorea707\.stm32cubemx\network_output\model.c               
C:\Users\TUKorea707\.stm32cubemx\network_output\model_data_params.h   
C:\Users\TUKorea707\.stm32cubemx\network_output\model_data_params.c   
C:\Users\TUKorea707\.stm32cubemx\network_output\model_data.h          
C:\Users\TUKorea707\.stm32cubemx\network_output\model_data.c          
