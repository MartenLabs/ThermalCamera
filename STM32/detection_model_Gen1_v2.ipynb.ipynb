{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "from IPython.display import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "DIR = 'main_150epochs_16'\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "RES_HEIGHT = 24\n",
    "RES_WIDTH = 32\n",
    "NUM_CLASS = 5\n",
    "N_BATCH =8\n",
    "N_EPOCH = 150\n",
    "LR = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_origin = np.load('npz/dataset1_origin.npz', allow_pickle=True)\n",
    "dataset1_horizon = np.load('npz/dataset1_horizon.npz', allow_pickle=True)\n",
    "dataset1_vertical = np.load('npz/dataset1_vertical.npz', allow_pickle=True)\n",
    "dataset1_vh = np.load('npz/dataset1_vh.npz', allow_pickle=True)\n",
    "dataset2_origin = np.load('npz/dataset2_origin.npz', allow_pickle=True)\n",
    "dataset2_horizon = np.load('npz/dataset2_horizon.npz', allow_pickle=True)\n",
    "dataset2_vertical = np.load('npz/dataset2_vertical.npz', allow_pickle=True)\n",
    "dataset2_vh = np.load('npz/dataset2_vh.npz', allow_pickle=True)\n",
    "\n",
    "d1o_origin_images, d1o_target_images, d1o_number_labels, d1o_coordinates = dataset1_origin['images'], dataset1_origin['filters'], dataset1_origin['numbers'],  dataset1_origin['coordinates']\n",
    "d1h_origin_images, d1h_target_images, d1h_number_labels, d1h_coordinates = dataset1_horizon['images'], dataset1_horizon['filters'], dataset1_horizon['numbers'],  dataset1_horizon['coordinates']\n",
    "d1v_origin_images, d1v_target_images, d1v_number_labels, d1v_coordinates = dataset1_vertical['images'], dataset1_vertical['filters'], dataset1_vertical['numbers'],  dataset1_vertical['coordinates']\n",
    "d1vh_origin_images, d1vh_target_images, d1vh_number_labels, d1vh_coordinates = dataset1_vh['images'], dataset1_vh['filters'], dataset1_vh['numbers'],  dataset1_vh['coordinates']\n",
    "d2o_origin_images, d2o_target_images, d2o_number_labels, d2o_coordinates = dataset2_origin['images'], dataset2_origin['filters'], dataset2_origin['numbers'],  dataset2_origin['coordinates']\n",
    "d2h_origin_images, d2h_target_images, d2h_number_labels, d2h_coordinates = dataset2_horizon['images'], dataset2_horizon['filters'], dataset2_horizon['numbers'],  dataset2_horizon['coordinates']\n",
    "d2v_origin_images, d2v_target_images, d2v_number_labels, d2v_coordinates = dataset2_vertical['images'], dataset2_vertical['filters'], dataset2_vertical['numbers'],  dataset2_vertical['coordinates']\n",
    "d2vh_origin_images, d2vh_target_images, d2vh_number_labels, d2vh_coordinates = dataset2_vh['images'], dataset2_vh['filters'], dataset2_vh['numbers'],  dataset2_vh['coordinates']\n",
    "\n",
    "origin_images = np.concatenate([d1o_origin_images, d2o_origin_images, d1h_origin_images, d2h_origin_images, d1v_origin_images, d2v_origin_images, d1vh_origin_images, d2vh_origin_images], axis = 0)\n",
    "target_images = np.concatenate([d1o_target_images, d2o_target_images, d1h_target_images, d2h_target_images, d1v_target_images, d2v_target_images, d1vh_target_images, d2vh_target_images], axis = 0)\n",
    "numbers_labels = np.concatenate([d1o_number_labels, d2o_number_labels, d1h_number_labels, d2h_number_labels, d1v_number_labels, d2v_number_labels, d1vh_number_labels, d2vh_number_labels], axis = 0)\n",
    "coordinates = np.concatenate([d1o_coordinates, d2o_coordinates, d1h_coordinates, d2h_coordinates, d1v_coordinates, d2v_coordinates, d1vh_coordinates, d2vh_coordinates], axis = 0)\n",
    "\n",
    "print(origin_images.shape)\n",
    "print(target_images.shape)\n",
    "print(numbers_labels.shape)\n",
    "print(coordinates.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_images = origin_images.reshape(13276, 24, 32, 1)\n",
    "target_images = target_images.reshape(13276, 24, 32, 1)\n",
    "print(origin_images.shape)\n",
    "print(target_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(origin_images.max(), origin_images.min())\n",
    "print(target_images.max(), target_images.min())\n",
    "\n",
    "origin_images = origin_images / origin_images.max()\n",
    "target_images = target_images / target_images.max()\n",
    "\n",
    "print(origin_images.max(), origin_images.min())\n",
    "print(target_images.max(), target_images.min())\n",
    "\n",
    "print(origin_images.shape)\n",
    "print(target_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    origin_images, numbers_labels, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(image, label):\n",
    "    label = tf.one_hot(label, depth=NUM_CLASS)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 13:51:30.742407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22292 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1d:00.0, compute capability: 8.6\n",
      "2024-01-06 13:51:30.744155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22292 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1f:00.0, compute capability: 8.6\n",
      "2024-01-06 13:51:30.745649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22292 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:20:00.0, compute capability: 8.6\n",
      "2024-01-06 13:51:30.747119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22292 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n",
      "2024-01-06 13:51:30.748591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 22292 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:22:00.0, compute capability: 8.6\n",
      "2024-01-06 13:51:30.750022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 22292 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:23:00.0, compute capability: 8.6\n",
      "2024-01-06 13:51:30.751507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 22292 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:24:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(one_hot_encode)\n",
    "val_ds = val_ds.map(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(buffer_size=len(train_images)).batch(N_BATCH).prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.batch(N_BATCH).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_ds.take(5):\n",
    "    plt.imshow(images[0].numpy())\n",
    "    plt.title(str(labels[0].numpy().argmax()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InputLayer \t\t\t(None, 24, 32, 1)\n",
    "\n",
    "UpSampling2D\t\t(None, 72, 96, 1)\n",
    "\n",
    "----------------------block 1---------------------------\n",
    "\n",
    "Conv2D \t\t\t\t(None, 72, 96, 3)\n",
    "\n",
    "BatchNormalization\t(None, 72, 96, 3)\n",
    "\n",
    "ELU\t\t\t\t\t(None, 72, 96, 3)\n",
    "\n",
    "ZeroPadding2D\t\t(None, 73, 97, 3)\n",
    "\n",
    "Conv2D\t\t\t\t(None, 36, 48, 1)\n",
    "\n",
    "BatchNormalization\t(None, 36, 48, 1) \t\tblock_1_project_BN\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "----------------------block 2---------------------------\n",
    "\n",
    "Conv2D\t\t\t\t(None, 36, 48, 3)\n",
    "\n",
    "BatchNormalization\t(None, 36, 48, 3)\n",
    "\n",
    "ELU\t\t\t\t\t(None, 36, 48, 3)\n",
    "\n",
    "Conv2D\t\t\t\t(None, 36, 48, 1)\n",
    "\n",
    "BatchNormalization\t(None, 36, 48, 1)\t\tblock_2_project_BN\n",
    "\n",
    "Add\t(OutPut-1)\t\t(None, 36, 48, 1)\t\tblock_1_project_BN + block_2_project_BN \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "----------------------block 3---------------------------\n",
    "\n",
    "Conv2D\t\t\t\t(None, 36, 48, 3)\n",
    "\n",
    "BatchNormalization\t(None, 36, 48, 3)\n",
    "\n",
    "ELU\t\t\t\t\t(None, 36, 48, 3)\n",
    "\n",
    "ZeroPadding2D\t\t(None, 37, 49, 3)\n",
    "\n",
    "Conv2D\t\t\t\t(None, 18, 24, 1)\n",
    "\n",
    "BatchNormalization\t(None, 18, 24, 1)\t\tblock_3_project_BN\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "----------------------block 4---------------------------\n",
    "\n",
    "Conv2D\t\t\t\t(None, 18, 24, 3)\n",
    "\n",
    "BatchNormalization\t(None, 18, 24, 3)\n",
    "\n",
    "ELU\t\t\t\t\t(None, 18, 24, 3)\n",
    "\n",
    "Conv2D\t\t\t\t(None, 18, 24, 1)\n",
    "\n",
    "BatchNormalization\t(None, 18, 24, 1)\t\tblock_4_project_BN\n",
    "\n",
    "Add\t(OutPut-2)\t\t(None, 18, 24, 1)\t\tblock_3_project_BN + block_4_project_BN \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "----------------------block 5---------------------------\n",
    "\n",
    "Conv2D\t\t\t\t(None, 18, 24, 3)\n",
    "\n",
    "BatchNormalization\t(None, 18, 24, 3)\n",
    "\n",
    "ELU\t\t\t\t\t(None, 18, 24, 3)\n",
    "\n",
    "ZeroPadding2D\t\t(None, 19, 25, 3)\n",
    "\n",
    "Conv2D\t\t\t\t(None, 9, 12, 1)\n",
    "\n",
    "BatchNormalization\t(None, 9, 12, 1)\t\tblock_5_project_BN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "from IPython.display import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Dense, Input, MaxPool2D, Conv2D, Conv2DTranspose, Flatten, Reshape, Activation\n",
    "from keras.layers import BatchNormalization, Dropout, Activation, concatenate, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import MultiHeadAttention\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, Conv2D, UpSampling2D, GlobalAveragePooling2D\n",
    "\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.regularizers import l2\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.layers import Conv2D, MaxPooling2D, concatenate, Add\n",
    "\n",
    "\n",
    "class BackBone:\n",
    "    def __init__(self):\n",
    "        self.l2_regularizer = l2(0.001)\n",
    "\n",
    "    def residual_layer(self, feature_map, latent, name:str):\n",
    "        add_layer = Add(name = name+'_output')([feature_map, latent])\n",
    "        return add_layer\n",
    "\n",
    "    def feature_extraction_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        feature_map = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name)(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        feature_map = Activation('relu')(feature_map)\n",
    "        feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        feature_map = ZeroPadding2D(padding=((0, 1), (0, 1)), name=name+'_pad')(feature_map)\n",
    "        feature_map = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 2, padding = 'valid', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        return feature_map\n",
    "\n",
    "    def convolutional_residual_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        latent = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name)(feature_map)\n",
    "        latent =  BatchNormalization()(latent)\n",
    "        latent = Activation('relu')(latent)\n",
    "        feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        latent = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(latent)\n",
    "        latent = BatchNormalization()(latent)\n",
    "        residual_block = self.residual_layer(feature_map, latent, name)\n",
    "        return residual_block\n",
    "\n",
    "\n",
    "    def __call__(self, input_shape=(24, 32, 1)):\n",
    "        inputs_image = Input(shape=input_shape)\n",
    "        upsample_layer = UpSampling2D(size=(3, 3))(inputs_image)\n",
    "        block_1 = self.feature_extraction_block(upsample_layer, 3, 5,'block_1')\n",
    "        block_1_output = self.convolutional_residual_block(block_1, 3, 5,'block_2')\n",
    "        block_2 = self.feature_extraction_block(block_1_output, 3, 5, 'block_3')\n",
    "        block_2_output = self.convolutional_residual_block(block_2, 3, 5,'block_4')\n",
    "        block_3_output = self.feature_extraction_block(block_2_output, 3, 3,'block_5')\n",
    "\n",
    "        # latent = GlobalAveragePooling2D()(block_3_output)\n",
    "        # latent = keras.layers.GlobalMaxPooling2D()(block_3_output)\n",
    "        latent = Flatten()(block_3_output)\n",
    "        # latent = Dense(64, activation='ReLU', dtype='float32')(latent)\n",
    "        # latent = Dropout(0.2)(latent)\n",
    "        latent = Dense(16, activation='ReLU', dtype='float32')(latent)\n",
    "        latent = Dropout(0.2)(latent)\n",
    "        number_output = Dense(5, activation='softmax', name='number_output', dtype='float32')(latent)\n",
    "\n",
    "        model = Model(inputs_image, number_output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 24, 32, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 72, 96, 1)            0         ['input_1[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block_1 (Conv2D)            (None, 72, 96, 3)            30        ['up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 72, 96, 3)            12        ['block_1[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 72, 96, 3)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 72, 96, 3)            0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D  (None, 73, 97, 3)            0         ['dropout[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_2 (Conv2D)          (None, 36, 48, 5)            140       ['block_1_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 36, 48, 5)            20        ['block_1_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_2 (Conv2D)            (None, 36, 48, 3)            138       ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 36, 48, 3)            12        ['block_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 36, 48, 3)            0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_2_2 (Conv2D)          (None, 36, 48, 5)            140       ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 36, 48, 5)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 36, 48, 5)            20        ['block_2_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_2_output (Add)        (None, 36, 48, 5)            0         ['dropout_1[0][0]',           \n",
      "                                                                     'batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_3 (Conv2D)            (None, 36, 48, 3)            138       ['block_2_output[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 36, 48, 3)            12        ['block_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 36, 48, 3)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 36, 48, 3)            0         ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D  (None, 37, 49, 3)            0         ['dropout_2[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_3_2 (Conv2D)          (None, 18, 24, 5)            140       ['block_3_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 18, 24, 5)            20        ['block_3_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_4 (Conv2D)            (None, 18, 24, 3)            138       ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 18, 24, 3)            12        ['block_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 18, 24, 3)            0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_4_2 (Conv2D)          (None, 18, 24, 5)            140       ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 18, 24, 5)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 18, 24, 5)            20        ['block_4_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_4_output (Add)        (None, 18, 24, 5)            0         ['dropout_3[0][0]',           \n",
      "                                                                     'batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_5 (Conv2D)            (None, 18, 24, 3)            138       ['block_4_output[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 18, 24, 3)            12        ['block_5[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 18, 24, 3)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 18, 24, 3)            0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " block_5_pad (ZeroPadding2D  (None, 19, 25, 3)            0         ['dropout_4[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_5_2 (Conv2D)          (None, 9, 12, 3)             84        ['block_5_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 9, 12, 3)             12        ['block_5_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 324)                  0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 16)                   5200      ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 16)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " number_output (Dense)       (None, 5)                    85        ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6663 (26.03 KB)\n",
      "Trainable params: 6587 (25.73 KB)\n",
      "Non-trainable params: 76 (304.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "backbone = BackBone()\n",
    "model = backbone()\n",
    "\n",
    "initial_learning_rate = LR\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1)\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "#     backbone = BackBone()\n",
    "#     model = backbone()\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "#     model.compile(optimizer=optimizer, loss=['categorical_crossentropy'], metrics=['accuracy', Precision(), Recall()])\n",
    "#     checkpoint = ModelCheckpoint('v4_backbone_1_best_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=optimizer, loss=['categorical_crossentropy'], metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "checkpoint = ModelCheckpoint('v4_backbone_1_best_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "\n",
    "# plot_model(model, to_file='./detection_model.png', show_shapes=True, show_layer_names=True)\n",
    "# Image(filename='detection_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (108610972.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    aa!!\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "aa!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = DIR\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 13:51:38.766041: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-01-06 13:51:40.227982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-06 13:51:40.988221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-01-06 13:51:40.990983: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7efba6a437a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-06 13:51:40.991002: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-06 13:51:40.991008: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-06 13:51:40.991011: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-06 13:51:40.991014: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-06 13:51:40.991017: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-06 13:51:40.991021: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-06 13:51:40.991024: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-06 13:51:40.994693: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-06 13:51:41.125803: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162/1162 [==============================] - ETA: 0s - loss: 1.3168 - accuracy: 0.4886 - precision: 0.5809 - recall: 0.3137\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64675, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 30s 19ms/step - loss: 1.3168 - accuracy: 0.4886 - precision: 0.5809 - recall: 0.3137 - val_loss: 0.9272 - val_accuracy: 0.6467 - val_precision: 0.7232 - val_recall: 0.5162\n",
      "Epoch 2/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.9677 - accuracy: 0.6110 - precision: 0.6724 - recall: 0.4908\n",
      "Epoch 2: val_accuracy improved from 0.64675 to 0.69395, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 22s 19ms/step - loss: 0.9682 - accuracy: 0.6109 - precision: 0.6724 - recall: 0.4909 - val_loss: 0.7680 - val_accuracy: 0.6939 - val_precision: 0.7460 - val_recall: 0.5958\n",
      "Epoch 3/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.8544 - accuracy: 0.6447 - precision: 0.6968 - recall: 0.5514\n",
      "Epoch 3: val_accuracy improved from 0.69395 to 0.73563, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.8542 - accuracy: 0.6446 - precision: 0.6967 - recall: 0.5514 - val_loss: 0.6692 - val_accuracy: 0.7356 - val_precision: 0.7724 - val_recall: 0.6525\n",
      "Epoch 4/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.7759 - accuracy: 0.6842 - precision: 0.7238 - recall: 0.6078\n",
      "Epoch 4: val_accuracy improved from 0.73563 to 0.76299, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.7761 - accuracy: 0.6841 - precision: 0.7236 - recall: 0.6077 - val_loss: 0.6099 - val_accuracy: 0.7630 - val_precision: 0.7843 - val_recall: 0.7103\n",
      "Epoch 5/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.7176 - accuracy: 0.7039 - precision: 0.7361 - recall: 0.6491\n",
      "Epoch 5: val_accuracy did not improve from 0.76299\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.7172 - accuracy: 0.7043 - precision: 0.7364 - recall: 0.6493 - val_loss: 0.6159 - val_accuracy: 0.7494 - val_precision: 0.7681 - val_recall: 0.7052\n",
      "Epoch 6/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.6761 - accuracy: 0.7181 - precision: 0.7452 - recall: 0.6717\n",
      "Epoch 6: val_accuracy did not improve from 0.76299\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.6765 - accuracy: 0.7180 - precision: 0.7451 - recall: 0.6715 - val_loss: 0.5785 - val_accuracy: 0.7630 - val_precision: 0.7816 - val_recall: 0.7359\n",
      "Epoch 7/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.6563 - accuracy: 0.7305 - precision: 0.7574 - recall: 0.6893\n",
      "Epoch 7: val_accuracy improved from 0.76299 to 0.80090, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.6562 - accuracy: 0.7304 - precision: 0.7575 - recall: 0.6892 - val_loss: 0.5144 - val_accuracy: 0.8009 - val_precision: 0.8131 - val_recall: 0.7811\n",
      "Epoch 8/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.6359 - accuracy: 0.7481 - precision: 0.7725 - recall: 0.7134\n",
      "Epoch 8: val_accuracy improved from 0.80090 to 0.80944, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.6359 - accuracy: 0.7480 - precision: 0.7725 - recall: 0.7132 - val_loss: 0.5028 - val_accuracy: 0.8094 - val_precision: 0.8289 - val_recall: 0.7796\n",
      "Epoch 9/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.6259 - accuracy: 0.7482 - precision: 0.7723 - recall: 0.7127\n",
      "Epoch 9: val_accuracy improved from 0.80944 to 0.81070, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.6255 - accuracy: 0.7483 - precision: 0.7724 - recall: 0.7129 - val_loss: 0.4879 - val_accuracy: 0.8107 - val_precision: 0.8190 - val_recall: 0.7941\n",
      "Epoch 10/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.6119 - accuracy: 0.7555 - precision: 0.7752 - recall: 0.7257\n",
      "Epoch 10: val_accuracy improved from 0.81070 to 0.82953, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.6119 - accuracy: 0.7555 - precision: 0.7752 - recall: 0.7257 - val_loss: 0.4647 - val_accuracy: 0.8295 - val_precision: 0.8394 - val_recall: 0.8099\n",
      "Epoch 11/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.5818 - accuracy: 0.7710 - precision: 0.7897 - recall: 0.7416\n",
      "Epoch 11: val_accuracy did not improve from 0.82953\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.5817 - accuracy: 0.7710 - precision: 0.7897 - recall: 0.7416 - val_loss: 0.4578 - val_accuracy: 0.8293 - val_precision: 0.8424 - val_recall: 0.8160\n",
      "Epoch 12/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.5734 - accuracy: 0.7734 - precision: 0.7904 - recall: 0.7443\n",
      "Epoch 12: val_accuracy did not improve from 0.82953\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5734 - accuracy: 0.7734 - precision: 0.7904 - recall: 0.7443 - val_loss: 0.4961 - val_accuracy: 0.8019 - val_precision: 0.8096 - val_recall: 0.7878\n",
      "Epoch 13/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7751 - precision: 0.7914 - recall: 0.7467\n",
      "Epoch 13: val_accuracy improved from 0.82953 to 0.84082, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 22s 18ms/step - loss: 0.5669 - accuracy: 0.7751 - precision: 0.7914 - recall: 0.7467 - val_loss: 0.4311 - val_accuracy: 0.8408 - val_precision: 0.8508 - val_recall: 0.8260\n",
      "Epoch 14/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.7813 - precision: 0.7974 - recall: 0.7524\n",
      "Epoch 14: val_accuracy improved from 0.84082 to 0.84158, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5551 - accuracy: 0.7813 - precision: 0.7974 - recall: 0.7524 - val_loss: 0.4389 - val_accuracy: 0.8416 - val_precision: 0.8493 - val_recall: 0.8290\n",
      "Epoch 15/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7800 - precision: 0.7969 - recall: 0.7552\n",
      "Epoch 15: val_accuracy did not improve from 0.84158\n",
      "1162/1162 [==============================] - 21s 17ms/step - loss: 0.5510 - accuracy: 0.7799 - precision: 0.7968 - recall: 0.7551 - val_loss: 0.4388 - val_accuracy: 0.8356 - val_precision: 0.8464 - val_recall: 0.8190\n",
      "Epoch 16/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.5530 - accuracy: 0.7814 - precision: 0.7999 - recall: 0.7548\n",
      "Epoch 16: val_accuracy improved from 0.84158 to 0.84308, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5537 - accuracy: 0.7813 - precision: 0.7999 - recall: 0.7549 - val_loss: 0.4225 - val_accuracy: 0.8431 - val_precision: 0.8506 - val_recall: 0.8335\n",
      "Epoch 17/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7874 - precision: 0.8029 - recall: 0.7612\n",
      "Epoch 17: val_accuracy improved from 0.84308 to 0.84559, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5347 - accuracy: 0.7874 - precision: 0.8029 - recall: 0.7612 - val_loss: 0.4101 - val_accuracy: 0.8456 - val_precision: 0.8542 - val_recall: 0.8356\n",
      "Epoch 18/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.5324 - accuracy: 0.7871 - precision: 0.8046 - recall: 0.7631\n",
      "Epoch 18: val_accuracy improved from 0.84559 to 0.84685, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5323 - accuracy: 0.7873 - precision: 0.8047 - recall: 0.7632 - val_loss: 0.4037 - val_accuracy: 0.8468 - val_precision: 0.8558 - val_recall: 0.8373\n",
      "Epoch 19/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.5298 - accuracy: 0.7903 - precision: 0.8045 - recall: 0.7677\n",
      "Epoch 19: val_accuracy did not improve from 0.84685\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.5305 - accuracy: 0.7902 - precision: 0.8042 - recall: 0.7674 - val_loss: 0.4116 - val_accuracy: 0.8453 - val_precision: 0.8525 - val_recall: 0.8358\n",
      "Epoch 20/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.5277 - accuracy: 0.7915 - precision: 0.8083 - recall: 0.7681\n",
      "Epoch 20: val_accuracy improved from 0.84685 to 0.85162, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.5279 - accuracy: 0.7912 - precision: 0.8081 - recall: 0.7679 - val_loss: 0.3963 - val_accuracy: 0.8516 - val_precision: 0.8596 - val_recall: 0.8393\n",
      "Epoch 21/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.5082 - accuracy: 0.7971 - precision: 0.8151 - recall: 0.7751\n",
      "Epoch 21: val_accuracy did not improve from 0.85162\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5077 - accuracy: 0.7974 - precision: 0.8153 - recall: 0.7754 - val_loss: 0.3966 - val_accuracy: 0.8484 - val_precision: 0.8555 - val_recall: 0.8383\n",
      "Epoch 22/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.5172 - accuracy: 0.7939 - precision: 0.8086 - recall: 0.7722\n",
      "Epoch 22: val_accuracy did not improve from 0.85162\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5174 - accuracy: 0.7937 - precision: 0.8083 - recall: 0.7719 - val_loss: 0.4145 - val_accuracy: 0.8423 - val_precision: 0.8533 - val_recall: 0.8308\n",
      "Epoch 23/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.5035 - accuracy: 0.7987 - precision: 0.8152 - recall: 0.7782\n",
      "Epoch 23: val_accuracy improved from 0.85162 to 0.85790, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5034 - accuracy: 0.7988 - precision: 0.8153 - recall: 0.7782 - val_loss: 0.3869 - val_accuracy: 0.8579 - val_precision: 0.8636 - val_recall: 0.8471\n",
      "Epoch 24/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.5043 - accuracy: 0.8004 - precision: 0.8158 - recall: 0.7807\n",
      "Epoch 24: val_accuracy did not improve from 0.85790\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.5041 - accuracy: 0.8006 - precision: 0.8159 - recall: 0.7808 - val_loss: 0.3947 - val_accuracy: 0.8554 - val_precision: 0.8646 - val_recall: 0.8436\n",
      "Epoch 25/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4936 - accuracy: 0.8035 - precision: 0.8164 - recall: 0.7852\n",
      "Epoch 25: val_accuracy did not improve from 0.85790\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4940 - accuracy: 0.8034 - precision: 0.8163 - recall: 0.7851 - val_loss: 0.4420 - val_accuracy: 0.8280 - val_precision: 0.8344 - val_recall: 0.8150\n",
      "Epoch 26/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4905 - accuracy: 0.8089 - precision: 0.8229 - recall: 0.7839\n",
      "Epoch 26: val_accuracy improved from 0.85790 to 0.86116, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4901 - accuracy: 0.8091 - precision: 0.8231 - recall: 0.7841 - val_loss: 0.3772 - val_accuracy: 0.8612 - val_precision: 0.8683 - val_recall: 0.8526\n",
      "Epoch 27/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.8064 - precision: 0.8230 - recall: 0.7859\n",
      "Epoch 27: val_accuracy improved from 0.86116 to 0.86568, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4986 - accuracy: 0.8064 - precision: 0.8230 - recall: 0.7859 - val_loss: 0.3736 - val_accuracy: 0.8657 - val_precision: 0.8715 - val_recall: 0.8549\n",
      "Epoch 28/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4884 - accuracy: 0.8065 - precision: 0.8200 - recall: 0.7844\n",
      "Epoch 28: val_accuracy did not improve from 0.86568\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4890 - accuracy: 0.8061 - precision: 0.8196 - recall: 0.7839 - val_loss: 0.3715 - val_accuracy: 0.8632 - val_precision: 0.8692 - val_recall: 0.8529\n",
      "Epoch 29/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.8075 - precision: 0.8218 - recall: 0.7864\n",
      "Epoch 29: val_accuracy did not improve from 0.86568\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4853 - accuracy: 0.8076 - precision: 0.8219 - recall: 0.7865 - val_loss: 0.3687 - val_accuracy: 0.8609 - val_precision: 0.8662 - val_recall: 0.8546\n",
      "Epoch 30/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4809 - accuracy: 0.8135 - precision: 0.8268 - recall: 0.7940\n",
      "Epoch 30: val_accuracy did not improve from 0.86568\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4807 - accuracy: 0.8134 - precision: 0.8269 - recall: 0.7939 - val_loss: 0.3677 - val_accuracy: 0.8649 - val_precision: 0.8712 - val_recall: 0.8559\n",
      "Epoch 31/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4759 - accuracy: 0.8150 - precision: 0.8306 - recall: 0.7969\n",
      "Epoch 31: val_accuracy did not improve from 0.86568\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4759 - accuracy: 0.8149 - precision: 0.8305 - recall: 0.7968 - val_loss: 0.3712 - val_accuracy: 0.8649 - val_precision: 0.8698 - val_recall: 0.8569\n",
      "Epoch 32/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4784 - accuracy: 0.8105 - precision: 0.8269 - recall: 0.7907\n",
      "Epoch 32: val_accuracy did not improve from 0.86568\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4785 - accuracy: 0.8104 - precision: 0.8267 - recall: 0.7906 - val_loss: 0.3758 - val_accuracy: 0.8632 - val_precision: 0.8728 - val_recall: 0.8546\n",
      "Epoch 33/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4718 - accuracy: 0.8122 - precision: 0.8272 - recall: 0.7925\n",
      "Epoch 33: val_accuracy improved from 0.86568 to 0.86693, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4721 - accuracy: 0.8121 - precision: 0.8270 - recall: 0.7923 - val_loss: 0.3658 - val_accuracy: 0.8669 - val_precision: 0.8723 - val_recall: 0.8607\n",
      "Epoch 34/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4766 - accuracy: 0.8167 - precision: 0.8321 - recall: 0.7971\n",
      "Epoch 34: val_accuracy improved from 0.86693 to 0.86894, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4772 - accuracy: 0.8162 - precision: 0.8318 - recall: 0.7967 - val_loss: 0.3650 - val_accuracy: 0.8689 - val_precision: 0.8764 - val_recall: 0.8599\n",
      "Epoch 35/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4821 - accuracy: 0.8092 - precision: 0.8248 - recall: 0.7886\n",
      "Epoch 35: val_accuracy improved from 0.86894 to 0.87145, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 22s 18ms/step - loss: 0.4821 - accuracy: 0.8092 - precision: 0.8248 - recall: 0.7886 - val_loss: 0.3667 - val_accuracy: 0.8715 - val_precision: 0.8777 - val_recall: 0.8629\n",
      "Epoch 36/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.8146 - precision: 0.8282 - recall: 0.7963\n",
      "Epoch 36: val_accuracy did not improve from 0.87145\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4658 - accuracy: 0.8146 - precision: 0.8282 - recall: 0.7963 - val_loss: 0.4252 - val_accuracy: 0.8373 - val_precision: 0.8428 - val_recall: 0.8268\n",
      "Epoch 37/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.8183 - precision: 0.8340 - recall: 0.8004\n",
      "Epoch 37: val_accuracy did not improve from 0.87145\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4654 - accuracy: 0.8183 - precision: 0.8339 - recall: 0.8004 - val_loss: 0.3861 - val_accuracy: 0.8584 - val_precision: 0.8651 - val_recall: 0.8499\n",
      "Epoch 38/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4714 - accuracy: 0.8113 - precision: 0.8273 - recall: 0.7945\n",
      "Epoch 38: val_accuracy did not improve from 0.87145\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4715 - accuracy: 0.8111 - precision: 0.8272 - recall: 0.7944 - val_loss: 0.3754 - val_accuracy: 0.8659 - val_precision: 0.8716 - val_recall: 0.8569\n",
      "Epoch 39/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4586 - accuracy: 0.8193 - precision: 0.8316 - recall: 0.8007\n",
      "Epoch 39: val_accuracy improved from 0.87145 to 0.87246, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4585 - accuracy: 0.8193 - precision: 0.8316 - recall: 0.8007 - val_loss: 0.3571 - val_accuracy: 0.8725 - val_precision: 0.8785 - val_recall: 0.8657\n",
      "Epoch 40/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4572 - accuracy: 0.8236 - precision: 0.8366 - recall: 0.8038\n",
      "Epoch 40: val_accuracy did not improve from 0.87246\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4576 - accuracy: 0.8232 - precision: 0.8363 - recall: 0.8035 - val_loss: 0.4064 - val_accuracy: 0.8446 - val_precision: 0.8520 - val_recall: 0.8368\n",
      "Epoch 41/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4584 - accuracy: 0.8230 - precision: 0.8347 - recall: 0.8058\n",
      "Epoch 41: val_accuracy improved from 0.87246 to 0.87371, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4584 - accuracy: 0.8229 - precision: 0.8346 - recall: 0.8058 - val_loss: 0.3543 - val_accuracy: 0.8737 - val_precision: 0.8797 - val_recall: 0.8662\n",
      "Epoch 42/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4597 - accuracy: 0.8191 - precision: 0.8327 - recall: 0.8018\n",
      "Epoch 42: val_accuracy did not improve from 0.87371\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4594 - accuracy: 0.8192 - precision: 0.8328 - recall: 0.8020 - val_loss: 0.3902 - val_accuracy: 0.8609 - val_precision: 0.8689 - val_recall: 0.8521\n",
      "Epoch 43/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4508 - accuracy: 0.8287 - precision: 0.8425 - recall: 0.8130\n",
      "Epoch 43: val_accuracy did not improve from 0.87371\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4508 - accuracy: 0.8286 - precision: 0.8423 - recall: 0.8129 - val_loss: 0.3551 - val_accuracy: 0.8725 - val_precision: 0.8768 - val_recall: 0.8652\n",
      "Epoch 44/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4636 - accuracy: 0.8176 - precision: 0.8317 - recall: 0.8018\n",
      "Epoch 44: val_accuracy did not improve from 0.87371\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4637 - accuracy: 0.8176 - precision: 0.8317 - recall: 0.8017 - val_loss: 0.3500 - val_accuracy: 0.8697 - val_precision: 0.8742 - val_recall: 0.8637\n",
      "Epoch 45/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4590 - accuracy: 0.8187 - precision: 0.8322 - recall: 0.8008\n",
      "Epoch 45: val_accuracy did not improve from 0.87371\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4590 - accuracy: 0.8187 - precision: 0.8322 - recall: 0.8007 - val_loss: 0.3918 - val_accuracy: 0.8534 - val_precision: 0.8607 - val_recall: 0.8453\n",
      "Epoch 46/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4569 - accuracy: 0.8226 - precision: 0.8352 - recall: 0.8048\n",
      "Epoch 46: val_accuracy did not improve from 0.87371\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4576 - accuracy: 0.8222 - precision: 0.8348 - recall: 0.8044 - val_loss: 0.3540 - val_accuracy: 0.8737 - val_precision: 0.8800 - val_recall: 0.8669\n",
      "Epoch 47/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.8332 - precision: 0.8453 - recall: 0.8145\n",
      "Epoch 47: val_accuracy improved from 0.87371 to 0.87396, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4389 - accuracy: 0.8333 - precision: 0.8455 - recall: 0.8146 - val_loss: 0.3564 - val_accuracy: 0.8740 - val_precision: 0.8787 - val_recall: 0.8654\n",
      "Epoch 48/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4544 - accuracy: 0.8252 - precision: 0.8356 - recall: 0.8064\n",
      "Epoch 48: val_accuracy did not improve from 0.87396\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4543 - accuracy: 0.8254 - precision: 0.8357 - recall: 0.8064 - val_loss: 0.3721 - val_accuracy: 0.8662 - val_precision: 0.8724 - val_recall: 0.8581\n",
      "Epoch 49/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4512 - accuracy: 0.8216 - precision: 0.8359 - recall: 0.8046\n",
      "Epoch 49: val_accuracy did not improve from 0.87396\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4511 - accuracy: 0.8217 - precision: 0.8360 - recall: 0.8047 - val_loss: 0.3732 - val_accuracy: 0.8682 - val_precision: 0.8732 - val_recall: 0.8609\n",
      "Epoch 50/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4414 - accuracy: 0.8267 - precision: 0.8393 - recall: 0.8117\n",
      "Epoch 50: val_accuracy improved from 0.87396 to 0.87472, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4415 - accuracy: 0.8265 - precision: 0.8391 - recall: 0.8116 - val_loss: 0.3457 - val_accuracy: 0.8747 - val_precision: 0.8794 - val_recall: 0.8694\n",
      "Epoch 51/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4483 - accuracy: 0.8244 - precision: 0.8378 - recall: 0.8065\n",
      "Epoch 51: val_accuracy did not improve from 0.87472\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4482 - accuracy: 0.8244 - precision: 0.8379 - recall: 0.8064 - val_loss: 0.3432 - val_accuracy: 0.8737 - val_precision: 0.8777 - val_recall: 0.8684\n",
      "Epoch 52/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4390 - accuracy: 0.8309 - precision: 0.8414 - recall: 0.8141\n",
      "Epoch 52: val_accuracy improved from 0.87472 to 0.87622, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4389 - accuracy: 0.8308 - precision: 0.8415 - recall: 0.8141 - val_loss: 0.3463 - val_accuracy: 0.8762 - val_precision: 0.8801 - val_recall: 0.8699\n",
      "Epoch 53/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4449 - accuracy: 0.8295 - precision: 0.8407 - recall: 0.8124\n",
      "Epoch 53: val_accuracy did not improve from 0.87622\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4447 - accuracy: 0.8295 - precision: 0.8408 - recall: 0.8125 - val_loss: 0.3578 - val_accuracy: 0.8722 - val_precision: 0.8751 - val_recall: 0.8657\n",
      "Epoch 54/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4463 - accuracy: 0.8325 - precision: 0.8432 - recall: 0.8161\n",
      "Epoch 54: val_accuracy improved from 0.87622 to 0.87823, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4461 - accuracy: 0.8326 - precision: 0.8432 - recall: 0.8162 - val_loss: 0.3471 - val_accuracy: 0.8782 - val_precision: 0.8819 - val_recall: 0.8722\n",
      "Epoch 55/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4385 - accuracy: 0.8280 - precision: 0.8406 - recall: 0.8108\n",
      "Epoch 55: val_accuracy did not improve from 0.87823\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4385 - accuracy: 0.8280 - precision: 0.8406 - recall: 0.8108 - val_loss: 0.3467 - val_accuracy: 0.8767 - val_precision: 0.8815 - val_recall: 0.8707\n",
      "Epoch 56/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4345 - accuracy: 0.8285 - precision: 0.8411 - recall: 0.8113\n",
      "Epoch 56: val_accuracy improved from 0.87823 to 0.87848, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4343 - accuracy: 0.8286 - precision: 0.8412 - recall: 0.8114 - val_loss: 0.3476 - val_accuracy: 0.8785 - val_precision: 0.8822 - val_recall: 0.8725\n",
      "Epoch 57/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.8301 - precision: 0.8415 - recall: 0.8132\n",
      "Epoch 57: val_accuracy improved from 0.87848 to 0.87949, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4348 - accuracy: 0.8300 - precision: 0.8414 - recall: 0.8130 - val_loss: 0.3426 - val_accuracy: 0.8795 - val_precision: 0.8840 - val_recall: 0.8747\n",
      "Epoch 58/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8309 - precision: 0.8433 - recall: 0.8158\n",
      "Epoch 58: val_accuracy did not improve from 0.87949\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4405 - accuracy: 0.8309 - precision: 0.8433 - recall: 0.8159 - val_loss: 0.3521 - val_accuracy: 0.8747 - val_precision: 0.8796 - val_recall: 0.8679\n",
      "Epoch 59/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.8252 - precision: 0.8388 - recall: 0.8105\n",
      "Epoch 59: val_accuracy did not improve from 0.87949\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4400 - accuracy: 0.8252 - precision: 0.8388 - recall: 0.8105 - val_loss: 0.3456 - val_accuracy: 0.8772 - val_precision: 0.8814 - val_recall: 0.8712\n",
      "Epoch 60/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.8269 - precision: 0.8383 - recall: 0.8102\n",
      "Epoch 60: val_accuracy did not improve from 0.87949\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4419 - accuracy: 0.8268 - precision: 0.8381 - recall: 0.8101 - val_loss: 0.3533 - val_accuracy: 0.8742 - val_precision: 0.8793 - val_recall: 0.8684\n",
      "Epoch 61/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4301 - accuracy: 0.8343 - precision: 0.8464 - recall: 0.8177\n",
      "Epoch 61: val_accuracy did not improve from 0.87949\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4299 - accuracy: 0.8344 - precision: 0.8465 - recall: 0.8178 - val_loss: 0.3444 - val_accuracy: 0.8785 - val_precision: 0.8839 - val_recall: 0.8720\n",
      "Epoch 62/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.8279 - precision: 0.8391 - recall: 0.8101\n",
      "Epoch 62: val_accuracy improved from 0.87949 to 0.88049, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4430 - accuracy: 0.8279 - precision: 0.8391 - recall: 0.8101 - val_loss: 0.3416 - val_accuracy: 0.8805 - val_precision: 0.8842 - val_recall: 0.8745\n",
      "Epoch 63/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4442 - accuracy: 0.8315 - precision: 0.8438 - recall: 0.8154\n",
      "Epoch 63: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4441 - accuracy: 0.8316 - precision: 0.8439 - recall: 0.8156 - val_loss: 0.3518 - val_accuracy: 0.8742 - val_precision: 0.8795 - val_recall: 0.8682\n",
      "Epoch 64/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4342 - accuracy: 0.8362 - precision: 0.8473 - recall: 0.8182\n",
      "Epoch 64: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4349 - accuracy: 0.8362 - precision: 0.8473 - recall: 0.8181 - val_loss: 0.3519 - val_accuracy: 0.8767 - val_precision: 0.8814 - val_recall: 0.8694\n",
      "Epoch 65/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4313 - accuracy: 0.8309 - precision: 0.8420 - recall: 0.8163\n",
      "Epoch 65: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4312 - accuracy: 0.8309 - precision: 0.8421 - recall: 0.8164 - val_loss: 0.3442 - val_accuracy: 0.8782 - val_precision: 0.8816 - val_recall: 0.8727\n",
      "Epoch 66/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8315 - precision: 0.8441 - recall: 0.8135\n",
      "Epoch 66: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4355 - accuracy: 0.8315 - precision: 0.8441 - recall: 0.8135 - val_loss: 0.3383 - val_accuracy: 0.8797 - val_precision: 0.8835 - val_recall: 0.8737\n",
      "Epoch 67/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4389 - accuracy: 0.8292 - precision: 0.8423 - recall: 0.8153\n",
      "Epoch 67: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4387 - accuracy: 0.8293 - precision: 0.8425 - recall: 0.8155 - val_loss: 0.3433 - val_accuracy: 0.8780 - val_precision: 0.8823 - val_recall: 0.8717\n",
      "Epoch 68/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.8300 - precision: 0.8423 - recall: 0.8135\n",
      "Epoch 68: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4387 - accuracy: 0.8300 - precision: 0.8424 - recall: 0.8134 - val_loss: 0.3473 - val_accuracy: 0.8782 - val_precision: 0.8826 - val_recall: 0.8699\n",
      "Epoch 69/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4393 - accuracy: 0.8266 - precision: 0.8390 - recall: 0.8121\n",
      "Epoch 69: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4391 - accuracy: 0.8265 - precision: 0.8390 - recall: 0.8121 - val_loss: 0.3407 - val_accuracy: 0.8790 - val_precision: 0.8838 - val_recall: 0.8727\n",
      "Epoch 70/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.8309 - precision: 0.8438 - recall: 0.8159\n",
      "Epoch 70: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4351 - accuracy: 0.8309 - precision: 0.8440 - recall: 0.8160 - val_loss: 0.3380 - val_accuracy: 0.8785 - val_precision: 0.8820 - val_recall: 0.8727\n",
      "Epoch 71/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.8273 - precision: 0.8403 - recall: 0.8118\n",
      "Epoch 71: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4356 - accuracy: 0.8273 - precision: 0.8403 - recall: 0.8118 - val_loss: 0.3481 - val_accuracy: 0.8760 - val_precision: 0.8802 - val_recall: 0.8704\n",
      "Epoch 72/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4407 - accuracy: 0.8300 - precision: 0.8429 - recall: 0.8146\n",
      "Epoch 72: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4403 - accuracy: 0.8303 - precision: 0.8431 - recall: 0.8149 - val_loss: 0.3471 - val_accuracy: 0.8762 - val_precision: 0.8802 - val_recall: 0.8710\n",
      "Epoch 73/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.8325 - precision: 0.8450 - recall: 0.8164\n",
      "Epoch 73: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4384 - accuracy: 0.8328 - precision: 0.8452 - recall: 0.8166 - val_loss: 0.3379 - val_accuracy: 0.8792 - val_precision: 0.8823 - val_recall: 0.8732\n",
      "Epoch 74/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4396 - accuracy: 0.8295 - precision: 0.8394 - recall: 0.8126\n",
      "Epoch 74: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4396 - accuracy: 0.8294 - precision: 0.8394 - recall: 0.8125 - val_loss: 0.3376 - val_accuracy: 0.8787 - val_precision: 0.8828 - val_recall: 0.8740\n",
      "Epoch 75/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4343 - accuracy: 0.8316 - precision: 0.8423 - recall: 0.8145\n",
      "Epoch 75: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4343 - accuracy: 0.8317 - precision: 0.8425 - recall: 0.8144 - val_loss: 0.3355 - val_accuracy: 0.8792 - val_precision: 0.8819 - val_recall: 0.8737\n",
      "Epoch 76/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.8269 - precision: 0.8410 - recall: 0.8108\n",
      "Epoch 76: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4391 - accuracy: 0.8269 - precision: 0.8410 - recall: 0.8108 - val_loss: 0.3400 - val_accuracy: 0.8790 - val_precision: 0.8833 - val_recall: 0.8745\n",
      "Epoch 77/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.8342 - precision: 0.8448 - recall: 0.8191\n",
      "Epoch 77: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 22s 18ms/step - loss: 0.4389 - accuracy: 0.8342 - precision: 0.8448 - recall: 0.8190 - val_loss: 0.3435 - val_accuracy: 0.8787 - val_precision: 0.8838 - val_recall: 0.8727\n",
      "Epoch 78/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.8249 - precision: 0.8380 - recall: 0.8062\n",
      "Epoch 78: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4474 - accuracy: 0.8249 - precision: 0.8380 - recall: 0.8062 - val_loss: 0.3395 - val_accuracy: 0.8782 - val_precision: 0.8836 - val_recall: 0.8727\n",
      "Epoch 79/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.8303 - precision: 0.8429 - recall: 0.8127\n",
      "Epoch 79: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4299 - accuracy: 0.8303 - precision: 0.8429 - recall: 0.8127 - val_loss: 0.3373 - val_accuracy: 0.8800 - val_precision: 0.8832 - val_recall: 0.8737\n",
      "Epoch 80/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4379 - accuracy: 0.8288 - precision: 0.8411 - recall: 0.8109\n",
      "Epoch 80: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4376 - accuracy: 0.8290 - precision: 0.8414 - recall: 0.8111 - val_loss: 0.3401 - val_accuracy: 0.8787 - val_precision: 0.8830 - val_recall: 0.8737\n",
      "Epoch 81/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8321 - precision: 0.8462 - recall: 0.8176\n",
      "Epoch 81: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4300 - accuracy: 0.8321 - precision: 0.8462 - recall: 0.8176 - val_loss: 0.3449 - val_accuracy: 0.8797 - val_precision: 0.8841 - val_recall: 0.8730\n",
      "Epoch 82/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.8358 - precision: 0.8472 - recall: 0.8199\n",
      "Epoch 82: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4280 - accuracy: 0.8358 - precision: 0.8472 - recall: 0.8199 - val_loss: 0.3372 - val_accuracy: 0.8780 - val_precision: 0.8818 - val_recall: 0.8725\n",
      "Epoch 83/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4358 - accuracy: 0.8337 - precision: 0.8459 - recall: 0.8192\n",
      "Epoch 83: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4356 - accuracy: 0.8337 - precision: 0.8460 - recall: 0.8193 - val_loss: 0.3487 - val_accuracy: 0.8782 - val_precision: 0.8822 - val_recall: 0.8707\n",
      "Epoch 84/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4247 - accuracy: 0.8304 - precision: 0.8439 - recall: 0.8134\n",
      "Epoch 84: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4248 - accuracy: 0.8304 - precision: 0.8439 - recall: 0.8134 - val_loss: 0.3392 - val_accuracy: 0.8785 - val_precision: 0.8825 - val_recall: 0.8727\n",
      "Epoch 85/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4366 - accuracy: 0.8305 - precision: 0.8437 - recall: 0.8160\n",
      "Epoch 85: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4366 - accuracy: 0.8305 - precision: 0.8437 - recall: 0.8160 - val_loss: 0.3403 - val_accuracy: 0.8785 - val_precision: 0.8836 - val_recall: 0.8730\n",
      "Epoch 86/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4376 - accuracy: 0.8305 - precision: 0.8428 - recall: 0.8157\n",
      "Epoch 86: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4376 - accuracy: 0.8305 - precision: 0.8428 - recall: 0.8157 - val_loss: 0.3439 - val_accuracy: 0.8787 - val_precision: 0.8833 - val_recall: 0.8740\n",
      "Epoch 87/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.8332 - precision: 0.8462 - recall: 0.8185\n",
      "Epoch 87: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4294 - accuracy: 0.8332 - precision: 0.8461 - recall: 0.8185 - val_loss: 0.3370 - val_accuracy: 0.8787 - val_precision: 0.8819 - val_recall: 0.8735\n",
      "Epoch 88/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8291 - precision: 0.8409 - recall: 0.8128\n",
      "Epoch 88: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4352 - accuracy: 0.8292 - precision: 0.8411 - recall: 0.8130 - val_loss: 0.3376 - val_accuracy: 0.8787 - val_precision: 0.8823 - val_recall: 0.8730\n",
      "Epoch 89/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4272 - accuracy: 0.8344 - precision: 0.8483 - recall: 0.8198\n",
      "Epoch 89: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4274 - accuracy: 0.8344 - precision: 0.8482 - recall: 0.8198 - val_loss: 0.3388 - val_accuracy: 0.8782 - val_precision: 0.8825 - val_recall: 0.8735\n",
      "Epoch 90/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.8340 - precision: 0.8459 - recall: 0.8185\n",
      "Epoch 90: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4317 - accuracy: 0.8340 - precision: 0.8459 - recall: 0.8185 - val_loss: 0.3410 - val_accuracy: 0.8775 - val_precision: 0.8825 - val_recall: 0.8727\n",
      "Epoch 91/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4378 - accuracy: 0.8303 - precision: 0.8428 - recall: 0.8152\n",
      "Epoch 91: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4378 - accuracy: 0.8303 - precision: 0.8429 - recall: 0.8152 - val_loss: 0.3388 - val_accuracy: 0.8795 - val_precision: 0.8837 - val_recall: 0.8740\n",
      "Epoch 92/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8299 - precision: 0.8412 - recall: 0.8145\n",
      "Epoch 92: val_accuracy did not improve from 0.88049\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4405 - accuracy: 0.8298 - precision: 0.8412 - recall: 0.8144 - val_loss: 0.3371 - val_accuracy: 0.8802 - val_precision: 0.8838 - val_recall: 0.8750\n",
      "Epoch 93/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4317 - accuracy: 0.8307 - precision: 0.8430 - recall: 0.8179\n",
      "Epoch 93: val_accuracy improved from 0.88049 to 0.88175, saving model to v4_backbone_1_best_model.h5\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4316 - accuracy: 0.8307 - precision: 0.8430 - recall: 0.8179 - val_loss: 0.3349 - val_accuracy: 0.8817 - val_precision: 0.8841 - val_recall: 0.8750\n",
      "Epoch 94/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.8350 - precision: 0.8476 - recall: 0.8214\n",
      "Epoch 94: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4301 - accuracy: 0.8350 - precision: 0.8476 - recall: 0.8214 - val_loss: 0.3354 - val_accuracy: 0.8812 - val_precision: 0.8839 - val_recall: 0.8752\n",
      "Epoch 95/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.8311 - precision: 0.8424 - recall: 0.8162\n",
      "Epoch 95: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4307 - accuracy: 0.8312 - precision: 0.8425 - recall: 0.8163 - val_loss: 0.3385 - val_accuracy: 0.8792 - val_precision: 0.8831 - val_recall: 0.8742\n",
      "Epoch 96/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8347 - precision: 0.8448 - recall: 0.8184\n",
      "Epoch 96: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4267 - accuracy: 0.8348 - precision: 0.8449 - recall: 0.8184 - val_loss: 0.3364 - val_accuracy: 0.8800 - val_precision: 0.8832 - val_recall: 0.8750\n",
      "Epoch 97/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4341 - accuracy: 0.8306 - precision: 0.8451 - recall: 0.8151\n",
      "Epoch 97: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4345 - accuracy: 0.8304 - precision: 0.8449 - recall: 0.8149 - val_loss: 0.3388 - val_accuracy: 0.8807 - val_precision: 0.8837 - val_recall: 0.8740\n",
      "Epoch 98/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4320 - accuracy: 0.8361 - precision: 0.8468 - recall: 0.8183\n",
      "Epoch 98: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4320 - accuracy: 0.8362 - precision: 0.8469 - recall: 0.8184 - val_loss: 0.3422 - val_accuracy: 0.8787 - val_precision: 0.8842 - val_recall: 0.8742\n",
      "Epoch 99/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8350 - precision: 0.8465 - recall: 0.8193\n",
      "Epoch 99: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4267 - accuracy: 0.8349 - precision: 0.8464 - recall: 0.8191 - val_loss: 0.3401 - val_accuracy: 0.8792 - val_precision: 0.8833 - val_recall: 0.8742\n",
      "Epoch 100/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4333 - accuracy: 0.8315 - precision: 0.8439 - recall: 0.8173\n",
      "Epoch 100: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4332 - accuracy: 0.8315 - precision: 0.8439 - recall: 0.8174 - val_loss: 0.3389 - val_accuracy: 0.8800 - val_precision: 0.8842 - val_recall: 0.8742\n",
      "Epoch 101/150\n",
      "1159/1162 [============================>.] - ETA: 0s - loss: 0.4325 - accuracy: 0.8355 - precision: 0.8493 - recall: 0.8203\n",
      "Epoch 101: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4325 - accuracy: 0.8357 - precision: 0.8494 - recall: 0.8204 - val_loss: 0.3404 - val_accuracy: 0.8782 - val_precision: 0.8824 - val_recall: 0.8737\n",
      "Epoch 102/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8316 - precision: 0.8441 - recall: 0.8171\n",
      "Epoch 102: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4316 - accuracy: 0.8316 - precision: 0.8441 - recall: 0.8171 - val_loss: 0.3397 - val_accuracy: 0.8780 - val_precision: 0.8819 - val_recall: 0.8737\n",
      "Epoch 103/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4380 - accuracy: 0.8350 - precision: 0.8467 - recall: 0.8196\n",
      "Epoch 103: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4381 - accuracy: 0.8348 - precision: 0.8465 - recall: 0.8194 - val_loss: 0.3382 - val_accuracy: 0.8787 - val_precision: 0.8825 - val_recall: 0.8747\n",
      "Epoch 104/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.8323 - precision: 0.8425 - recall: 0.8165\n",
      "Epoch 104: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4348 - accuracy: 0.8323 - precision: 0.8426 - recall: 0.8166 - val_loss: 0.3390 - val_accuracy: 0.8790 - val_precision: 0.8823 - val_recall: 0.8732\n",
      "Epoch 105/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4363 - accuracy: 0.8355 - precision: 0.8479 - recall: 0.8184\n",
      "Epoch 105: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 20s 17ms/step - loss: 0.4372 - accuracy: 0.8354 - precision: 0.8478 - recall: 0.8183 - val_loss: 0.3377 - val_accuracy: 0.8792 - val_precision: 0.8821 - val_recall: 0.8732\n",
      "Epoch 106/150\n",
      "1161/1162 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.8346 - precision: 0.8449 - recall: 0.8183\n",
      "Epoch 106: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4256 - accuracy: 0.8346 - precision: 0.8449 - recall: 0.8183 - val_loss: 0.3401 - val_accuracy: 0.8792 - val_precision: 0.8827 - val_recall: 0.8747\n",
      "Epoch 107/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.8329 - precision: 0.8450 - recall: 0.8155\n",
      "Epoch 107: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4306 - accuracy: 0.8329 - precision: 0.8450 - recall: 0.8155 - val_loss: 0.3369 - val_accuracy: 0.8787 - val_precision: 0.8831 - val_recall: 0.8740\n",
      "Epoch 108/150\n",
      "1160/1162 [============================>.] - ETA: 0s - loss: 0.4203 - accuracy: 0.8400 - precision: 0.8526 - recall: 0.8230\n",
      "Epoch 108: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4202 - accuracy: 0.8400 - precision: 0.8526 - recall: 0.8230 - val_loss: 0.3397 - val_accuracy: 0.8787 - val_precision: 0.8834 - val_recall: 0.8735\n",
      "Epoch 109/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.8289 - precision: 0.8438 - recall: 0.8132\n",
      "Epoch 109: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4391 - accuracy: 0.8289 - precision: 0.8438 - recall: 0.8132 - val_loss: 0.3381 - val_accuracy: 0.8795 - val_precision: 0.8830 - val_recall: 0.8737\n",
      "Epoch 110/150\n",
      "1162/1162 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8361 - precision: 0.8471 - recall: 0.8171\n",
      "Epoch 110: val_accuracy did not improve from 0.88175\n",
      "1162/1162 [==============================] - 21s 18ms/step - loss: 0.4284 - accuracy: 0.8361 - precision: 0.8471 - recall: 0.8171 - val_loss: 0.3369 - val_accuracy: 0.8790 - val_precision: 0.8826 - val_recall: 0.8742\n",
      "Epoch 111/150\n",
      " 972/1162 [========================>.....] - ETA: 3s - loss: 0.4248 - accuracy: 0.8318 - precision: 0.8444 - recall: 0.8166"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=N_EPOCH,\n",
    "    verbose=1,  \n",
    "    callbacks=[checkpoint, tensorboard_callback]\n",
    "    # callbacks=[checkpoint, lr_reducer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label = 'loss')\n",
    "plt.plot(hist.history['accuracy'], label = 'accuracy')\n",
    "plt.plot(hist.history['val_loss'], label = 'val_loss')\n",
    "plt.plot(hist.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq!!   86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('./v3_3().h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.sparsity.keras import UpdatePruningStep\n",
    "\n",
    "# train_dataset, train_number_labels,\n",
    "\n",
    "# 초기 모델 로드\n",
    "model = tf.keras.models.load_model('./v1_13.h5')\n",
    "\n",
    "# 프루닝 설정\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_train_samples = origin_images.shape[0]\n",
    "end_step = np.ceil(num_train_samples / batch_size).astype(np.int32) * epochs\n",
    "pruning_params = {\n",
    "    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                 final_sparsity=0.80,\n",
    "                                                 begin_step=0,\n",
    "                                                 end_step=end_step)\n",
    "}\n",
    "\n",
    "# 모델에 프루닝 래퍼 적용\n",
    "model_for_pruning = sparsity.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# 프루닝 모델 컴파일 및 훈련\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "                          loss=tf.keras.losses.categorical_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    UpdatePruningStep()\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(origin_images, number_labels, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "# 프루닝 제거 및 최종 모델 저장\n",
    "final_model = sparsity.strip_pruning(model_for_pruning)\n",
    "final_model.save('pruned_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset_tf = tf.data.Dataset.from_tensor_slices(test_dataset)\n",
    "# train_dataset을 배치로 만듦\n",
    "batched_train_dataset = train_dataset_tf.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 기존 모델 로드\n",
    "model = tf.keras.models.load_model('pruned_model.h5')\n",
    "\n",
    "# 대표 데이터셋을 준비하는 함수\n",
    "def representative_data_gen():\n",
    "    for input_value in batched_train_dataset.take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "# 훈련 데이터셋을 로드하고 배치 처리\n",
    "# 여기서 test_dataset와 batch_size는 적절한 값으로 설정해야 합니다.\n",
    "batched_train_dataset = tf.data.Dataset.from_tensor_slices(test_dataset).batch(batch_size)\n",
    "batched_train_dataset = batched_train_dataset.map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "# TFLite 변환기 설정\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# 모든 연산을 float32로 제한\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "# 입력과 출력을 float32로 설정\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "# 모델을 TFLite 모델로 변환\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# TFLite 모델 저장\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
