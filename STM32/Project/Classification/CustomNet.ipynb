{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# class Logger(object):\n",
    "#     def __init__(self, filename=\"tensorflow_logs.log\"):\n",
    "#         self.terminal = sys.stdout\n",
    "#         self.log = open(filename, \"a\")\n",
    "\n",
    "#     def write(self, message):\n",
    "#         self.terminal.write(message)\n",
    "#         self.log.write(message)\n",
    "\n",
    "#     def flush(self):\n",
    "#         # 이 메소드는 flush() 인터페이스를 유지하기 위한 것입니다.\n",
    "#         pass\n",
    "\n",
    "# sys.stdout = Logger(\"tensorflow_logs.log\")\n",
    "\n",
    "# # 테스트 출력\n",
    "# print(\"This will be written to both console and file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13276, 24, 32, 1) (13276,) (13276, 4, 4) 13276\n",
      "255 0\n",
      "[[[0.25       0.625      0.46875    1.        ]\n",
      "  [0.         0.         0.25       0.33333333]\n",
      "  [0.46875    0.54166667 0.71875    0.95833333]\n",
      "  [0.8125     0.66666667 1.         1.        ]]\n",
      "\n",
      " [[0.28125    0.625      0.5        1.        ]\n",
      "  [0.46875    0.54166667 0.75       1.        ]\n",
      "  [0.         0.         0.25       0.33333333]\n",
      "  [0.8125     0.625      1.         0.83333333]]\n",
      "\n",
      " [[0.3125     0.66666667 0.5        1.        ]\n",
      "  [0.4375     0.54166667 0.78125    0.95833333]\n",
      "  [0.8125     0.54166667 1.         0.83333333]\n",
      "  [0.         0.         0.1875     0.29166667]]\n",
      "\n",
      " [[0.375      0.66666667 0.53125    1.        ]\n",
      "  [0.53125    0.58333333 0.78125    0.95833333]\n",
      "  [0.78125    0.58333333 1.         0.83333333]\n",
      "  [0.         0.         0.1875     0.375     ]]\n",
      "\n",
      " [[0.375      0.66666667 0.5625     1.        ]\n",
      "  [0.5        0.54166667 0.8125     0.91666667]\n",
      "  [0.         0.         0.21875    0.33333333]\n",
      "  [0.84375    0.54166667 1.         0.75      ]]\n",
      "\n",
      " [[0.8125     0.5        1.         0.70833333]\n",
      "  [0.40625    0.66666667 0.625      1.        ]\n",
      "  [0.5625     0.58333333 0.78125    0.95833333]\n",
      "  [0.         0.04166667 0.25       0.33333333]]\n",
      "\n",
      " [[0.         0.08333333 0.21875    0.33333333]\n",
      "  [0.46875    0.79166667 0.625      1.        ]\n",
      "  [0.5625     0.54166667 0.78125    0.91666667]\n",
      "  [0.84375    0.45833333 1.         0.66666667]]\n",
      "\n",
      " [[0.53125    0.79166667 0.6875     1.        ]\n",
      "  [0.5        0.54166667 0.8125     0.875     ]\n",
      "  [0.         0.04166667 0.21875    0.33333333]\n",
      "  [0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.53125    0.79166667 0.84375    1.        ]\n",
      "  [0.46875    0.54166667 0.8125     0.79166667]\n",
      "  [0.         0.04166667 0.25       0.29166667]\n",
      "  [0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.59375    0.75       0.8125     1.        ]\n",
      "  [0.53125    0.54166667 0.75       0.83333333]\n",
      "  [0.         0.04166667 0.1875     0.29166667]\n",
      "  [0.         0.         0.         0.        ]]]\n",
      "[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 1, 0], [1, 1, 1, 0]]\n",
      "(12880, 24, 32, 1)\n",
      "(12880, 4, 4)\n",
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "datasets = np.load('npz/ObjectDetection.npz', allow_pickle=True)\n",
    "images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']\n",
    "\n",
    "max_label_length = 4\n",
    "labels = []\n",
    "for num in numbers:\n",
    "    cls = [1] * num if num != 0 else [0]\n",
    "    cls += [0] * (max_label_length - len(cls))\n",
    "    labels.append(cls)\n",
    "\n",
    "# labels = np.array(labels)\n",
    "expected_image_shape = images[9000].shape\n",
    "expected_bbox_shape = bboxes[9000].shape\n",
    "expected_label_length = bboxes[9000].shape\n",
    "\n",
    "non_zero_indices = np.where(numbers != 0)[0]\n",
    "\n",
    "images_filtered = images[non_zero_indices]\n",
    "bboxes_filtered = bboxes[non_zero_indices]\n",
    "labels_filtered = np.array(labels)[non_zero_indices]\n",
    "# labels = np.array(labels)\n",
    "\n",
    "\n",
    "for i in range(len(images_filtered)):\n",
    "    image = images_filtered[i]\n",
    "    bbox = bboxes_filtered[i]\n",
    "    label = labels_filtered[i]\n",
    "\n",
    "    # 이미지 형태와 데이터 타입 검증\n",
    "    if image.shape != expected_image_shape or image.dtype != np.uint8:\n",
    "        print(f\"이미지 {i}의 형태 또는 데이터 타입이 잘못되었습니다: 형태={image.shape}, 데이터 타입={image.dtype}\")\n",
    "\n",
    "    # 바운딩 박스 형태와 데이터 타입 검증\n",
    "    if bbox.shape != expected_bbox_shape or bbox.dtype != np.float64:\n",
    "        print(f\"바운딩 박스 {i}의 형태 또는 데이터 타입이 잘못되었습니다: 형태={bbox.shape}, 데이터 타입={bbox.dtype}\")\n",
    "\n",
    "    # # 레이블 길이와 데이터 타입 검증\n",
    "    # if len(label) != expected_label_length or not all(isinstance(x, int) for x in label):\n",
    "    #     print(f\"레이블 {i}의 길이 또는 데이터 타입이 잘못되었습니다: 길이={len(label)}, 레이블={label}\")\n",
    "\n",
    "print(images.shape, numbers.shape, bboxes.shape, len(labels))\n",
    "\n",
    "print(images.max(), images.min())\n",
    "print(bboxes[9000:9010])\n",
    "print(labels[9000:9010])\n",
    "\n",
    "\n",
    "dataset = {\n",
    "    'images' : images_filtered,\n",
    "    'bboxes' : bboxes_filtered,\n",
    "    'cls' : labels_filtered\n",
    "}\n",
    "\n",
    "print(dataset['images'].shape)\n",
    "print(dataset['bboxes'].shape)\n",
    "print(dataset['cls'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels[9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n",
      "24\n",
      "24\n",
      "tf.Tensor(\n",
      "[[11.  0. 20.  4.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(4, 4), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "images = dataset['images']\n",
    "# numbers =dataset['numbers']\n",
    "bboxes = dataset['bboxes']\n",
    "cls = dataset['cls']\n",
    "\n",
    "boxes = bboxes[0]\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.axis('off')\n",
    "image = images[0]\n",
    "print(image.shape)\n",
    "print(image.shape[0])\n",
    "print(image.shape[0])\n",
    "plt.imshow(images[0])\n",
    "ax = plt.gca()\n",
    "boxes = tf.stack([\n",
    "\tboxes[:, 0] * images.shape[2],\n",
    "\tboxes[:, 1] * images.shape[1],\n",
    "\tboxes[:, 2] * images.shape[2],\n",
    "\tboxes[:, 3] * images.shape[1]], axis = -1\n",
    ")\n",
    "\n",
    "print(boxes)\n",
    "\n",
    "for box in boxes:\n",
    "\txmin, ymin = box[:2]\n",
    "\tw, h = box[2:] - box[:2]\n",
    "\tpatch = plt.Rectangle(\n",
    "\t\t[xmin, ymin], w, h, fill = False, edgecolor = [1, 0, 0], linewidth = 2\n",
    "\t)\n",
    "\tax.add_patch(patch)\n",
    "plt.show()\n",
    "print(cls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_SIZE_WIDTH:   32\n",
      "IMG_SIZE_HEIGHT:  24\n",
      "N_DATA:           12880\n",
      "N_TRAIN:          12880\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE_WIDTH = images.shape[2]\n",
    "IMG_SIZE_HEIGHT = images.shape[1]\n",
    "N_DATA = images.shape[0]\n",
    "N_TRAIN = images.shape[0]\n",
    "# N_VAL = images.shape[0] - N_TRAIN\n",
    "LOG_DIR = 'ObjectDetectionLog'\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'tfrecord/ObjectDetection')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "print(\"IMG_SIZE_WIDTH:  \", IMG_SIZE_WIDTH)\n",
    "print(\"IMG_SIZE_HEIGHT: \", IMG_SIZE_HEIGHT)\n",
    "print(\"N_DATA:          \", N_DATA)\n",
    "print(\"N_TRAIN:         \", N_TRAIN)\n",
    "\n",
    "\n",
    "\n",
    "shuffle_list = list(range(N_DATA))\n",
    "random.shuffle(shuffle_list)\n",
    "\n",
    "train_idx_list = shuffle_list[:N_TRAIN]\n",
    "# val_idx_list = shuffle_list[N_TRAIN:]\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'tfrecord/ObjectDetection')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "# tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "writer_train = tf.io.TFRecordWriter(tfr_train_dir)\n",
    "# writer_val = tf.io.TFRecordWriter(tfr_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.max(), images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list = tf.train.FloatList(value = value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list = tf.train.Int64List(value = [value]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in train_idx_list:\n",
    "    bbox = bboxes[idx]\n",
    "    xmin, ymin, xmax, ymax = bbox[:, 0], bbox[:, 1], bbox[:, 2], bbox[:, 3]\n",
    "    bbox = np.stack([xmin, ymin, xmax, ymax], axis=-1).flatten()\n",
    "\n",
    "    image = images[idx]\n",
    "    bimage = image.tobytes()\n",
    "\n",
    "    number = numbers[idx]\n",
    "    class_id = cls[idx]\n",
    "    # print(len(cls))\n",
    "    serialized_cls = tf.io.serialize_tensor(tf.constant(class_id, dtype=tf.int32)).numpy()\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image': _bytes_feature(bimage),\n",
    "        'bbox': _float_feature(bbox),\n",
    "        'label': _bytes_feature(serialized_cls),\n",
    "        # 'number': _int64_feature(number)\n",
    "    }))\n",
    "    \n",
    "    writer_train.write(example.SerializeToString())\n",
    "writer_train.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "RES_HEIGHT =24\n",
    "RES_WIDTH = 32\n",
    "# N_EPOCHS = 100\n",
    "# N_BATCH = 8\n",
    "# LR = 0.0005\n",
    "\n",
    "\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    features = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'bbox': tf.io.VarLenFeature(tf.float32),  \n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'number': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "\n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, [RES_HEIGHT, RES_WIDTH, 1])\n",
    "    image = image / tf.reduce_max(image)\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    # image = image / tf.reduce_max(image)\n",
    "\n",
    "    bbox = tf.sparse.to_dense(parsed_features['bbox']) \n",
    "    bbox = tf.cast(bbox, tf.float32)\n",
    "    # num_boxes = tf.shape(bbox)[0] // 4\n",
    "    bbox = tf.reshape(bbox, [-1, 4])\n",
    "\n",
    "    serialized_cls = parsed_features['label']\n",
    "    label = tf.io.parse_tensor(serialized_cls, out_type=tf.int32)\n",
    "    \n",
    "    # number = tf.cast(parsed_features['number'], tf.int64)\n",
    "    return image, bbox, label\n",
    "\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(AUTOTUNE).batch(N_BATCH, drop_remainder=True)\n",
    "\n",
    "# val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
    "# val_dataset = val_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    image = image.numpy()\n",
    "    print(image.shape)\n",
    "    # print(label)\n",
    "    # # plt.axis('off')\n",
    "    # plt.imshow(image)\n",
    "    # ax = plt.gca()  \n",
    "    # print(bbox)\n",
    "\n",
    "    # boxes = tf.stack(\n",
    "    # \t[\n",
    "    # \t bbox[:,0] * image.shape[1],\n",
    "    # \t bbox[:,1] * image.shape[0],\n",
    "    # \t bbox[:,2] * image.shape[1],\n",
    "    # \t bbox[:,3] * image.shape[0]\n",
    "    # \t], axis = -1\n",
    "    # )\n",
    "    # print(image.shape)\n",
    "    # print(boxes)\n",
    "    # for box in boxes:\n",
    "    #     xmin, ymin = box[:2]\n",
    "    #     w, h = box[2:] - box[:2]\n",
    "    #     patch = plt.Rectangle(\n",
    "    #         [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "    #     )\n",
    "    #     ax.add_patch(patch)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):    \n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_corners(boxes):\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.78125    0.20833333 1.         0.5416667 ]\n",
      " [0.         0.5416667  0.28125    0.7916667 ]\n",
      " [0.         0.75       0.21875    1.        ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.890625   0.375      0.21875    0.33333337]\n",
      " [0.140625   0.6666667  0.28125    0.25      ]\n",
      " [0.109375   0.875      0.21875    0.25      ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    print(bbox)\n",
    "    print(convert_to_xywh(bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_image(image, min_side=96, max_side=128):\n",
    "    \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype = tf.float32)\n",
    "    # print(f\"image_shape: {image_shape}\")\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    # print(f\"ratio: {ratio}\")\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "      ratio = max_side / tf.reduce_max(image_shape)\n",
    "    # print(f\"ratio: {ratio}\")\n",
    "\n",
    "    new_image_shape = ratio * image_shape\n",
    "    # print(f\"new_image_shape: {new_image_shape}\")\n",
    "\n",
    "    image = tf.image.resize(image, \n",
    "                            tf.cast(new_image_shape, dtype=tf.int32), \n",
    "                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # print(f\"image: {image.shape}\")\n",
    "\n",
    "    return image, new_image_shape, ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image, gt_boxes, cls_ids):\n",
    "    # image = sample[\"images\"]\n",
    "    # bbox = sample[\"bboxes\"]\n",
    "    cls_ids = tf.cast(cls_ids, dtype = tf.int32)\n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "    \n",
    "    bbox = tf.stack([\n",
    "        gt_boxes[:, 0] * image_shape[1],\n",
    "        gt_boxes[:, 1] * image_shape[0],\n",
    "        gt_boxes[:, 2] * image_shape[1],\n",
    "        gt_boxes[:, 3] * image_shape[0]],\n",
    "        axis = -1\n",
    "    )\n",
    "    \n",
    "    bbox = convert_to_xywh(bbox)\n",
    "    print(image.shape)\n",
    "    print(bbox.shape)\n",
    "    print(cls_ids.shape)\n",
    "    return image, bbox, cls_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    resize_and_pad_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 1 0], shape=(4,), dtype=int32)\n",
      "(96, 128, 1)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(96, 128, 1)\n",
      "(96, 128, 3)\n",
      "tf.Tensor(\n",
      "[[114.  36.  28.  32.]\n",
      " [ 18.  64.  36.  24.]\n",
      " [ 14.  84.  28.  24.]\n",
      " [  0.   0.   0.   0.]], shape=(4, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.02834008, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    print(label)\n",
    "    img, bbox, class_id = preprocess_data(image, bbox, label)\n",
    "    print(img.shape) # (72, 96, 1)\n",
    "\n",
    "    anchor_img = np.zeros((*img.shape[:2], 3), dtype=np.uint8)\n",
    "    print(anchor_img.shape)\n",
    "\n",
    "    print(bbox)\n",
    "\n",
    "    strides = [4, 8, 16, 32]\n",
    "    colors = {\n",
    "        4: [0, 255, 0],  # 초록색\n",
    "        8: [0, 0, 255],  # 파란색\n",
    "        16: [255, 0, 0],   # 빨간색\n",
    "        32:[255, 255, 255],  # 노란색\n",
    "        # 64:[255, 255, 0],  # 노란색\n",
    "    }\n",
    "\n",
    "    for stride in strides:\n",
    "        color = colors[stride]\n",
    "        for y in range(0, anchor_img.shape[0], stride):\n",
    "            for x in range(0, anchor_img.shape[1], stride):\n",
    "                anchor_img[y, x, :] = color\n",
    "\n",
    "    # 이미지 표시\n",
    "    plt.imshow(img, alpha=1)  \n",
    "    plt.imshow(anchor_img, alpha=0.5) \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 128, 1)\n",
      "(4, 4)\n",
      "(4,)\n",
      "tf.Tensor([1 1 1 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[114.  36.  28.  32.]\n",
      " [ 18.  64.  36.  24.]\n",
      " [ 14.  84.  28.  24.]\n",
      " [  0.   0.   0.   0.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.02834008, shape=(), dtype=float32)\n",
      "width:  128\n",
      "height:  96\n",
      "bbox:  tf.Tensor(\n",
      "[[100.  20. 128.  52.]\n",
      " [  0.  52.  36.  76.]\n",
      " [  0.  72.  28.  96.]\n",
      " [  0.   0.   0.   0.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor([ 0. 72. 28. 96.], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 1 0], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    img, box, label = preprocess_data(image, bbox, label)\n",
    "    print(label)\n",
    "    print(box)\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))\n",
    "    # 이미지 시각화\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    print(\"width: \", width)\n",
    "    print(\"height: \", height)\n",
    "    boxes = tf.stack(\n",
    "        [\n",
    "            (box[:, 0] - 0.5 * box[:, 2])  ,  # xmin = x_center - width/2\n",
    "            (box[:, 1] - 0.5 * box[:, 3])  ,  # ymin = y_center - height/2\n",
    "            (box[:, 0] + 0.5 * box[:, 2])  ,  # xmax = x_center + width/2\n",
    "            (box[:, 1] + 0.5 * box[:, 3])     # ymax = y_center + height/2\n",
    "        ], axis=-1\n",
    "    )\n",
    "    print(\"bbox: \", boxes)\n",
    "    # 각 바운딩 박스에 대해 반복하여 그리기\n",
    "    for box in boxes[2:3]:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        # print(\"xmin, ymin: \", xmin, ymin)\n",
    "        print(box)\n",
    "        w, h = xmax - xmin, ymax - ymin\n",
    "        patch = plt.Rectangle(\n",
    "            [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    plt.show()\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBox:\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]        \n",
    "        self.scales = [2** x for x in [0, 1/3, 2/3]]\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(2, 10)]\n",
    "        self._areas = [x ** 2 for x in [4.0, 8.0, 16.0, 32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        anchor_dims_all = []\n",
    "\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios: \n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis = -1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales: \n",
    "                    anchor_dims.append(scale * dims) \n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis = -2))\n",
    "        return anchor_dims_all \n",
    "    \n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        rx = tf.range(feature_width, dtype = tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype = tf.float32) + 0.5\n",
    "\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis = -1) * self._strides[level - 2]\n",
    "        # print(f\"centers: {centers}\")\n",
    "        centers = tf.expand_dims(centers, axis = -2)\n",
    "        # print(f\"centers: {centers}\")\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        # print(f\"centers: {centers}\")\n",
    "\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 2], [feature_height, feature_width, 1, 1] \n",
    "        )\n",
    "        # print(f\"dims: {dims}\")\n",
    "\n",
    "        \n",
    "        anchors = tf.concat([centers, dims], axis=-1) \n",
    "        # print(f\"anchors: {anchors}\")\n",
    "\n",
    "        # print(f\"{tf.reshape(anchors, [feature_height * feature_width * self._num_anchors, 4]).shape}\")\n",
    "\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i), # 올림\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i\n",
    "            )\n",
    "            for i in range(2, 10)\n",
    "        ]\n",
    "\n",
    "        anchors = tf.concat(anchors, axis=0)\n",
    "\n",
    "        # 앵커 박스의 좌표를 이미지 크기 내로 제한\n",
    "        anchors = tf.clip_by_value(anchors, 0, [image_height, image_width, image_height, image_width])\n",
    "        return tf.concat(anchors, axis=0)\n",
    "        # return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor 음수 값: False\n",
      "(24648, 4)\n",
      "[[36.        12.        10.583005   6.0474315]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anchors = AnchorBox()\n",
    "anchor = anchors.get_anchors(96, 128)\n",
    "\n",
    "has_negative_values = tf.reduce_any(tf.less(anchor, 0))\n",
    "print(\"Anchor 음수 값:\", has_negative_values.numpy())\n",
    "\n",
    "# print(anchor)\n",
    "print(anchor.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bounding_boxes(data, num_samples):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    # print(img.shape)\n",
    "    data_np = data.numpy()\n",
    "\n",
    "    if len(data) > num_samples:\n",
    "        sampled_indices = np.random.choice(len(data), num_samples, replace=False)\n",
    "        sample_data = data_np[sampled_indices]\n",
    "    else : \n",
    "        sample_data = data_np\n",
    "    print(sample_data)\n",
    "    for center_x, center_y, width, height in sample_data:\n",
    "        top_left_x = center_x - width / 2\n",
    "        top_left_y = center_y - height / 2\n",
    "\n",
    "        rect = patches.Rectangle((top_left_x, top_left_y), width, height, linewidth=0.8, edgecolor='white', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "draw_bounding_boxes(anchor, 1)\n",
    "\n",
    "\n",
    "# 24192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    print(\"boxes1: \", boxes1)\n",
    "    print(\"boxes1_corners: \", boxes1_corners)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    print(\"boxes2: \", boxes2)\n",
    "    print(\"boxes2_corners: \", boxes2_corners)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    print(\"lu: \", lu)\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])  \n",
    "    print(\"rd: \", rd)\n",
    "\n",
    "    \n",
    "\n",
    "    intersection = tf.maximum(rd - lu, 0.0)\n",
    "    print(\"intersection: \", intersection)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    print(\"intersection_area: \", intersection_area)\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8)\n",
    "    print(\"union_area: \", union_area)\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[62. 54. 78. 70.]], shape=(1, 4), dtype=float64)\n",
      "tf.Tensor([[60. 52. 92. 92.]], shape=(1, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "b1 = np.array([[70, 62, 16, 16]]) \n",
    "b1c = convert_to_corners(b1)\n",
    "print(b1c)\n",
    "\n",
    "b2 = np.array([[76, 72, 32, 40]])\n",
    "b2c = convert_to_corners(b2)\n",
    "print(b2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# 주어진 바운딩 박스 데이터\n",
    "box1 = [62, 54, 78, 70]  # [x_min, y_min, x_max, y_max]\n",
    "box2 = [60, 52, 82, 82]\n",
    "\n",
    "# 그림 생성\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 첫 번째 바운딩 박스 추가\n",
    "rect1 = patches.Rectangle((box1[0], box1[1]), box1[2] - box1[0], box1[3] - box1[1], \n",
    "                          linewidth=2, edgecolor='blue', facecolor='none')\n",
    "ax.add_patch(rect1)\n",
    "\n",
    "# 두 번째 바운딩 박스 추가\n",
    "rect2 = patches.Rectangle((box2[0], box2[1]), box2[2] - box2[0], box2[3] - box2[1], \n",
    "                          linewidth=2, edgecolor='red', facecolor='none')\n",
    "ax.add_patch(rect2)\n",
    "\n",
    "# 축 범위 설정\n",
    "ax.set_xlim(0, 90)\n",
    "ax.set_ylim(0, 90)\n",
    "\n",
    "# 그림 표시\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes1:  [[70 62 16 16]]\n",
      "boxes1_corners:  tf.Tensor([[62. 54. 78. 70.]], shape=(1, 4), dtype=float64)\n",
      "boxes2:  [[76 72 32 40]]\n",
      "boxes2_corners:  tf.Tensor([[60. 52. 92. 92.]], shape=(1, 4), dtype=float64)\n",
      "lu:  tf.Tensor([[[62. 54.]]], shape=(1, 1, 2), dtype=float64)\n",
      "rd:  tf.Tensor([[[78. 70.]]], shape=(1, 1, 2), dtype=float64)\n",
      "intersection:  tf.Tensor([[[16. 16.]]], shape=(1, 1, 2), dtype=float64)\n",
      "intersection_area:  tf.Tensor([[256.]], shape=(1, 1), dtype=float64)\n",
      "union_area:  tf.Tensor([[1280.]], shape=(1, 1), dtype=float64)\n",
      "tf.Tensor([[0.2]], shape=(1, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "GA = np.array([[70, 62, 16, 16]])\n",
    "GT = np.array([[76, 72, 32, 40]])\n",
    "\n",
    "print(compute_iou(GA, GT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    \n",
    "    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou = 0.5, ignore_iou = 0.4):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        print(\"iou_matrix:  \", iou_matrix)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        print(\"max_iou:  \", max_iou)\n",
    "\n",
    "\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis = 1)\n",
    "        print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "\n",
    "    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        print(\"positive_mask:  \", positive_mask)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        print(\"negative_mask:  \", negative_mask)\n",
    "\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        print(\"ignore_mask:  \", ignore_mask)\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype = tf.float32),\n",
    "            tf.cast(ignore_mask, dtype = tf.float32),\n",
    "        )\n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:])\n",
    "            ],\n",
    "            axis = -1,\n",
    "        )\n",
    "        print(\"box_target:  \", box_target)\n",
    "        box_target = box_target / self._box_variance\n",
    "        print(\"box_target:  \", box_target)\n",
    "        return box_target\n",
    "    \n",
    "\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        print(\"anchor_boxes  : \", anchor_boxes)\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        print(\"cls_ids\", cls_ids)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "        print(\"positive_mask:  \", positive_mask)\n",
    "        print(\"ignore_mask:  \", ignore_mask)\n",
    "\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "\n",
    "        print(\"matched_gt_boxes:  \", matched_gt_boxes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        print(\"box_target:  \", box_target)\n",
    "\n",
    "\n",
    "\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        print(\"matched_gt_cls_ids:  \", matched_gt_cls_ids)\n",
    "        \n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
    "        )\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "\n",
    "\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        print(\"label:  \", label)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):       \n",
    "        images_shape = tf.shape(batch_images)\n",
    "        print(\"images_shape:  \", images_shape)\n",
    "        batch_size = images_shape[0]\n",
    "        print(\"batch_size:  \", batch_size)\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        print(\"labels:  \", labels)\n",
    "        # batch_size_val = batch_size.numpy()\n",
    "        for i in range(1):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            print(\"label:  \", label)\n",
    "            labels = labels.write(i, label)\n",
    "        return batch_images, labels.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Eager execution: \", tf.executing_eagerly())\n",
    "if not tf.executing_eagerly():\n",
    "    tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 128, 1)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(1, 96, 128, 1) (1, 4, 4) (1, 4)\n",
      "images_shape:   tf.Tensor([  1  96 128   1], shape=(4,), dtype=int32)\n",
      "batch_size:   tf.Tensor(1, shape=(), dtype=int32)\n",
      "labels:   <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcb083df100>\n",
      "anchor_boxes  :  tf.Tensor(\n",
      "[[  2.          2.          2.          8.       ]\n",
      " [  2.          2.          2.5198421  10.079369 ]\n",
      " [  2.          2.          3.174802   12.699208 ]\n",
      " ...\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]], shape=(24648, 4), dtype=float32)\n",
      "cls_ids tf.Tensor([1. 1. 1. 0.], shape=(4,), dtype=float32)\n",
      "boxes1:  tf.Tensor(\n",
      "[[  2.          2.          2.          8.       ]\n",
      " [  2.          2.          2.5198421  10.079369 ]\n",
      " [  2.          2.          3.174802   12.699208 ]\n",
      " ...\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]], shape=(24648, 4), dtype=float32)\n",
      "boxes1_corners:  tf.Tensor(\n",
      "[[  1.          -2.           3.           6.        ]\n",
      " [  0.7400789   -3.0396843    3.259921     7.0396843 ]\n",
      " [  0.41259897  -4.349604     3.587401     8.349604  ]\n",
      " ...\n",
      " [ 48.          64.         144.         192.        ]\n",
      " [ 48.          64.         144.         192.        ]\n",
      " [ 48.          64.         144.         192.        ]], shape=(24648, 4), dtype=float32)\n",
      "boxes2:  [[114.  36.  28.  32.]\n",
      " [ 18.  64.  36.  24.]\n",
      " [ 14.  84.  28.  24.]\n",
      " [  0.   0.   0.   0.]]\n",
      "boxes2_corners:  tf.Tensor(\n",
      "[[100.  20. 128.  52.]\n",
      " [  0.  52.  36.  76.]\n",
      " [  0.  72.  28.  96.]\n",
      " [  0.   0.   0.   0.]], shape=(4, 4), dtype=float32)\n",
      "lu:  tf.Tensor(\n",
      "[[[100.          20.        ]\n",
      "  [  1.          52.        ]\n",
      "  [  1.          72.        ]\n",
      "  [  1.           0.        ]]\n",
      "\n",
      " [[100.          20.        ]\n",
      "  [  0.7400789   52.        ]\n",
      "  [  0.7400789   72.        ]\n",
      "  [  0.7400789    0.        ]]\n",
      "\n",
      " [[100.          20.        ]\n",
      "  [  0.41259897  52.        ]\n",
      "  [  0.41259897  72.        ]\n",
      "  [  0.41259897   0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[100.          64.        ]\n",
      "  [ 48.          64.        ]\n",
      "  [ 48.          72.        ]\n",
      "  [ 48.          64.        ]]\n",
      "\n",
      " [[100.          64.        ]\n",
      "  [ 48.          64.        ]\n",
      "  [ 48.          72.        ]\n",
      "  [ 48.          64.        ]]\n",
      "\n",
      " [[100.          64.        ]\n",
      "  [ 48.          64.        ]\n",
      "  [ 48.          72.        ]\n",
      "  [ 48.          64.        ]]], shape=(24648, 4, 2), dtype=float32)\n",
      "rd:  tf.Tensor(\n",
      "[[[  3.          6.       ]\n",
      "  [  3.          6.       ]\n",
      "  [  3.          6.       ]\n",
      "  [  0.          0.       ]]\n",
      "\n",
      " [[  3.259921    7.0396843]\n",
      "  [  3.259921    7.0396843]\n",
      "  [  3.259921    7.0396843]\n",
      "  [  0.          0.       ]]\n",
      "\n",
      " [[  3.587401    8.349604 ]\n",
      "  [  3.587401    8.349604 ]\n",
      "  [  3.587401    8.349604 ]\n",
      "  [  0.          0.       ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[128.         52.       ]\n",
      "  [ 36.         76.       ]\n",
      "  [ 28.         96.       ]\n",
      "  [  0.          0.       ]]\n",
      "\n",
      " [[128.         52.       ]\n",
      "  [ 36.         76.       ]\n",
      "  [ 28.         96.       ]\n",
      "  [  0.          0.       ]]\n",
      "\n",
      " [[128.         52.       ]\n",
      "  [ 36.         76.       ]\n",
      "  [ 28.         96.       ]\n",
      "  [  0.          0.       ]]], shape=(24648, 4, 2), dtype=float32)\n",
      "intersection:  tf.Tensor(\n",
      "[[[ 0.         0.       ]\n",
      "  [ 2.         0.       ]\n",
      "  [ 2.         0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[ 0.         0.       ]\n",
      "  [ 2.5198421  0.       ]\n",
      "  [ 2.5198421  0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[ 0.         0.       ]\n",
      "  [ 3.1748018  0.       ]\n",
      "  [ 3.1748018  0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[28.         0.       ]\n",
      "  [ 0.        12.       ]\n",
      "  [ 0.        24.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[28.         0.       ]\n",
      "  [ 0.        12.       ]\n",
      "  [ 0.        24.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[28.         0.       ]\n",
      "  [ 0.        12.       ]\n",
      "  [ 0.        24.       ]\n",
      "  [ 0.         0.       ]]], shape=(24648, 4, 2), dtype=float32)\n",
      "intersection_area:  tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(24648, 4), dtype=float32)\n",
      "union_area:  tf.Tensor(\n",
      "[[  912.         880.         688.          16.      ]\n",
      " [  921.39844    889.39844    697.39844     25.398418]\n",
      " [  936.3175     904.3175     712.3175      40.317474]\n",
      " ...\n",
      " [13184.       13152.       12960.       12288.      ]\n",
      " [13184.       13152.       12960.       12288.      ]\n",
      " [13184.       13152.       12960.       12288.      ]], shape=(24648, 4), dtype=float32)\n",
      "iou_matrix:   tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(24648, 4), dtype=float32)\n",
      "max_iou:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(24648,), dtype=float32)\n",
      "matched_gt_idx:   tf.Tensor([0 0 0 ... 0 0 0], shape=(24648,), dtype=int64)\n",
      "positive_mask:   tf.Tensor([False False False ... False False False], shape=(24648,), dtype=bool)\n",
      "negative_mask:   tf.Tensor([ True  True  True ...  True  True  True], shape=(24648,), dtype=bool)\n",
      "ignore_mask:   tf.Tensor([False False False ... False False False], shape=(24648,), dtype=bool)\n",
      "matched_gt_idx:   tf.Tensor([0 0 0 ... 0 0 0], shape=(24648,), dtype=int64)\n",
      "positive_mask:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(24648,), dtype=float32)\n",
      "ignore_mask:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(24648,), dtype=float32)\n",
      "matched_gt_boxes:   tf.Tensor(\n",
      "[[114.  36.  28.  32.]\n",
      " [114.  36.  28.  32.]\n",
      " [114.  36.  28.  32.]\n",
      " ...\n",
      " [114.  36.  28.  32.]\n",
      " [114.  36.  28.  32.]\n",
      " [114.  36.  28.  32.]], shape=(24648, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[56.          4.25        2.6390574   1.3862944 ]\n",
      " [44.447227    3.3732271   2.408008    1.1552453 ]\n",
      " [35.27779     2.6773322   2.1769593   0.92419624]\n",
      " ...\n",
      " [ 0.1875     -0.71875    -1.2321438  -1.3862944 ]\n",
      " [ 0.1875     -0.71875    -1.2321438  -1.3862944 ]\n",
      " [ 0.1875     -0.71875    -1.2321438  -1.3862944 ]], shape=(24648, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[560.         42.5        13.195287    6.931472 ]\n",
      " [444.47226    33.73227    12.04004     5.7762265]\n",
      " [352.7779     26.773321   10.884796    4.620981 ]\n",
      " ...\n",
      " [  1.875      -7.1875     -6.160719   -6.931472 ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472 ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472 ]], shape=(24648, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[560.         42.5        13.195287    6.931472 ]\n",
      " [444.47226    33.73227    12.04004     5.7762265]\n",
      " [352.7779     26.773321   10.884796    4.620981 ]\n",
      " ...\n",
      " [  1.875      -7.1875     -6.160719   -6.931472 ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472 ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472 ]], shape=(24648, 4), dtype=float32)\n",
      "matched_gt_cls_ids:   tf.Tensor([1. 1. 1. ... 1. 1. 1.], shape=(24648,), dtype=float32)\n",
      "cls_target:   tf.Tensor([-1. -1. -1. ... -1. -1. -1.], shape=(24648,), dtype=float32)\n",
      "cls_target:   tf.Tensor([-1. -1. -1. ... -1. -1. -1.], shape=(24648,), dtype=float32)\n",
      "cls_target:   tf.Tensor(\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]], shape=(24648, 1), dtype=float32)\n",
      "label:   tf.Tensor(\n",
      "[[560.         42.5        13.195287    6.931472   -1.       ]\n",
      " [444.47226    33.73227    12.04004     5.7762265  -1.       ]\n",
      " [352.7779     26.773321   10.884796    4.620981   -1.       ]\n",
      " ...\n",
      " [  1.875      -7.1875     -6.160719   -6.931472   -1.       ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472   -1.       ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472   -1.       ]], shape=(24648, 5), dtype=float32)\n",
      "label:   tf.Tensor(\n",
      "[[560.         42.5        13.195287    6.931472   -1.       ]\n",
      " [444.47226    33.73227    12.04004     5.7762265  -1.       ]\n",
      " [352.7779     26.773321   10.884796    4.620981   -1.       ]\n",
      " ...\n",
      " [  1.875      -7.1875     -6.160719   -6.931472   -1.       ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472   -1.       ]\n",
      " [  1.875      -7.1875     -6.160719   -6.931472   -1.       ]], shape=(24648, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    img, box, label = preprocess_data(image, bbox, label)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    box = np.expand_dims(box, axis = 0)\n",
    "    label = np.expand_dims(label, axis = 0)\n",
    "\n",
    "    print(img.shape, box.shape, label.shape)\n",
    "    label_encoder.encode_batch(img, box, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_axis(img, box, label):\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    box = np.expand_dims(box, axis = 0)\n",
    "    label = np.expand_dims(label, axis = 0)\n",
    "    return img, box, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 1)\n",
      "(None, 4)\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "autotune = tf.data.AUTOTUNE\n",
    "num_classes = 1\n",
    "batch_size = 2\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# train_dataset = train_dataset.shuffle(batch_size * 8)\n",
    "\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "    batch_size=batch_size, \n",
    "    padded_shapes = ([96, 128, 1], [4, 4], [4]),\n",
    "    padding_values=(0.0, 1e-8, -1), \n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "\n",
    "# val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# val_dataset = val_dataset.padded_batch(\n",
    "#     batch_size=batch_size, \n",
    "#     padded_shapes = ([96, 128, 1], [4, 4], [4]),\n",
    "#     padding_values=(0.0, 1e-8, 0), \n",
    "#     drop_remainder=True\n",
    "# )\n",
    "# val_dataset = val_dataset.map(\n",
    "#     label_encoder.encode_batch, num_parallel_calls=autotune\n",
    "# )\n",
    "\n",
    "# val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_shape:   Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "batch_size:   Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "labels:   <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcb0830fc70>\n",
      "anchor_boxes  :  Tensor(\"concat_9/concat:0\", shape=(24648, 4), dtype=float32)\n",
      "cls_ids Tensor(\"Cast:0\", shape=(4,), dtype=float32)\n",
      "boxes1:  Tensor(\"concat_9/concat:0\", shape=(24648, 4), dtype=float32)\n",
      "boxes1_corners:  Tensor(\"concat_10:0\", shape=(24648, 4), dtype=float32)\n",
      "boxes2:  Tensor(\"strided_slice_1:0\", shape=(4, 4), dtype=float32)\n",
      "boxes2_corners:  Tensor(\"concat_11:0\", shape=(4, 4), dtype=float32)\n",
      "lu:  Tensor(\"Maximum:0\", shape=(24648, 4, 2), dtype=float32)\n",
      "rd:  Tensor(\"Minimum:0\", shape=(24648, 4, 2), dtype=float32)\n",
      "intersection:  Tensor(\"Maximum_1:0\", shape=(24648, 4, 2), dtype=float32)\n",
      "intersection_area:  Tensor(\"mul_24:0\", shape=(24648, 4), dtype=float32)\n",
      "union_area:  Tensor(\"Maximum_2:0\", shape=(24648, 4), dtype=float32)\n",
      "iou_matrix:   Tensor(\"clip_by_value_1:0\", shape=(24648, 4), dtype=float32)\n",
      "max_iou:   Tensor(\"Max:0\", shape=(24648,), dtype=float32)\n",
      "matched_gt_idx:   Tensor(\"ArgMax:0\", shape=(24648,), dtype=int64)\n",
      "positive_mask:   Tensor(\"GreaterEqual:0\", shape=(24648,), dtype=bool)\n",
      "negative_mask:   Tensor(\"Less:0\", shape=(24648,), dtype=bool)\n",
      "ignore_mask:   Tensor(\"LogicalNot:0\", shape=(24648,), dtype=bool)\n",
      "matched_gt_idx:   Tensor(\"ArgMax:0\", shape=(24648,), dtype=int64)\n",
      "positive_mask:   Tensor(\"Cast_1:0\", shape=(24648,), dtype=float32)\n",
      "ignore_mask:   Tensor(\"Cast_2:0\", shape=(24648,), dtype=float32)\n",
      "matched_gt_boxes:   Tensor(\"GatherV2:0\", shape=(24648, 4), dtype=float32)\n",
      "box_target:   Tensor(\"concat_12:0\", shape=(24648, 4), dtype=float32)\n",
      "box_target:   Tensor(\"truediv_23:0\", shape=(24648, 4), dtype=float32)\n",
      "box_target:   Tensor(\"truediv_23:0\", shape=(24648, 4), dtype=float32)\n",
      "matched_gt_cls_ids:   Tensor(\"GatherV2_1:0\", shape=(24648,), dtype=float32)\n",
      "cls_target:   Tensor(\"SelectV2:0\", shape=(24648,), dtype=float32)\n",
      "cls_target:   Tensor(\"SelectV2_1:0\", shape=(24648,), dtype=float32)\n",
      "cls_target:   Tensor(\"ExpandDims_8:0\", shape=(24648, 1), dtype=float32)\n",
      "label:   Tensor(\"concat_13:0\", shape=(24648, 5), dtype=float32)\n",
      "label:   Tensor(\"concat_13:0\", shape=(24648, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
    ")\n",
    "# train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24648, 5)\n",
      "Positive 개수: 5\n",
      "Negative 개수: 24623\n",
      "Ignore 개수: 20\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "ignore_count = []\n",
    "for batch in train_dataset.take(1):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    print(labels.shape)\n",
    "\n",
    "    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 1.0), tf.int32))\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "#     print(\"원본 레이블 shape:\", labels.shape)\n",
    "\n",
    "#     # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "#     mask = tf.not_equal(labels[:, :, 4], -2.0)\n",
    "#     filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "#     # 필터링된 레이블을 원하는 크기로 조절\n",
    "#     # 예: 36288개로 조절\n",
    "#     desired_count = 36288\n",
    "#     filtered_labels = filtered_labels[:desired_count, :]\n",
    "#     print(\"조정된 레이블 shape:\", filtered_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cut_labels(images, labels):\n",
    "#     # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "#     mask = tf.not_equal(labels[:, 4], -2.0)\n",
    "#     filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "#     # 필터링된 레이블을 원하는 크기로 조절\n",
    "#     desired_count = 36288\n",
    "#     # 주의: 여기서 filtered_labels의 크기가 desired_count보다 작을 수 있으므로, 적절한 처리가 필요합니다.\n",
    "#     if tf.shape(filtered_labels)[0] > desired_count:\n",
    "#         filtered_labels = filtered_labels[:desired_count, :]\n",
    "\n",
    "#     return images, filtered_labels\n",
    "\n",
    "\n",
    "def cut_labels(images, labels):\n",
    "    # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "    mask = tf.not_equal(labels[:, 4], -2.0)\n",
    "    filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "    # 필터링된 레이블을 원하는 크기로 조절\n",
    "    desired_count = 24192\n",
    "    current_count = tf.shape(filtered_labels)[0]\n",
    "\n",
    "    # 필요한 경우 레이블을 패딩\n",
    "    if current_count < desired_count:\n",
    "        # 누락된 개수만큼 ignore 상태(-2.0)로 패딩\n",
    "        padding_count = desired_count - current_count\n",
    "        padding = tf.fill([padding_count, 5], -2.0)  # -2.0으로 채워진 텐서 생성\n",
    "        filtered_labels = tf.concat([filtered_labels, padding], axis=0)\n",
    "    else:\n",
    "        filtered_labels = filtered_labels[:desired_count, :]\n",
    "\n",
    "    return images, filtered_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 원본 train_dataset에 process_labels 함수 적용\n",
    "train_dataset = train_dataset.unbatch().map(cut_labels).batch(batch_size)\n",
    "# val_dataset = val_dataset.unbatch().map(cut_labels).batch(batch_size)\n",
    "\n",
    "# buffer_size = 1000\n",
    "# 필요한 경우 배치 크기, 셔플, 반복 등을 적용\n",
    "# train_dataset = new_train_dataset.batch(batch_size).shuffle(buffer_size).repeat()\n",
    "# val_dataset = new_val_dataset.batch(batch_size).shuffle(buffer_size).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(560.0, shape=(), dtype=float32) tf.Tensor(-205.06097, shape=(), dtype=float32)\n",
      "Positive 개수: 1\n",
      "Negative 개수: 24191\n",
      "Ignore 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "ignore_count = []\n",
    "for batch in train_dataset.take(1):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    labels[:, :, 4:]\n",
    "    print(tf.reduce_max(images), tf.reduce_min(images))\n",
    "    print(tf.reduce_max(labels), tf.reduce_min(labels))\n",
    "\n",
    "    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 1.0), tf.int32))\n",
    "    \n",
    "    # pad_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 2.0), tf.int32))\n",
    "\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n",
    "    # print(\"Pad 개수:\", pad_count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 09:16:37.044354: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 15 of dimension 0 out of bounds.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 15 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m positive_boxes \u001b[38;5;241m=\u001b[39m image_labels[tf\u001b[38;5;241m.\u001b[39mequal(image_labels[:, \u001b[38;5;241m4\u001b[39m], \u001b[38;5;241m1.0\u001b[39m)]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mshape(positive_boxes)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Positive 바운딩 박스가 있는 경우에만\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     first_box \u001b[38;5;241m=\u001b[39m \u001b[43mpositive_boxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# 첫 번째 바운딩 박스\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m     22\u001b[0m     [\n\u001b[1;32m     23\u001b[0m         (first_box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m first_box[\u001b[38;5;241m2\u001b[39m])  ,  \u001b[38;5;66;03m# xmin = x_center - width/2\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         (first_box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m first_box[\u001b[38;5;241m3\u001b[39m])     \u001b[38;5;66;03m# ymax = y_center + height/2\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m     x_center, y_center, width, height \u001b[38;5;241m=\u001b[39m boxes[:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 15 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "for batch in train_dataset.take(1):\n",
    "    images, labels = batch\n",
    "    \n",
    "    # 첫 번째 이미지를 선택\n",
    "    image = images[0]\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    # 첫 번째 이미지에 대한 레이블\n",
    "    image_labels = labels[0]\n",
    "\n",
    "    # 첫 번째 Positive 바운딩 박스 추출\n",
    "    positive_boxes = image_labels[tf.equal(image_labels[:, 4], 1.0)]\n",
    "    if tf.shape(positive_boxes)[0] > 0:  # Positive 바운딩 박스가 있는 경우에만\n",
    "        first_box = positive_boxes[15]  # 첫 번째 바운딩 박스\n",
    "        \n",
    "        boxes = tf.stack(\n",
    "        [\n",
    "            (first_box[0] - 0.5 * first_box[2])  ,  # xmin = x_center - width/2\n",
    "            (first_box[1] - 0.5 * first_box[3])  ,  # ymin = y_center - height/2\n",
    "            (first_box[0] + 0.5 * first_box[2])  ,  # xmax = x_center + width/2\n",
    "            (first_box[1] + 0.5 * first_box[3])     # ymax = y_center + height/2\n",
    "        ], axis=-1)\n",
    "\n",
    "\n",
    "        x_center, y_center, width, height = boxes[:4]\n",
    "        # 상대 좌표를 절대 픽셀 값으로 변환\n",
    "        x_min = (x_center - width / 2) \n",
    "        y_min = (y_center - height / 2) \n",
    "\n",
    "        print(x_min)\n",
    "        print(y_min)\n",
    "        print(width)\n",
    "        print(height)\n",
    "\n",
    "        # 이미지 표시\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(image.numpy())\n",
    "\n",
    "        # 첫 번째 바운딩 박스 그리기\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Positive 바운딩 박스가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "    \n",
    "#     # 첫 번째 이미지를 선택\n",
    "#     image = images[0]\n",
    "#     image_height, image_width, _ = image.shape\n",
    "\n",
    "#     # 첫 번째 이미지에 대한 레이블\n",
    "#     image_labels = labels[0]\n",
    "\n",
    "#     # Positive 바운딩 박스 추출\n",
    "#     positive_boxes = image_labels[tf.equal(image_labels[:, 4], 1.0)]\n",
    "\n",
    "#     # 이미지 표시\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image.numpy())\n",
    "\n",
    "#     # Positive 바운딩 박스를 이미지에 그림\n",
    "#     for box in positive_boxes:\n",
    "#         x_center, y_center, width, height = box[:4]\n",
    "#         # 상대 좌표를 절대 픽셀 값으로 변환\n",
    "#         x_min = (x_center - width / 2) * image_width\n",
    "#         y_min = (y_center - height / 2) * image_height\n",
    "#         width *= image_width\n",
    "#         height *= image_height\n",
    "\n",
    "#         # 바운딩 박스 그리기\n",
    "#         rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Conv2D, Conv2DTranspose, Flatten, Activation\n",
    "from keras.layers import BatchNormalization, Dropout, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Add\n",
    "\n",
    "class BackBone:\n",
    "    def __init__(self):\n",
    "        self.l2_regularizer = l2(0.001)\n",
    "\n",
    "    def residual_layer(self, feature_map, latent, name:str):\n",
    "        add_layer = Add(name = name+'_output')([feature_map, latent])\n",
    "        return add_layer\n",
    "\n",
    "    def feature_extraction_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        feature_map = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name)(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        feature_map = Activation('relu')(feature_map)\n",
    "        feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        feature_map = ZeroPadding2D(padding=((0, 1), (0, 1)), name=name+'_pad')(feature_map)\n",
    "        feature_map = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 2, padding = 'valid', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        return feature_map\n",
    "\n",
    "    def convolutional_residual_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        latent = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name)(feature_map)\n",
    "        latent =  BatchNormalization()(latent)\n",
    "        latent = Activation('relu')(latent)\n",
    "        feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        latent = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(latent)\n",
    "        latent = BatchNormalization()(latent)\n",
    "        residual_block = self.residual_layer(feature_map, latent, name)\n",
    "        return residual_block\n",
    "    \n",
    "    def __call__(self, input_shape=(96, 128, 1)):\n",
    "        inputs_image = Input(shape=input_shape)\n",
    "        upsample_layer = Conv2DTranspose(filters = 16, kernel_size = 3, strides = (1, 1), padding = 'same')(inputs_image)\n",
    "        block_1 = self.feature_extraction_block(upsample_layer, 32, 64,'block_1')\n",
    "        block_1_output = self.convolutional_residual_block(block_1, 32, 64,'block_2')\n",
    "        block_2 = self.feature_extraction_block(block_1_output, 32, 64, 'block_3')\n",
    "        block_2_output = self.convolutional_residual_block(block_2, 32, 64, 'block_4')\n",
    "        block_3 = self.feature_extraction_block(block_2_output, 32, 64,'block_5')\n",
    "        block_3_output = self.convolutional_residual_block(block_3, 32, 64,'block_6')\n",
    "\n",
    "        model = Model(inputs_image, block_3_output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 128, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 96, 128, 16)          160       ['input_1[0][0]']             \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " block_1 (Conv2D)            (None, 96, 128, 32)          4640      ['conv2d_transpose[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 96, 128, 32)          128       ['block_1[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 96, 128, 32)          0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 96, 128, 32)          0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D  (None, 97, 129, 32)          0         ['dropout[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_2 (Conv2D)          (None, 48, 64, 64)           18496     ['block_1_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 48, 64, 64)           256       ['block_1_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_2 (Conv2D)            (None, 48, 64, 32)           18464     ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 48, 64, 32)           128       ['block_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 48, 64, 32)           0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_2_2 (Conv2D)          (None, 48, 64, 64)           18496     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 48, 64, 64)           0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 48, 64, 64)           256       ['block_2_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_2_output (Add)        (None, 48, 64, 64)           0         ['dropout_1[0][0]',           \n",
      "                                                                     'batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_3 (Conv2D)            (None, 48, 64, 32)           18464     ['block_2_output[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 48, 64, 32)           128       ['block_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 48, 64, 32)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 48, 64, 32)           0         ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D  (None, 49, 65, 32)           0         ['dropout_2[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_3_2 (Conv2D)          (None, 24, 32, 64)           18496     ['block_3_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 24, 32, 64)           256       ['block_3_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_4 (Conv2D)            (None, 24, 32, 32)           18464     ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 24, 32, 32)           128       ['block_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 24, 32, 32)           0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_4_2 (Conv2D)          (None, 24, 32, 64)           18496     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 24, 32, 64)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 24, 32, 64)           256       ['block_4_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_4_output (Add)        (None, 24, 32, 64)           0         ['dropout_3[0][0]',           \n",
      "                                                                     'batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_5 (Conv2D)            (None, 24, 32, 32)           18464     ['block_4_output[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 24, 32, 32)           128       ['block_5[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 24, 32, 32)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 24, 32, 32)           0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " block_5_pad (ZeroPadding2D  (None, 25, 33, 32)           0         ['dropout_4[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_5_2 (Conv2D)          (None, 12, 16, 64)           18496     ['block_5_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 12, 16, 64)           256       ['block_5_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_6 (Conv2D)            (None, 12, 16, 32)           18464     ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 12, 16, 32)           128       ['block_6[0][0]']             \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 12, 16, 32)           0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_6_2 (Conv2D)          (None, 12, 16, 64)           18496     ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 12, 16, 64)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 12, 16, 64)           256       ['block_6_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " block_6_output (Add)        (None, 12, 16, 64)           0         ['dropout_5[0][0]',           \n",
      "                                                                     'batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 210400 (821.88 KB)\n",
      "Trainable params: 209248 (817.38 KB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "backbone = BackBone()\n",
    "model = backbone()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def get_backbone():\n",
    "    backbone = BackBone() \n",
    "    backbone = backbone(input_shape=[None, None, 1])\n",
    "\n",
    "    b2_output, b4_output, b6_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in ['block_2_output', 'block_4_output', 'block_6_output']\n",
    "    ]\n",
    "\n",
    "    return keras.Model(\n",
    "        inputs = [backbone.inputs], outputs=[b2_output, b4_output, b6_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_2_output (Add)        (None, 48, 64, 64)\n",
    "\n",
    "# block_4_output (Add)        (None, 24, 32, 64)\n",
    "\n",
    "# block_6_output (Add)        (None, 12, 16, 64)\n",
    "\n",
    "class FeaturePyramid(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
    "        self.backbone = get_backbone()\n",
    "        self.conv_c2_1x1 = keras.layers.Conv2D(16, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = keras.layers.Conv2D(16, 1, 1, \"same\")\n",
    "        self.conv_c6_1x1 = keras.layers.Conv2D(16, 1, 1, \"same\")\n",
    "        \n",
    "        self.conv_c2_3x3 = keras.layers.Conv2D(16, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = keras.layers.Conv2D(16, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = keras.layers.Conv2D(16, 3, 1, \"same\")\n",
    "        # self.conv_c6_3x3 = keras.layers.Conv2D(32, 3, 2, \"same\")\n",
    "        # self.conv_c7_3x3 = keras.layers.Conv2D(32, 3, 2, \"same\")\n",
    "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        b2_output, b4_output, b6_output = self.backbone(images, training=training)\n",
    "        p2_output = self.conv_c2_1x1(b2_output)\n",
    "        p4_output = self.conv_c4_1x1(b4_output)\n",
    "        p6_output = self.conv_c6_1x1(b6_output)\n",
    "        \n",
    "        p4_output = p4_output + self.upsample_2x(p6_output)\n",
    "        p2_output = p2_output + self.upsample_2x(p4_output)\n",
    "        \n",
    "        # p1_output = self.conv_c3_3x3(p1_output)\n",
    "        p2_output = self.conv_c2_3x3(p2_output)\n",
    "        p4_output = self.conv_c4_3x3(p4_output)\n",
    "        p6_output = self.conv_c6_3x3(p6_output)\n",
    "        # pn_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "\n",
    "        # print(\"b2_output shape:\", tf.shape(b2_output))\n",
    "        # print(\"b4_output shape:\", tf.shape(b4_output))\n",
    "        # print(\"b6_output shape:\", tf.shape(b6_output))\n",
    "        # print(\"p2_output shape:\", tf.shape(p2_output))\n",
    "        # print(\"p4_output shape:\", tf.shape(p4_output))\n",
    "        # print(\"p6_output shape:\", tf.shape(p6_output))\n",
    "        # print(\"pn_output shape:\", tf.shape(pn_output))\n",
    "\n",
    "             \n",
    "        return p2_output, p4_output, p6_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_head(output_filters, bias_init):\n",
    "    head = keras.Sequential([keras.Input(shape=[None, None, 16])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01, seed=1)\n",
    "\n",
    "    for _ in range(4):\n",
    "        head.add(\n",
    "            keras.layers.Conv2D(32, 3, padding = 'same', kernel_initializer=kernel_init)\n",
    "        )\n",
    "        head.add(keras.layers.ReLU())\n",
    "\n",
    "    \n",
    "    head.add(\n",
    "        keras.layers.Conv2D(\n",
    "            output_filters,\n",
    "            3,\n",
    "            1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            bias_initializer=bias_init,\n",
    "        )\n",
    "    )\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(keras.Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(CustomNet, self).__init__(name=\"CustomNet\", **kwargs)\n",
    "        self.fpn = FeaturePyramid()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(6 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(6 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=True):\n",
    "        # # # # # # print(f\"shape: {image.shape}\")\n",
    "        features = self.fpn(image, training=training)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "\n",
    "        for feature in features:\n",
    "            box_output = tf.reshape(self.box_head(feature), [N, -1, 4])\n",
    "            cls_output = tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
    "\n",
    "            # print(\"feature shape:\", tf.shape(feature))\n",
    "            # print(\"box_output shape:\", tf.shape(box_output))\n",
    "            # print(\"cls_output shape:\", tf.shape(cls_output))\n",
    "\n",
    "            box_outputs.append(box_output)\n",
    "            cls_outputs.append(cls_output)\n",
    "        \n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        # print(\"Final cls_outputs shape:\", tf.shape(cls_outputs))\n",
    "        # print(\"Final box_outputs shape:\", tf.shape(box_outputs))\n",
    "        final_output = tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        # print(\"Final output shape:\", tf.shape(final_output))\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetBoxLoss(tf.losses.Loss):  \n",
    "    def __init__(self, delta):\n",
    "        super(CustomNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"CustomNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5\n",
    "        )\n",
    "\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetClassificationLoss(tf.losses.Loss):   \n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(CustomNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"CustomNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred\n",
    "        )\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetLoss(tf.losses.Loss):    \n",
    "    def __init__(self, num_classes=1, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "        super(CustomNetLoss, self).__init__(reduction=\"auto\", name=\"CustomNetLoss\")\n",
    "        self._cls_loss = CustomNetClassificationLoss(alpha, gamma)\n",
    "        self._box_loss = CustomNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # print(\"y_true shape:\", tf.shape(y_true))\n",
    "        # print(\"y_pred shape:\", tf.shape(y_pred))\n",
    "\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        # print(\"box_labels shape:\", tf.shape(box_labels))\n",
    "        # print(\"box_predictions shape:\", tf.shape(box_predictions))\n",
    "        \n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        # print(\"cls_labels shape:\", tf.shape(cls_labels))\n",
    "        # print(\"cls_predictions shape:\", tf.shape(cls_predictions))\n",
    "\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        # print(\"positive_mask shape:\", tf.shape(positive_mask))\n",
    "        # print(\"ignore_mask shape:\", tf.shape(ignore_mask))\n",
    "\n",
    "        cls_loss = self._cls_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        # print(\"cls_loss shape:\", tf.shape(cls_loss))\n",
    "        # print(\"box_loss shape:\", tf.shape(box_loss))\n",
    "\n",
    "        cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, cls_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        # print(\"normalizer:\", normalizer)\n",
    "\n",
    "        cls_loss = tf.math.divide_no_nan(tf.reduce_sum(cls_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        \n",
    "        loss = cls_loss + box_loss\n",
    "        # print(\"Final loss shape:\", tf.shape(loss))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "# class CustomNetLoss(tf.losses.Loss):\n",
    "#     def __init__(self, num_classes=1):\n",
    "#         super(CustomNetLoss, self).__init__(reduction=\"auto\", name=\"CustomNetLoss\")\n",
    "#         self._num_classes = num_classes\n",
    "#         self.mse_loss = MeanSquaredError(reduction=\"none\")\n",
    "#         self.cce_loss = CategoricalCrossentropy(reduction=\"none\", from_logits=True)\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # 박스 레이블과 예측 분리\n",
    "#         box_labels = y_true[:, :, :4]\n",
    "#         box_predictions = y_pred[:, :, :4]\n",
    "\n",
    "#         # 클래스 레이블과 예측 분리 및 one-hot 인코딩\n",
    "#         cls_labels = tf.one_hot(tf.cast(y_true[:, :, 4], dtype=tf.int32), depth=self._num_classes)\n",
    "#         cls_predictions = y_pred[:, :, 4:]\n",
    "\n",
    "#         # 박스 손실(MSE) 계산\n",
    "#         box_loss = self.mse_loss(box_labels, box_predictions)\n",
    "\n",
    "#         # 클래스 손실(Categorical Cross-Entropy) 계산\n",
    "#         cls_loss = self.cce_loss(cls_labels, cls_predictions)\n",
    "\n",
    "#         # 손실 합산\n",
    "#         total_loss = tf.reduce_mean(cls_loss) + tf.reduce_mean(box_loss)\n",
    "#         return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "\n",
    "# learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "# learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "# learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     boundaries=learning_rate_boundaries, values=learning_rates\n",
    "# )\n",
    "\n",
    "LR = 0.0005\n",
    "initial_learning_rate = LR\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.90,\n",
    "    staircase=True)\n",
    "\n",
    "\n",
    "model_dir = \"ObjectDetectionCheckpoint/customnet.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "# 멀티 GPU 전략 설정\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # 모델, 손실 함수, 옵티마이저를 전략 범위 내에서 정의\n",
    "    model = CustomNet(num_classes)\n",
    "    loss_fn = CustomNetLoss()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# 이후 모델 훈련 및 평가 코드는 변경 없이 사용 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (2, 96, 128, 1)\n",
      "Images max: tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Images min: tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Labels shape: (2, 24192, 5)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "for batch in train_dataset.take(1):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    \n",
    "    # 이미지와 레이블의 형태를 출력합니다\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Images max:\", tf.reduce_max(images))\n",
    "    print(\"Images min:\", tf.reduce_min(images))\n",
    "    print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # TensorFlow Dataset 예시\n",
    "# # train_dataset = tf.data.Dataset.from_tensor_slices(...)\n",
    "\n",
    "# def check_nan_in_dataset(dataset):\n",
    "#     for batch in dataset:\n",
    "#         images, labels = batch\n",
    "#         if tf.reduce_any(tf.math.is_nan(images)) or tf.reduce_any(tf.math.is_nan(labels)):\n",
    "#             return True\n",
    "#     # print(images)\n",
    "#     # print(labels)\n",
    "#     return False\n",
    "\n",
    "\n",
    "# contains_nan = check_nan_in_dataset(train_dataset)\n",
    "# if contains_nan:\n",
    "#     print(\"Data contains NaN values\")\n",
    "# else:\n",
    "#     print(\"Data does not contain NaN values\")\n",
    "\n",
    "# check_nan_in_dataset(train_dataset)\n",
    "# check_nan_in_dataset(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 10:08:06.564991: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"UnbatchDataset/_30\"\n",
      "op: \"UnbatchDataset\"\n",
      "input: \"MapDataset/_29\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021UnbatchDataset:50\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 128\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 24648\n",
      "        }\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:Collective all_reduce tensors: 82 all_reduces, num_devices = 7, group_size = 7, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 82 all_reduces, num_devices = 7, group_size = 7, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 10:08:42.290723: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inCustomNet/FeaturePyramid/model_4/dropout_12/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-01-22 10:08:50.713896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 10:08:55.547307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 10:08:56.214649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 10:08:56.989924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 10:08:57.778679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 10:08:58.100020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6440/Unknown - 399s 53ms/step - loss: 7.3062 - accuracy: 0.3205 - precision_1: 0.4971 - recall_1: 0.0112\n",
      "Epoch 1: loss improved from inf to 7.30616, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 10:14:47.168593: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13625896791061865298\n",
      "2024-01-22 10:14:47.168664: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2873567194675043691\n",
      "2024-01-22 10:14:47.168727: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13550569290572562724\n",
      "2024-01-22 10:14:47.168792: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1826834420501111562\n",
      "2024-01-22 10:14:47.168811: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17923443367661764799\n",
      "2024-01-22 10:14:47.168862: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 690957928517722189\n",
      "2024-01-22 10:14:47.168940: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7077786984181825374\n",
      "2024-01-22 10:14:47.168990: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14164526488952922411\n",
      "2024-01-22 10:14:47.169015: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13540182592532857708\n",
      "2024-01-22 10:14:47.169040: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1058352149495465250\n",
      "2024-01-22 10:14:47.169062: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16091594989723862559\n",
      "2024-01-22 10:14:47.169085: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2342809906417656005\n",
      "2024-01-22 10:14:47.169107: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12665079630411598470\n",
      "2024-01-22 10:14:47.169128: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14361875194009223787\n",
      "2024-01-22 10:14:47.169150: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15949110538842211732\n",
      "2024-01-22 10:14:47.169170: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9197596234452772698\n",
      "2024-01-22 10:14:47.169189: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2294958958311392855\n",
      "2024-01-22 10:14:47.169210: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14511263836864914389\n",
      "2024-01-22 10:14:47.169234: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16714953297298820574\n",
      "2024-01-22 10:14:47.169254: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 755152879528033539\n",
      "2024-01-22 10:14:47.169276: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8234723965370336540\n",
      "2024-01-22 10:14:47.169298: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6693131764054419538\n",
      "2024-01-22 10:14:47.169316: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2760966188632295551\n",
      "2024-01-22 10:14:47.169337: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2659494978330764805\n",
      "2024-01-22 10:14:47.169355: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1896712378422529774\n",
      "2024-01-22 10:14:47.169375: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10941988465116701011\n",
      "2024-01-22 10:14:47.169407: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4232054985938700148\n",
      "2024-01-22 10:14:47.169426: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17500316097032045258\n",
      "2024-01-22 10:14:47.169444: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17407022873649547695\n",
      "2024-01-22 10:14:47.169464: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16907118944379414421\n",
      "2024-01-22 10:14:47.169482: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9469640739713028094\n",
      "2024-01-22 10:14:47.169501: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5010699797528602027\n",
      "2024-01-22 10:14:47.169523: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7896835952967912540\n",
      "2024-01-22 10:14:47.169550: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14894328830605296562\n",
      "2024-01-22 10:14:47.169568: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16420521005263817983\n",
      "2024-01-22 10:14:47.169588: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13617705461849343797\n",
      "2024-01-22 10:14:47.169606: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3788429395685494646\n",
      "2024-01-22 10:14:47.169619: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1777192412163631525\n",
      "2024-01-22 10:14:47.169637: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10944946353683228459\n",
      "2024-01-22 10:14:47.169659: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10960295611400883492\n",
      "2024-01-22 10:14:47.169682: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5193249579971858266\n",
      "2024-01-22 10:14:47.169701: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15242086019253374599\n",
      "2024-01-22 10:14:47.169722: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10530666235446729582\n",
      "2024-01-22 10:14:47.169744: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15586375183676451229\n",
      "2024-01-22 10:14:47.169765: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 843241237816411995\n",
      "2024-01-22 10:14:47.169790: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5024424515572583588\n",
      "2024-01-22 10:14:47.169811: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1409920812784062938\n",
      "2024-01-22 10:14:47.169831: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14280017110979793415\n",
      "2024-01-22 10:14:47.169854: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10735029048151940158\n",
      "2024-01-22 10:14:47.169878: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9590428854149065509\n",
      "2024-01-22 10:14:47.169900: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9680557496809408627\n",
      "2024-01-22 10:14:47.169919: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3880182688879333932\n",
      "2024-01-22 10:14:47.169939: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11449565857102538130\n",
      "2024-01-22 10:14:47.169959: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2378851815430951815\n",
      "2024-01-22 10:14:47.169978: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18171505997479713750\n",
      "2024-01-22 10:14:47.169996: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14850639702351154037\n",
      "2024-01-22 10:14:47.170016: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8064686684476851259\n",
      "2024-01-22 10:14:47.170040: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16275564113407855812\n",
      "2024-01-22 10:14:47.170059: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5205813195384396218\n",
      "2024-01-22 10:14:47.170077: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10786787044428223567\n",
      "2024-01-22 10:14:47.170100: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9359095428453572622\n",
      "2024-01-22 10:14:47.170121: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16459996309151451741\n",
      "2024-01-22 10:14:47.170141: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15177396243569848851\n",
      "2024-01-22 10:14:47.170161: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2796734538764720220\n",
      "2024-01-22 10:14:47.170182: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17230793238529333778\n",
      "2024-01-22 10:14:47.170207: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16894257212912836047\n",
      "2024-01-22 10:14:47.170226: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15741216881337017326\n",
      "2024-01-22 10:14:47.170243: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 430674654830995053\n",
      "2024-01-22 10:14:47.170263: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5394009674199184371\n",
      "2024-01-22 10:14:47.170284: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13755122684619726324\n",
      "2024-01-22 10:14:47.170305: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11876839951858702962\n",
      "2024-01-22 10:14:47.170325: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10826837563479007279\n",
      "2024-01-22 10:14:47.170347: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1610023921834272502\n",
      "2024-01-22 10:14:47.170369: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4698450226254082869\n",
      "2024-01-22 10:14:47.170389: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7857828450855392715\n",
      "2024-01-22 10:14:47.170411: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15410386929241875908\n",
      "2024-01-22 10:14:47.170435: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9940981177484786762\n",
      "2024-01-22 10:14:47.170458: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9932308942014554719\n",
      "2024-01-22 10:14:47.170476: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6094180971494319094\n",
      "2024-01-22 10:14:47.170494: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9602228749057217317\n",
      "2024-01-22 10:14:47.170513: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6091238126485485331\n",
      "2024-01-22 10:14:47.170535: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3722727240754275868\n",
      "2024-01-22 10:14:47.170560: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9714744716382575074\n",
      "2024-01-22 10:14:47.170580: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12594756364199307039\n",
      "2024-01-22 10:14:47.170599: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13826738184762354446\n",
      "2024-01-22 10:14:47.170616: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16771723975450547581\n",
      "2024-01-22 10:14:47.170635: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5616321744393016507\n",
      "2024-01-22 10:14:47.170653: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15943682265594462209\n",
      "2024-01-22 10:14:47.170672: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3427186832683411148\n",
      "2024-01-22 10:14:47.170702: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7451434152834885839\n",
      "2024-01-22 10:14:47.170729: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1937903122791739182\n",
      "2024-01-22 10:14:47.170747: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12114002482036209557\n",
      "2024-01-22 10:14:47.170766: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16534139081834552035\n",
      "2024-01-22 10:14:47.170786: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7241768563655787337\n",
      "2024-01-22 10:14:47.170804: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3304171043287361220\n",
      "2024-01-22 10:14:47.170825: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8706011832241105167\n",
      "2024-01-22 10:14:47.170836: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4901633573730201652\n",
      "2024-01-22 10:14:47.170847: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3683009333448387735\n",
      "2024-01-22 10:14:47.170860: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7750377377378054804\n",
      "2024-01-22 10:14:47.170884: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5090029891449614382\n",
      "2024-01-22 10:14:47.170903: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13746797030601935445\n",
      "2024-01-22 10:14:47.170921: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4575603816224111771\n",
      "2024-01-22 10:14:47.170942: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9507008830294950897\n",
      "2024-01-22 10:14:47.170960: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8900361408467955079\n",
      "2024-01-22 10:14:47.170978: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1606701925587973980\n",
      "2024-01-22 10:14:47.170999: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15045985184020230678\n",
      "2024-01-22 10:14:47.171016: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13643639253017537061\n",
      "2024-01-22 10:14:47.171034: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 123264807649172875\n",
      "2024-01-22 10:14:47.171052: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5851358199967221233\n",
      "2024-01-22 10:14:47.171070: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13003740156843882063\n",
      "2024-01-22 10:14:47.171092: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1326597875104987628\n",
      "2024-01-22 10:14:47.171115: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12798115236110054366\n",
      "2024-01-22 10:14:47.171131: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 772017619036158669\n",
      "2024-01-22 10:14:47.171150: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11274092862647978169\n",
      "2024-01-22 10:14:47.171172: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6681174374807469311\n",
      "2024-01-22 10:14:47.171191: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13895298940674784060\n",
      "2024-01-22 10:14:47.171210: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 477306381934016230\n",
      "2024-01-22 10:14:47.171227: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3399671360384801493\n",
      "2024-01-22 10:14:47.171245: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13985500092029782329\n",
      "2024-01-22 10:14:47.171263: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9408732600348949551\n",
      "2024-01-22 10:14:47.171285: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7604336461948517260\n",
      "2024-01-22 10:14:47.171303: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1800098651762931686\n",
      "2024-01-22 10:14:47.171323: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6320888116594645397\n",
      "2024-01-22 10:14:47.171340: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15817858120317455049\n",
      "2024-01-22 10:14:47.171360: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2685969966183907135\n",
      "2024-01-22 10:14:47.171379: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10931151603054541892\n",
      "2024-01-22 10:14:47.171398: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4894208696236195750\n",
      "2024-01-22 10:14:47.171417: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8845531811911617145\n",
      "2024-01-22 10:14:47.171443: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8711719536517233500\n",
      "2024-01-22 10:14:47.171463: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10137780918831959505\n",
      "2024-01-22 10:14:47.171485: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11580614798525453033\n",
      "2024-01-22 10:14:47.171502: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9029307591713542721\n",
      "2024-01-22 10:14:47.171518: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17968083420298646745\n",
      "2024-01-22 10:14:47.171537: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7137652807684707897\n",
      "2024-01-22 10:14:47.171555: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2379887280571062145\n",
      "2024-01-22 10:14:47.171575: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16731166910088764105\n",
      "2024-01-22 10:14:47.171681: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17797898282148125537\n",
      "2024-01-22 10:14:47.171698: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11433497367127251913\n",
      "2024-01-22 10:14:47.171711: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18025385674294304481\n",
      "2024-01-22 10:14:47.171724: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13777769078431933513\n",
      "2024-01-22 10:14:47.171743: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9117345676315240153\n",
      "2024-01-22 10:14:47.171868: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16869459750082947241\n",
      "2024-01-22 10:14:47.171885: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9391514562074110305\n",
      "2024-01-22 10:14:47.171899: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14426810996722133681\n",
      "2024-01-22 10:14:47.171915: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10161059015413787929\n",
      "2024-01-22 10:14:47.171932: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17781886410201085281\n",
      "2024-01-22 10:14:47.172049: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6106390099292616529\n",
      "2024-01-22 10:14:47.172069: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14290512789866559593\n",
      "2024-01-22 10:14:47.172087: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18275876698936812312\n",
      "2024-01-22 10:14:47.172102: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3997226414890771280\n",
      "2024-01-22 10:14:47.172121: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18119352470923274384\n",
      "2024-01-22 10:14:47.172136: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1418802707100903728\n",
      "2024-01-22 10:14:47.172150: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15477050343118785344\n",
      "2024-01-22 10:14:47.172282: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11672425605451738208\n",
      "2024-01-22 10:14:47.172304: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 232792079032683592\n",
      "2024-01-22 10:14:47.172319: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4149957693251628048\n",
      "2024-01-22 10:14:47.172332: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10701685917070394904\n",
      "2024-01-22 10:14:47.172347: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5372107554844602792\n",
      "2024-01-22 10:14:47.172362: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14605681344702395352\n",
      "2024-01-22 10:14:47.172388: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6693441390768100896\n",
      "2024-01-22 10:14:47.172406: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8154051789221656320\n",
      "2024-01-22 10:14:47.172528: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3931619731249879696\n",
      "2024-01-22 10:14:47.172546: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1273366268843956800\n",
      "2024-01-22 10:14:47.172560: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16987784965880790152\n",
      "2024-01-22 10:14:47.172652: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8806040241394474904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6440/6440 [==============================] - 400s 53ms/step - loss: 7.3062 - accuracy: 0.3205 - precision_1: 0.4971 - recall_1: 0.0112\n",
      "Epoch 2/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 1.3172 - accuracy: 0.1376 - precision_1: 0.4955 - recall_1: 0.0104\n",
      "Epoch 2: loss improved from 7.30616 to 1.31724, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_2\n",
      "6440/6440 [==============================] - 344s 53ms/step - loss: 1.3172 - accuracy: 0.1376 - precision_1: 0.4955 - recall_1: 0.0104\n",
      "Epoch 3/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 1.1084 - accuracy: 0.1174 - precision_1: 0.4961 - recall_1: 0.0108\n",
      "Epoch 3: loss improved from 1.31724 to 1.10839, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_3\n",
      "6440/6440 [==============================] - 345s 54ms/step - loss: 1.1084 - accuracy: 0.1174 - precision_1: 0.4961 - recall_1: 0.0108\n",
      "Epoch 4/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 1.1616 - accuracy: 0.1193 - precision_1: 0.4956 - recall_1: 0.0101\n",
      "Epoch 4: loss did not improve from 1.10839\n",
      "6440/6440 [==============================] - 345s 54ms/step - loss: 1.1616 - accuracy: 0.1193 - precision_1: 0.4956 - recall_1: 0.0101\n",
      "Epoch 5/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 1.0435 - accuracy: 0.1131 - precision_1: 0.4959 - recall_1: 0.0098\n",
      "Epoch 5: loss improved from 1.10839 to 1.04353, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_5\n",
      "6440/6440 [==============================] - 344s 53ms/step - loss: 1.0435 - accuracy: 0.1131 - precision_1: 0.4959 - recall_1: 0.0098\n",
      "Epoch 6/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 1.0255 - accuracy: 0.1296 - precision_1: 0.4963 - recall_1: 0.0099\n",
      "Epoch 6: loss improved from 1.04353 to 1.02555, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_6\n",
      "6440/6440 [==============================] - 345s 53ms/step - loss: 1.0255 - accuracy: 0.1296 - precision_1: 0.4963 - recall_1: 0.0099\n",
      "Epoch 7/100\n",
      "6439/6440 [============================>.] - ETA: 0s - loss: 1.0040 - accuracy: 0.1625 - precision_1: 0.4958 - recall_1: 0.0096\n",
      "Epoch 7: loss improved from 1.02555 to 1.00393, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_7\n",
      "6440/6440 [==============================] - 345s 54ms/step - loss: 1.0039 - accuracy: 0.1625 - precision_1: 0.4958 - recall_1: 0.0096\n",
      "Epoch 8/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 0.9875 - accuracy: 0.1486 - precision_1: 0.4956 - recall_1: 0.0095\n",
      "Epoch 8: loss improved from 1.00393 to 0.98752, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_8\n",
      "6440/6440 [==============================] - 342s 53ms/step - loss: 0.9875 - accuracy: 0.1486 - precision_1: 0.4956 - recall_1: 0.0095\n",
      "Epoch 9/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 0.9709 - accuracy: 0.1176 - precision_1: 0.4960 - recall_1: 0.0094\n",
      "Epoch 9: loss improved from 0.98752 to 0.97085, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_9\n",
      "6440/6440 [==============================] - 342s 53ms/step - loss: 0.9709 - accuracy: 0.1176 - precision_1: 0.4960 - recall_1: 0.0094\n",
      "Epoch 10/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 0.9602 - accuracy: 0.1101 - precision_1: 0.4956 - recall_1: 0.0094\n",
      "Epoch 10: loss improved from 0.97085 to 0.96023, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_10\n",
      "6440/6440 [==============================] - 341s 53ms/step - loss: 0.9602 - accuracy: 0.1101 - precision_1: 0.4956 - recall_1: 0.0094\n",
      "Epoch 11/100\n",
      "6440/6440 [==============================] - ETA: 0s - loss: 0.9447 - accuracy: 0.1256 - precision_1: 0.4959 - recall_1: 0.0095\n",
      "Epoch 11: loss improved from 0.96023 to 0.94465, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_11\n",
      "6440/6440 [==============================] - 344s 53ms/step - loss: 0.9447 - accuracy: 0.1256 - precision_1: 0.4959 - recall_1: 0.0095\n",
      "Epoch 12/100\n",
      "3509/6440 [===============>..............] - ETA: 2:36 - loss: 0.9302 - accuracy: 0.1179 - precision_1: 0.4945 - recall_1: 0.0095"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/ssd/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CustomNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " FeaturePyramid (FeaturePyr  multiple                  220480    \n",
      " amid)                                                           \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, None, None, 6)     34118     \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, None, None, 24)    39320     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 293918 (1.12 MB)\n",
      "Trainable params: 292766 (1.12 MB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
