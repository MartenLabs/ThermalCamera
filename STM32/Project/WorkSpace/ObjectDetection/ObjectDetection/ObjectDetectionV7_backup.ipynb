{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "RES_HEIGHT = 24\n",
    "RES_WIDTH = 32\n",
    "NUM_CLASS = 1\n",
    "N_BATCH = 3\n",
    "N_EPOCH = 200\n",
    "LR = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 15:46:46.571359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 22455 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11999834835012370839\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23546626048\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6654171168533717200\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1d:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 416903419,\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23546626048\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 15288086976808801698\n",
       " physical_device_desc: \"device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1f:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 2144165316,\n",
       " name: \"/device:GPU:2\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23546626048\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 8648910039442402751\n",
       " physical_device_desc: \"device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:20:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 1651660799,\n",
       " name: \"/device:GPU:3\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23546626048\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12906078419860076028\n",
       " physical_device_desc: \"device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 878896533,\n",
       " name: \"/device:GPU:4\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23546626048\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 2964038243634833100\n",
       " physical_device_desc: \"device: 4, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:22:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 615190153,\n",
       " name: \"/device:GPU:5\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23546626048\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10333324278266248498\n",
       " physical_device_desc: \"device: 5, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:23:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 1769886423,\n",
       " name: \"/device:GPU:6\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23546626048\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 9427711241711318973\n",
       " physical_device_desc: \"device: 6, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:24:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 893286608]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 15:46:46.571772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:1 with 22455 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1f:00.0, compute capability: 8.6\n",
      "2024-04-08 15:46:46.572156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:2 with 22455 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:20:00.0, compute capability: 8.6\n",
      "2024-04-08 15:46:46.572540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:3 with 22455 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n",
      "2024-04-08 15:46:46.572921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:4 with 22455 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:22:00.0, compute capability: 8.6\n",
      "2024-04-08 15:46:46.573309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:5 with 22455 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:23:00.0, compute capability: 8.6\n",
      "2024-04-08 15:46:46.573685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:6 with 22455 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:24:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현제 바운딩박스는 xmin, ymin, xmax, ymax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17820, 24, 32, 1) (17820,) (17820, 4, 4) 17820\n",
      "255 0\n",
      "(13544, 24, 32, 1)\n",
      "(13544, 4, 4)\n",
      "13544\n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " ...\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "datasets = np.load('dataset/ObjectDetection.npz', allow_pickle=True)\n",
    "images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']\n",
    "\n",
    "max_label_length = 4\n",
    "labels = []\n",
    "for num in numbers:\n",
    "    cls = [1] * num if num != 0 else [0]\n",
    "    cls += [0] * (max_label_length - len(cls))\n",
    "    labels.append(cls)\n",
    "\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# non_zero_indices = np.where(numbers != 0)[0]\n",
    "non_zero_indices = np.where(numbers > 1)[0]\n",
    "\n",
    "# numbers가 0이 아닌 항목만 유지\n",
    "images_filtered = images[non_zero_indices]\n",
    "bboxes_filtered = bboxes[non_zero_indices]\n",
    "labels_filtered = np.array(labels)[non_zero_indices]\n",
    "\n",
    "print(images.shape, numbers.shape, bboxes.shape, len(labels))\n",
    "\n",
    "print(images.max(), images.min())\n",
    "\n",
    "dataset = {\n",
    "    'images' : images_filtered,\n",
    "    'bboxes' : bboxes_filtered,\n",
    "    'class' : labels_filtered\n",
    "}\n",
    "\n",
    "print(dataset['images'].shape)\n",
    "print(dataset['bboxes'].shape)\n",
    "print(len(dataset['class']))\n",
    "print(dataset['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " ...\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  3 14 14]\n",
      " [24  0 32  6]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]]\n",
      "255 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset['images'][0])\n",
    "print(dataset['bboxes'][0])\n",
    "print(dataset['images'].max(), dataset['images'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n",
      "24\n",
      "32\n",
      "bbox:  tf.Tensor(\n",
      "[[ 3  3 14 14]\n",
      " [24  0 32  6]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]], shape=(4, 4), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " ...\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "images = dataset['images']\n",
    "bboxes = dataset['bboxes']\n",
    "cls = dataset['class']\n",
    "\n",
    "boxes = bboxes\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.axis('off')\n",
    "image = images\n",
    "print(image[0].shape)\n",
    "print(image[0].shape[0])\n",
    "print(image[0].shape[1])\n",
    "plt.imshow(image[0])\n",
    "ax = plt.gca()\n",
    "boxes = boxes[0]\n",
    "boxes = tf.stack([\n",
    "\t(boxes[:, 0] ), \n",
    "\t(boxes[:, 1] ),\n",
    "\t(boxes[:, 2] ),\n",
    "\t(boxes[:, 3] )], axis = -1\n",
    ")\n",
    "print(\"bbox: \", boxes)\n",
    "# 각 바운딩 박스에 대해 반복하여 그리기\n",
    "for box in boxes:\n",
    "    xmin, ymin, xmax, ymax = box \n",
    "    w, h = xmax - xmin, ymax - ymin\n",
    "    patch = plt.Rectangle(\n",
    "        [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "    )\n",
    "    ax.add_patch(patch)\n",
    "plt.show()\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 0)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.max(), images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_SIZE_WIDTH:   32\n",
      "IMG_SIZE_HEIGHT:  24\n",
      "N_DATA:           13544\n",
      "N_TRAIN:          10836\n",
      "N_VAL:            2708\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE_WIDTH = images.shape[2]\n",
    "IMG_SIZE_HEIGHT = images.shape[1]\n",
    "N_DATA = images.shape[0]\n",
    "N_VAL = int(images.shape[0] * 0.2)\n",
    "N_TRAIN = int(images.shape[0] - N_VAL)\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'test/tfrecord/')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "print(\"IMG_SIZE_WIDTH:  \", IMG_SIZE_WIDTH)\n",
    "print(\"IMG_SIZE_HEIGHT: \", IMG_SIZE_HEIGHT)\n",
    "print(\"N_DATA:          \", N_DATA)\n",
    "print(\"N_TRAIN:         \", N_TRAIN)\n",
    "print(\"N_VAL:           \", N_VAL)\n",
    "\n",
    "shuffle_list = list(range(N_DATA))\n",
    "random.shuffle(shuffle_list)\n",
    "\n",
    "train_idx_list = shuffle_list[:N_TRAIN]\n",
    "val_idx_list = shuffle_list[N_TRAIN:]\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "writer_train = tf.io.TFRecordWriter(tfr_train_dir)\n",
    "writer_val = tf.io.TFRecordWriter(tfr_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list = tf.train.FloatList(value = value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int32_list = tf.train.Int64List(value = [value]))\n",
    "\n",
    "\n",
    "def _bytes_feature_list(value_list):\n",
    "    \"\"\"value_list가 리스트일 때, 이를 serialize하여 bytes list로 변환하는 함수.\"\"\"\n",
    "    value_list = [tf.io.serialize_tensor(tf.constant(v)).numpy() for v in value_list]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13544, 24, 32, 1)\n",
      "(13544, 4, 4)\n",
      "(13544, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset['images'] = dataset['images']\n",
    "dataset['bboxes'] = dataset['bboxes']\n",
    "dataset['class'] = np.array(dataset['class'])\n",
    "images = dataset['images']\n",
    "bboxes = dataset['bboxes']\n",
    "cls = dataset['class']\n",
    "print(images.shape)\n",
    "print(bboxes.shape)\n",
    "print(cls.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in train_idx_list:\n",
    "    bbox = bboxes[idx]\n",
    "    xmin, ymin, xmax, ymax = bbox[:, 0] / RES_WIDTH, bbox[:, 1] / RES_HEIGHT, bbox[:, 2] / RES_WIDTH, bbox[:, 3] / RES_HEIGHT\n",
    "    bbox = np.stack([xmin, ymin, xmax, ymax], axis=-1).flatten()\n",
    "\n",
    "    image = images[idx]\n",
    "    bimage = image.tobytes()\n",
    "\n",
    "    number = numbers[idx]\n",
    "    class_id = cls[idx]\n",
    "    # print(len(cls))\n",
    "    serialized_cls = tf.io.serialize_tensor(tf.constant(class_id)).numpy()\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image': _bytes_feature(bimage),\n",
    "        'bbox': _float_feature(bbox),\n",
    "        'label': _bytes_feature(serialized_cls),\n",
    "        # 'number': _int64_feature(number)\n",
    "    }))\n",
    "    \n",
    "    writer_train.write(example.SerializeToString())\n",
    "writer_train.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in val_idx_list:\n",
    "    bbox = bboxes[idx]\n",
    "    xmin, ymin, xmax, ymax = bbox[:, 0] / RES_WIDTH, bbox[:, 1] / RES_HEIGHT, bbox[:, 2] / RES_WIDTH, bbox[:, 3] / RES_HEIGHT\n",
    "    bbox = np.stack([xmin, ymin, xmax, ymax], axis=-1).flatten()\n",
    "\n",
    "    image = images[idx]\n",
    "    bimage = image.tobytes()\n",
    "\n",
    "    number = numbers[idx]\n",
    "    class_id = cls[idx]\n",
    "    # print(len(cls))\n",
    "    serialized_cls = tf.io.serialize_tensor(tf.constant(class_id)).numpy()\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image': _bytes_feature(bimage),\n",
    "        'bbox': _float_feature(bbox),\n",
    "        'label': _bytes_feature(serialized_cls),\n",
    "        # 'number': _int64_feature(number)\n",
    "    }))\n",
    "    \n",
    "    writer_val.write(example.SerializeToString())\n",
    "writer_val.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "N_BATCH = 1\n",
    "# LR = 0.0005\n",
    "\n",
    "\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    features = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'bbox': tf.io.VarLenFeature(tf.float32),  \n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'number': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "\n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, [RES_HEIGHT, RES_WIDTH, 1])\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    # image = image / tf.reduce_max(image)\n",
    "\n",
    "    bbox = tf.sparse.to_dense(parsed_features['bbox']) \n",
    "    bbox = tf.cast(bbox, tf.float32)\n",
    "    # num_boxes = tf.shape(bbox)[0] // 4\n",
    "    bbox = tf.reshape(bbox, [-1, 4])\n",
    "\n",
    "    serialized_cls = parsed_features['label']\n",
    "    label = tf.io.parse_tensor(serialized_cls, out_type=tf.int64)\n",
    "    \n",
    "    # number = tf.cast(parsed_features['number'], tf.int64)\n",
    "    return image, bbox, label\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(AUTOTUNE).batch(N_BATCH, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "N_BATCH = 1\n",
    "# LR = 0.0005\n",
    "\n",
    "\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    features = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'bbox': tf.io.VarLenFeature(tf.float32),  \n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'number': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "\n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, [RES_HEIGHT, RES_WIDTH, 1])\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    # image = image / tf.reduce_max(image)\n",
    "\n",
    "    bbox = tf.sparse.to_dense(parsed_features['bbox']) \n",
    "    bbox = tf.cast(bbox, tf.float32)\n",
    "    # num_boxes = tf.shape(bbox)[0] // 4\n",
    "    bbox = tf.reshape(bbox, [-1, 4])\n",
    "\n",
    "    serialized_cls = parsed_features['label']\n",
    "    label = tf.io.parse_tensor(serialized_cls, out_type=tf.int64)\n",
    "    \n",
    "    # number = tf.cast(parsed_features['number'], tf.int64)\n",
    "    return image, bbox, label\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
    "val_dataset = val_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=N_TRAIN).prefetch(AUTOTUNE).batch(N_BATCH, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 32, 1)\n",
      "tf.Tensor(\n",
      "[[[0.375      0.25       0.71875    0.6666667 ]\n",
      "  [0.21875    0.         0.46875    0.25      ]\n",
      "  [0.0625     0.41666666 0.375      0.7916667 ]\n",
      "  [0.         0.         0.         0.        ]]], shape=(1, 4, 4), dtype=float32)\n",
      "tf.Tensor([[1 1 1 0]], shape=(1, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for img, bbox, label in val_dataset.take(1):\n",
    "    print(img.shape)\n",
    "    print(bbox)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n",
      "tf.Tensor([1 1 1 0], shape=(4,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0.21875    0.         0.40625    0.20833333]\n",
      " [0.34375    0.125      0.625      0.5416667 ]\n",
      " [0.65625    0.20833333 0.9375     0.5       ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 0\n",
    "for image, bbox, label in val_dataset.take(1):\n",
    "    image = image[idx]\n",
    "    bbox = bbox[idx]\n",
    "    label = label[idx]\n",
    "    image = image.numpy()\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()  \n",
    "    print(image.shape)\n",
    "    print(label)\n",
    "    print(bbox)\n",
    "    boxes = tf.stack(\n",
    "    \t[\n",
    "    \t bbox[:,0] * RES_WIDTH,\n",
    "    \t bbox[:,1] * RES_HEIGHT,\n",
    "    \t bbox[:,2] * RES_WIDTH,\n",
    "    \t bbox[:,3] * RES_HEIGHT\n",
    "    \t], axis = -1\n",
    "    )\n",
    "    for box in boxes:\n",
    "        xmin, ymin = box[:2]\n",
    "        w, h = box[2:] - box[:2]\n",
    "        patch = plt.Rectangle(\n",
    "            [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image, gt_boxes, cls_ids):\n",
    "    bbox = convert_to_xywh(gt_boxes)\n",
    "    return image, bbox, cls_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 32, 1)\n",
      "(1, 4, 4)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "for image, bbox, label in val_dataset.take(1):\n",
    "    print(image.shape)\n",
    "    print(bbox.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAGFCAYAAACL7UsMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAScklEQVR4nO3dXYzld1kH8Od/ZmZndrf71i1twZJgWq+KBhOC0Wu59cqYECIxMSQasWARaMt2d2e23RakFsuLvARiROOV3ivhjqhBYgKRkEiREN5s6b53X2fm/LyoV6Y4x+ehHYfn87n+f89z/v/zP//5zrl5pjHGCACgldluvwEA4NWnAABAQwoAADSkAABAQwoAADSkAABAQwoAADSkAABAQ8uLHviFY7+aHrJ+7Xo6+9ILFHrK07fVZs9PpqPjwoP5uSe289mImD60ms6evLVZmr1eCZ/arcG7PHyXRp8c88LgiI1KOP/V+ikMr1zwiNpFL4wtnXPE6aNH8tkLl2rDC5d8+vPjuzM4IsbzD6Sz0xP553BExObn8+f93X//4o7H+AUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgoWmMMRY58L5Db8kPWVl46/DLGtOUn71dW3ca80J+lu9XY7O2knfcvJXOToXrzd6y4NefnxXb+TXj0/FjtdlLhefh8+dqswumu1+Tzs5Xl0qzZ+eupLPPXvnKzq+ffnUAYM9SAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpSAACgIQUAABpaXvjIJ9fSQ+bvKu5yPlXIfvJ4bfaj+X3p44Hz+bmVc46I2Z8dTmfHe6/Vhq8XspXzrsztOvtkIRsRsWH2qzq7Mjcipif3p7PjoQu14YV7fPb0oXy4+FnP35//+zU7u1qb/Yn8c3wRfgEAgIYUAABoSAEAgIYUAABoSAEAgIYUAABoSAEAgIYUAABoSAEAgIYUAABoSAEAgIYUAABoSAEAgIYUAABoaBpjLLTv9t7p/vSQ2dHiSsP8Rt6YX7pcm10w3XZw12aPF6/mwzO9sI0x3+13wKup8Cyd1mqrbSvPlXHtem12wexwfhXx2LdSmj1tbaWz37rwTzse40kPAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0tL3rgdOcz6SHz5x9IZyMi4lQ+On3saGn0NE6ms/MLD+YHF845onbe4+If1YbHej5aOe/C2F0fvmujizdaZXj+q/WSjUp4j5536ZwjprWz6ey48UhteOGSz54+tDuDI2J++Y/T2enxfaXZ2586VsrvxC8AANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADU1jjLHIgfdO96eHzI4eTmcjImKhd/jy5pcu12YXzA7dtmuz51deTGen5YW3RLPHLfj152fF9nY6Oq2t1mbP8v9vjhs3a7MLZrcdTGfHvpXa8ML389kXvrzjMX4BAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaGjhva/Tk/vTQ+YPFVfynspHpw8VV1gWZs8fzq/krcyNiJie3JcPn5jXhq8XspXzrsztOvtkIRsRsWH2qzq7MjcipmduT2fHA+drwyv3+Gdek45Oxc96/vs/zofXa/9jb/31a0v5nfgFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAamsYYY5ED7136xfyQtdV0tuzWZim+4OV5WdPqvvzg+TyfjYhx82Y+POmFbYzafcYek3+cxezYkdroWf65Ms5fKAzORyMipjuP50fvWy7Nnl29kc5+6/w/7vz66VcHAPYsBQAAGlIAAKAhBQAAGlIAAKAhBQAAGlIAAKAhBQAAGlIAAKAhBQAAGlIAAKAhBQAAGlIAAKAhBQAAGlp4V+GpaUoP2Siu5I1T+eg4XdwFOU7mozc30tlpvdbNprP709nC5Y6IiPWtrXw4f7kjztZWb8b2iXR03Ppgbfbp/Oc9fSi/bvvR69fT2YiIjcrXq3qPnymsl916pDQ7Iv/dnj11OJ299elD6WxExAeO5q/Zma3as/S2t19OZ8/9/T3p7PGvP5TORkRsHj6bzi6/7Yel2dt/cXcpvxO/AABAQwoAADSkAABAQwoAADSkAABAQwoAADSkAABAQwoAADSkAABAQwoAADSkAABAQwoAADSkAABAQwoAADSkAABAQ9MYY6Elz/cdfPMr/V5eGdvbpfjYKuTnhexU7GbTVMtXFK951rS8vCtzIyLG9lbtBQqf97S2ms6Oa9fT2bLiLTot5T/v6udVmn3n7ens9bv2pbMRETfyo2PpVu0D2zyev+ar/5m/3ttr6WhERBz9xrV0dqwslWZPF6+ks89e/eqOx/gFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoCEFAAAaUgAAoKGFdyxO64W1hhv5aEREnMxHx6na6OnEPD+78L7j0fzciKhd89O1FZZRueaF7NgoruQ9XejDp2ujS9fszObuzI2IWC9kK9+PiBiPLbTJ/OVVPuuIiCfy35GxUVir+4l8NCJi/pH8qu756do64NmD+Wt28HOX0tnrv3k4nY2I2H53fgXz7Ld/VJo9PndXKb8TvwAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEPTGGOhpdr3rf1yYUptj3TMCj1lPq/N3srvzx7V2RWFaz4t1XrhKFyz8udVUbnPqnbrvItfzVjo6fEKqbz36vsuzJ4dye+n3z5yID84Iq6+biWdvXl8976bB76fv+Br5zZLs8dy/rkwbdWu2ezc5XT22atf3fn1068OAOxZCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANLS88JHbj+anzDby2YiIE4WViqdrez/HvHDe41Q+ezIfjYiIx5bS0bF1oja7ct6FaKwXshER88J5T8V7fLfOe1RvtMJ5V865OLr8BRv54fN351e8zv6kttp2dscz6ewdX3moNPv6p/answd+51I6e+3n/zSdjYhY+9o709np48dLs2Ojtv55J34BAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaGgaYyy0L/e+1Te9wm/lfzHPr/Qd88Iq4YiIan63LOXXAcf2dm32VMkWOule/awiIma71MV385pV7pOIvvdKwbS8+Ab4/2ncXVttO93ayofP59cBj63C3IiIe+5KR6fN2rN0PP9COvvt+Td2PMYvAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQ0OLLoU8WppwehXDEeLSwu/tUce/3qUJ2PR+dHlspDI6Is/m93/HwZmn09OG1fPh0vpOOh6/n50bEqJx34bOOiIjKPX6m0OMr93dE7bwrz5SIiI3Kc6E4u3LNK5919T57vPBc+cBzpdFjPX/Npify73t66lA6GxER7z2Xz55dLY2enjxayu/ELwAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0JACAAANTWOMhXb13ju7Pz+ltg14z5qW8yt5pwP7S7PHSmEd8GK3xE+O79+XD09TOjq7UlsHPL90uZSnkfxtWrObz9LqOU+F/zfnhRXKs9r/udO+wgrlgwdKsyuefeHLOx7jFwAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaGjhpfGnZkvpIevb2+nsS8Pz0dnTh0qjx+rj6ey09Wg6e+WvDqezERE3P7+Zzp68lv+sIyI+8oP84vD9bz+Xzo71dPS/nSxkN2qjC/d4FM67csYRxbOunPNuD5/O7M7o4j1++vaj+ewL50uzp7Mr+fDpkY6OWw/n50ZExJP56INXS5Ov/u2dpfxO/AIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQkAIAAA0pAADQ0DTGWGjP4r3T/ekhs2NH0tmIiLGUX087iissK669+Z50dns1v1I3ImLzUGF95uq8NPvIN/PZ2Vb+fc++86P84IiYVlfT2XErv345IiLmtWveUu0rUpy9N/93mtby93gU7/GxvZUPF9bRV01T/kabbjtQmr19OJ//j+9+acdj9uZdDACUKAAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0JACAAANKQAA0NDyogdOn7wjPWQ8cDGdjYgYH8yvkZyeOliavfnpo+nsgXfk19Ne/NLr0tmIiHgmf81mDy58W7ysaw/l12cefusP84M/e1c+GxHjd5/Lh9dLo2N6bCWdHScKa1pP5aMRUTvv6uyNQvZkcfaZQrZy3sX7bLzvej78udr3K0rfr+10dHrm9vzciBgPFFbKP5J/3xER8fFafCd+AQCAhhQAAGhIAQCAhhQAAGhIAQCAhhQAAGhIAQCAhhQAAGhIAQCAhhQAAGhIAQCAhhQAAGhIAQCAhhQAAGhIAQCAhqYxxljkwF84/CvpIQuO+MlW9+Wzxdnj/MV09sqvvT6dXb5ee9837s3viJ/fWirNPvb1/Hu/dSQ/e+1r309nIyKmtdV0dty4WZpNwrSbs/fm/06zI4fS2fmFSz/Fd/J/M915PJ0dz58rzZ4dui2dvfzGY6XZh/7lR+nss5tf2/GYvXkXAwAlCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANKQAAEBDCgAANLS86IFj+0R6yLRyNp2NiIhHttPRcba22vbmGz+bzh7+13els+e/+Jp0NiJiPL6Szi7fOF2afeUN6+ns0bc+l85uX35tOhsRMfvOOwvpjdLs6cMH09nx/quFyScL2YjSeZ/avdH14Wd2Z3T+qxUREfOL70lnp3215/h4OL+ifPpoPht3fSyfjYj5c3+Yzh76jcL7joiLq7Vn2k78AgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADSkAANCQAgAADU1jjLHIgfcu/1J+yPGj6WxExPbBfens0ou3SrPHj8+ls1ff8vp0dvXiPJ2NiLhxe77bjWItPPyta+nsrTvW0tmVb/4gnY2ImJaX09mxtVWaXTIVsgt9+/+fqpx3efbe/N9pWlvNZ/etlGbPL14u5bNmRw6X8rfuOpTOjqXS6NKz+Hv/9g87HrM372IAoEQBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFt5/uvWXd6eH7Pu9S+lsRMTSH+bz49PHS7M3/+bn0tmDb/teOjt98o50NiJifCy/9nPfH9yozX483ytXfiu/0nf20draz/l7CutK10ujYzpTWEX8aGEV8al8NCJq512dvVHInizOPlPIVs67eJ+N913Ph58orrxez+/GvfF3r01nV95bW62+/I7CmvHP3FmavfLPL5byO/ELAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0NI0xxiIHvuG+t+aHzBca8RMtXb2VD1+u7VMem/kd2NMdx/KDrxb2dkfEuH4jnZ2W83u7IyLGzfznNR3Yn59742Y6+9ILFPaGz2rXrGR7e/dm76ZpN2fvzf+dZkcO5cNbtfts/mL+WTw7eiQ/98KldDYi4sKv35POPvCmr5Zmf+Gpu9PZb8+/seMxe/MuBgBKFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaEgBAICGFAAAaGjhdcAAwM8OvwAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEMKAAA0pAAAQEP/BX0uH/AVCTUbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(156.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    anchor_img = np.zeros((*image.shape[:3], 3), dtype=np.uint8)\n",
    "    anchor_img = anchor_img[idx]\n",
    "\n",
    "    strides = [2, 4, 8]\n",
    "    colors = {\n",
    "        2: [0, 255, 0],  # 초록색\n",
    "        4: [0, 0, 255],  # 파란색\n",
    "        8: [255, 0, 0],   # 빨간색\n",
    "    }\n",
    "\n",
    "    for stride in strides:\n",
    "        color = colors[stride]\n",
    "        for y in range(0, anchor_img.shape[0], stride):\n",
    "            for x in range(0, anchor_img.shape[1], stride):\n",
    "                anchor_img[y, x, :] = color\n",
    "\n",
    "    # 이미지 표시\n",
    "    plt.imshow(image[idx], alpha=1)  \n",
    "    plt.imshow(anchor_img, alpha=0.5) \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n",
      "tf.Tensor(\n",
      "[[0.140625   0.625      0.28125    0.33333334]\n",
      " [0.90625    0.3125     0.1875     0.2916667 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor([1 1 0 0], shape=(4,), dtype=int64)\n",
      "tf.Tensor(235.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "width:  32\n",
      "height:  24\n",
      "bbox:  tf.Tensor(\n",
      "[[0.         0.4583333  0.28125    0.33333334]\n",
      " [0.8125     0.16666666 0.1875     0.2916667 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor([0.         0.4583333  0.28125    0.33333334], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0.8125     0.16666666 0.1875     0.2916667 ], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for image, bbox, label in val_dataset.take(1):\n",
    "    image, bbox, label = preprocess_data(image, bbox, label)\n",
    "    img = image[idx]\n",
    "    box = bbox[idx]\n",
    "    label = label[idx]\n",
    "    print(img.shape)\n",
    "    print(box)\n",
    "    print(label)\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))\n",
    "    # 이미지 시각화\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    print(\"width: \", width)\n",
    "    print(\"height: \", height)\n",
    "    boxes = tf.stack(\n",
    "        [\n",
    "            (box[:, 0] - 0.5 * box[:, 2]),  # xmin = x_center - width/2\n",
    "            (box[:, 1] - 0.5 * box[:, 3]),  # ymin = y_center - height/2\n",
    "            box[:, 2],\n",
    "            box[:, 3],\n",
    "            \n",
    "        ], axis=-1\n",
    "    )\n",
    "    print(\"bbox: \", boxes)\n",
    "    # 각 바운딩 박스에 대해 반복하여 그리기\n",
    "    for box in boxes:\n",
    "        xmin, ymin, w, h = box\n",
    "        print(box)\n",
    "        patch = plt.Rectangle(\n",
    "            [xmin * RES_WIDTH, ymin * RES_HEIGHT], w * RES_WIDTH, h * RES_HEIGHT, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "    plt.show()\n",
    "    print(label)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n",
      "tf.Tensor(\n",
      "[[0.171875   0.2916667  0.28125    0.33333334]\n",
      " [0.4375     0.16666667 0.25       0.33333334]\n",
      " [0.75       0.5208334  0.3125     0.2916667 ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor([1 1 1 0], shape=(4,), dtype=int64)\n",
      "tf.Tensor(246.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "width:  32\n",
      "height:  24\n",
      "bbox:  tf.Tensor(\n",
      "[[0.03125    0.12500001 0.28125    0.33333334]\n",
      " [0.3125     0.         0.25       0.33333334]\n",
      " [0.59375    0.37500003 0.3125     0.2916667 ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor([0.03125    0.12500001 0.28125    0.33333334], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0.3125     0.         0.25       0.33333334], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0.59375    0.37500003 0.3125     0.2916667 ], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 1 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    image, bbox, label = preprocess_data(image, bbox, label)\n",
    "    img = image[idx]\n",
    "    box = bbox[idx]\n",
    "    label = label[idx]\n",
    "    print(img.shape)\n",
    "    print(box)\n",
    "    print(label)\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))\n",
    "    # 이미지 시각화\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    print(\"width: \", width)\n",
    "    print(\"height: \", height)\n",
    "    boxes = tf.stack(\n",
    "        [\n",
    "            (box[:, 0] - 0.5 * box[:, 2]),  # xmin = x_center - width/2\n",
    "            (box[:, 1] - 0.5 * box[:, 3]),  # ymin = y_center - height/2\n",
    "            box[:, 2],\n",
    "            box[:, 3],\n",
    "            \n",
    "        ], axis=-1\n",
    "    )\n",
    "    print(\"bbox: \", boxes)\n",
    "    # 각 바운딩 박스에 대해 반복하여 그리기\n",
    "    for box in boxes:\n",
    "        xmin, ymin, w, h = box\n",
    "        print(box)\n",
    "        patch = plt.Rectangle(\n",
    "            [xmin * RES_WIDTH, ymin * RES_HEIGHT], w * RES_WIDTH, h * RES_HEIGHT, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "    plt.show()\n",
    "    print(label)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBox:\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.6, 1.1, 1.6]         # 이거랑 2268\n",
    "        self.scales = [2** x for x in [0, 1/3, 2/3]] # 이걸로 바운딩박스 갯수 조절가능\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(0, 3)]\n",
    "        self._areas = [x ** 2 for x in [5.5, 6.2, 6.8]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        anchor_dims_all = []\n",
    "\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios: \n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis = -1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales: \n",
    "                    anchor_dims.append(scale * dims) \n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis = -2))\n",
    "        return anchor_dims_all \n",
    "    \n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        rx = tf.range(feature_width, dtype = tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype = tf.float32) + 0.5\n",
    "\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis = -1) * self._strides[level - 0] # stride시작점에 따라 바꿔야함 \n",
    "        centers = tf.expand_dims(centers, axis = -2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 0], [feature_height, feature_width, 1, 1] \n",
    "        )\n",
    "\n",
    "        anchors = tf.concat([centers, dims], axis=-1) \n",
    "\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i), # 올림\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i\n",
    "            )\n",
    "            for i in range(0, 3)\n",
    "        ]\n",
    "\n",
    "        return tf.concat(anchors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor 음수 값: False\n",
      "tf.Tensor(\n",
      "[[ 0.5        0.5        4.2602816  7.1004696]\n",
      " [ 0.5        0.5        5.3676186  8.946032 ]\n",
      " [ 0.5        0.5        6.7627754 11.271293 ]\n",
      " ...\n",
      " [30.        22.         8.601397   5.3758717]\n",
      " [30.        22.        10.837081   6.773174 ]\n",
      " [30.        22.        13.653866   8.533664 ]], shape=(9072, 4), dtype=float32)\n",
      "(9072, 4)\n",
      "(24, 32, 1)\n",
      "[[0.140625   0.4375     0.18026404 0.21850182]\n",
      " [0.734375   0.10416666 0.2174066  0.18117215]\n",
      " [0.734375   0.14583333 0.1331338  0.2958529 ]\n",
      " [0.078125   0.35416666 0.34511146 0.28759286]\n",
      " [0.703125   0.27083334 0.28615132 0.34685004]\n",
      " [0.640625   0.5208333  0.27391517 0.2282626 ]\n",
      " [0.609375   0.7708333  0.28615132 0.34685004]\n",
      " [0.109375   0.02083333 0.2174066  0.18117215]\n",
      " [0.796875   0.39583334 0.1331338  0.2958529 ]\n",
      " [0.46875    0.45833334 0.38903472 0.3241956 ]\n",
      " [0.796875   0.1875     0.2174066  0.18117215]\n",
      " [0.578125   0.7291667  0.28615132 0.34685004]\n",
      " [0.515625   0.3125     0.27391517 0.2282626 ]\n",
      " [0.828125   0.8125     0.16773808 0.37275133]\n",
      " [0.984375   0.02083333 0.22711846 0.27529505]\n",
      " [0.421875   0.7291667  0.18026404 0.21850182]\n",
      " [0.203125   0.3125     0.2174066  0.18117215]\n",
      " [0.71875    0.375      0.2032067  0.24631117]\n",
      " [0.640625   0.4375     0.16773808 0.37275133]\n",
      " [0.03125    0.9583333  0.32257053 0.39099458]\n",
      " [0.515625   0.5625     0.16773808 0.37275133]\n",
      " [0.453125   0.0625     0.16773808 0.37275133]\n",
      " [0.265625   0.9791667  0.22711846 0.27529505]\n",
      " [0.234375   0.7708333  0.2174066  0.18117215]\n",
      " [0.859375   0.27083334 0.22711846 0.27529505]\n",
      " [0.046875   0.02083333 0.21133673 0.4696372 ]\n",
      " [0.640625   0.22916667 0.18026404 0.21850182]\n",
      " [0.421875   0.6041667  0.2174066  0.18117215]\n",
      " [0.84375    0.5416667  0.25602442 0.31033263]\n",
      " [0.921875   0.6875     0.2174066  0.18117215]\n",
      " [0.765625   0.02083333 0.34511146 0.28759286]\n",
      " [0.046875   0.35416666 0.1331338  0.2958529 ]\n",
      " [0.234375   0.6458333  0.2174066  0.18117215]\n",
      " [0.078125   0.27083334 0.1331338  0.2958529 ]\n",
      " [0.453125   0.10416666 0.2174066  0.18117215]\n",
      " [0.765625   0.3125     0.1331338  0.2958529 ]\n",
      " [0.171875   0.9791667  0.16773808 0.37275133]\n",
      " [0.28125    0.375      0.32257053 0.39099458]\n",
      " [0.640625   0.5625     0.27391517 0.2282626 ]\n",
      " [0.984375   0.3125     0.27391517 0.2282626 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anchors = AnchorBox()\n",
    "anchor = anchors.get_anchors(24, 32)\n",
    "\n",
    "# 앵커 박스 정규화\n",
    "xmin = anchor[:, 0] / RES_WIDTH\n",
    "ymin = anchor[:, 1] / RES_HEIGHT\n",
    "xmax = anchor[:, 2] / RES_WIDTH\n",
    "ymax = anchor[:, 3] / RES_HEIGHT\n",
    "\n",
    "# 정규화된 좌표를 스택으로 결합\n",
    "normalized_anchor = tf.stack([xmin, ymin, xmax, ymax], axis=-1)\n",
    "\n",
    "has_negative_values = tf.reduce_any(tf.less(anchor, 0))\n",
    "print(\"Anchor 음수 값:\", has_negative_values.numpy())\n",
    "\n",
    "print(anchor)\n",
    "print(anchor.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bounding_boxes(data, num_samples):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    print(img.shape)\n",
    "    data_np = data.numpy()\n",
    "\n",
    "    if len(data) > num_samples:\n",
    "        sampled_indices = np.random.choice(len(data), num_samples, replace=False)\n",
    "        sample_data = data_np[sampled_indices]\n",
    "    else : \n",
    "        sample_data = data_np\n",
    "    print(sample_data)\n",
    "    for center_x, center_y, width, height in sample_data:\n",
    "        top_left_x = center_x - width / 2\n",
    "        top_left_y = center_y - height / 2\n",
    "\n",
    "        rect = patches.Rectangle((top_left_x * RES_WIDTH, top_left_y * RES_HEIGHT), width * RES_WIDTH, height * RES_HEIGHT, linewidth=0.8, edgecolor='white', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "draw_bounding_boxes(normalized_anchor, 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    print(boxes1_corners.shape)\n",
    "    print(boxes2_corners.shape)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])  \n",
    "    \n",
    "    intersection = tf.maximum(rd - lu, 0.0)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = (boxes1_corners[:, 2] - boxes1_corners[:, 0]) * (boxes1_corners[:, 3] - boxes1_corners[:, 1])\n",
    "    boxes2_area = (boxes2_corners[:, 2] - boxes2_corners[:, 0]) * (boxes2_corners[:, 3] - boxes2_corners[:, 1])\n",
    "    union_area = tf.maximum(boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8)\n",
    "\n",
    "    return intersection_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA.shape: (2, 4)\n",
      "GT.shape: (2, 4)\n",
      "GA (XYWH): tf.Tensor(\n",
      "[[60. 45. 20. 30.]\n",
      " [60. 45. 20. 30.]], shape=(2, 4), dtype=float64)\n",
      "GT (XYWH): tf.Tensor(\n",
      "[[60. 45. 20. 30.]\n",
      " [45. 60. 20. 30.]], shape=(2, 4), dtype=float64)\n",
      "(2, 4)\n",
      "(2, 4)\n",
      "IoU: tf.Tensor(\n",
      "[[1.         0.06666667]\n",
      " [1.         0.06666667]], shape=(2, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "GA = np.array([[50, 30, 70, 60], [50, 30, 70, 60]])  # 예: [xmin, ymin, xmax, ymax]\n",
    "GT = np.array([[50, 30, 70, 60], [35, 45, 55, 75]])  # 예: [xmin, ymin, xmax, ymax]\n",
    "\n",
    "print(\"GA.shape:\", GA.shape)\n",
    "print(\"GT.shape:\", GT.shape)\n",
    "\n",
    "GA_xywh = convert_to_xywh(GA)\n",
    "print(\"GA (XYWH):\", GA_xywh)\n",
    "\n",
    "GT_xywh = convert_to_xywh(GT)\n",
    "print(\"GT (XYWH):\", GT_xywh)\n",
    "\n",
    "iou = compute_iou(GA_xywh, GT_xywh)\n",
    "print(\"IoU:\", iou)\n",
    "# GA = convert_to_corners(GA)\n",
    "# print(GA)\n",
    "# GT = convert_to_corners(GT)\n",
    "# print(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGiCAYAAADNzj2mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAohElEQVR4nO3dfXSU9Z3//9fkbkiBTCTCTAJJCEgNqKAEDQN0e1azzWHRhSVa8dDdKLRs10gJUZFogSJiEHe9QblZuyy0KlLpCoq7QjFqPOyGuygW2hqgciRAZuiNmQE0E5p8fn/4Y76O4Ookg/nM8HycM6fOdV1zzfvTy5M8ncwkDmOMEQAAgEWSunsAAACAzyNQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHWiDpSTJ0+qsrJS+fn5Sk9P15gxY7R79+7wfmOM5s+fr+zsbKWnp6ukpEQHDx6M6dAAACCxRR0o3//+97Vt2zY9++yz2rdvn77zne+opKREx44dkyQtXbpUy5Yt06pVq7Rz50717NlTpaWlam1tjfnwAAAgMTmi+WOBn3zyiXr37q2XX35ZEyZMCG8vKirS+PHjtWjRIuXk5Ojuu+/WPffcI0kKBAJyu91au3atpkyZEvsVAACAhJMSzcF/+ctf1N7erh49ekRsT09P1/bt23X48GH5fD6VlJSE97lcLhUXF6u+vv68gRIKhRQKhcL3Ozo69Oc//1lZWVlyOBzRrgcAAHQDY4xOnjypnJwcJSV1/S2uUQVK79695fV6tWjRIg0dOlRut1svvPCC6uvrddlll8nn80mS3G53xOPcbnd43+fV1NRo4cKFnRwfAADYpKmpSQMGDOjyeaIKFEl69tlnNW3aNPXv31/JyckaOXKkbrvtNjU0NHRqgOrqalVVVYXvBwIB5eXlqampSRkZGZ06JwAA+HoFg0Hl5uaqd+/eMTlf1IEyePBg1dXV6fTp0woGg8rOztatt96qQYMGyePxSJL8fr+ys7PDj/H7/br66qvPez6n0ymn03nO9oyMDAIFAIA4E6u3Z3T6h0Q9e/ZUdna2PvroI23dulUTJ05UQUGBPB6Pamtrw8cFg0Ht3LlTXq83JgMDAIDEF/UrKFu3bpUxRpdffrkOHTqke++9V4WFhbrjjjvkcDhUWVmphx56SEOGDFFBQYHmzZunnJwcTZo06QKMDwAAElHUgRIIBFRdXa2jR4+qT58+Kisr0+LFi5WamipJmjNnjk6fPq0ZM2aopaVF48aN05YtW8755A8AAMAXier3oHwdgsGgXC6XAoEA70EBACBOxPr7N3+LBwAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWiSpQ2tvbNW/ePBUUFCg9PV2DBw/WokWLZIwJH2OM0fz585Wdna309HSVlJTo4MGDMR8cAAAkrqgC5ZFHHtHKlSv19NNP63e/+50eeeQRLV26VE899VT4mKVLl2rZsmVatWqVdu7cqZ49e6q0tFStra0xHx4AACQmh/nsyx9f4sYbb5Tb7dbq1avD28rKypSenq7nnntOxhjl5OTo7rvv1j333CNJCgQCcrvdWrt2raZMmfKlzxEMBuVyuRQIBJSRkdGJJQEAgK9brL9/R/UKypgxY1RbW6sDBw5Ikt577z1t375d48ePlyQdPnxYPp9PJSUl4ce4XC4VFxervr7+vOcMhUIKBoMRNwAAcHFLiebguXPnKhgMqrCwUMnJyWpvb9fixYs1depUSZLP55Mkud3uiMe53e7wvs+rqanRwoULOzM7AABIUFG9gvLiiy/q+eef17p16/TOO+/oZz/7mf7lX/5FP/vZzzo9QHV1tQKBQPjW1NTU6XMBAIDEENUrKPfee6/mzp0bfi/JVVddpQ8//FA1NTUqLy+Xx+ORJPn9fmVnZ4cf5/f7dfXVV5/3nE6nU06ns5PjAwCARBTVKygff/yxkpIiH5KcnKyOjg5JUkFBgTwej2pra8P7g8Ggdu7cKa/XG4NxAQDAxSCqV1BuuukmLV68WHl5ebriiiv07rvv6rHHHtO0adMkSQ6HQ5WVlXrooYc0ZMgQFRQUaN68ecrJydGkSZMuxPwAACABRRUoTz31lObNm6c777xTJ06cUE5Ojv7pn/5J8+fPDx8zZ84cnT59WjNmzFBLS4vGjRunLVu2qEePHjEfHgAAJKaofg/K14HfgwIAQPzp1t+DAgAA8HUgUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFgnqr9mDOBrMGqU5PN19xToLI9H2rOnu6cA4h6BAtjG55OOHevuKQCgWxEogK2SkqTs7O6eAl9Vc7PU0dHdUwAJg0ABbJWdLR092t1T4KsaMIBXvoAY4k2yAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwTlSBMnDgQDkcjnNuFRUVkqTW1lZVVFQoKytLvXr1UllZmfx+/wUZHAAAJK6oAmX37t1qbm4O37Zt2yZJuuWWWyRJs2fP1ubNm7VhwwbV1dXp+PHjmjx5cuynBgAACS0lmoP79u0bcX/JkiUaPHiwvv3tbysQCGj16tVat26drr/+eknSmjVrNHToUO3YsUOjR4+O3dQAACChdfo9KG1tbXruuec0bdo0ORwONTQ06MyZMyopKQkfU1hYqLy8PNXX13/heUKhkILBYMQNAABc3DodKJs2bVJLS4tuv/12SZLP51NaWpoyMzMjjnO73fL5fF94npqaGrlcrvAtNze3syMBAIAE0elAWb16tcaPH6+cnJwuDVBdXa1AIBC+NTU1del8AAAg/kX1HpSzPvzwQ73++ut66aWXwts8Ho/a2trU0tIS8SqK3++Xx+P5wnM5nU45nc7OjAEAABJUp15BWbNmjfr166cJEyaEtxUVFSk1NVW1tbXhbY2NjTpy5Ii8Xm/XJwUAABeNqF9B6ejo0Jo1a1ReXq6UlP/3cJfLpenTp6uqqkp9+vRRRkaGZs6cKa/Xyyd4AABAVKIOlNdff11HjhzRtGnTztn3+OOPKykpSWVlZQqFQiotLdWKFStiMigAALh4OIwxpruH+KxgMCiXy6VAIKCMjIzuHgf4+g0YIB07JvXvLx092t3T4KviuuEiF+vv3/wtHgAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYJ+pAOXbsmL73ve8pKytL6enpuuqqq7Rnz57wfmOM5s+fr+zsbKWnp6ukpEQHDx6M6dAAACCxRRUoH330kcaOHavU1FS99tpr+u1vf6t//dd/1SWXXBI+ZunSpVq2bJlWrVqlnTt3qmfPniotLVVra2vMhwcAAIkpJZqDH3nkEeXm5mrNmjXhbQUFBeF/NsboiSee0I9//GNNnDhRkvTzn/9cbrdbmzZt0pQpU2I0NgAASGRRBcorr7yi0tJS3XLLLaqrq1P//v1155136gc/+IEk6fDhw/L5fCopKQk/xuVyqbi4WPX19ecNlFAopFAoFL4fDAY7uxYAiDBqlOTzfT3PtbtZypbU3CxdO+Drec5E5/FIn3kHAS4yUQXKBx98oJUrV6qqqkr333+/du/erR/96EdKS0tTeXm5fP//VwK32x3xOLfbHd73eTU1NVq4cGEnxweAL+bzSceOfT3P1X72fzu+vucEEllUgdLR0aFRo0bp4YcfliRdc8012r9/v1atWqXy8vJODVBdXa2qqqrw/WAwqNzc3E6dCwDOJylJys6+sM+R3CypQ0pOkvpf4OdKdM3NUkdHd0+B7hZVoGRnZ2vYsGER24YOHar//M//lCR5PB5Jkt/vV/Znvhr4/X5dffXV5z2n0+mU0+mMZgwAiEp2tnT06AV+kgGSjn1Nz5XgBgzgVShE+SmesWPHqrGxMWLbgQMHlJ+fL+nTN8x6PB7V1taG9weDQe3cuVNerzcG4wIAgItBVK+gzJ49W2PGjNHDDz+s7373u9q1a5eeeeYZPfPMM5Ikh8OhyspKPfTQQxoyZIgKCgo0b9485eTkaNKkSRdifgAAkICiCpRrr71WGzduVHV1tR588EEVFBToiSee0NSpU8PHzJkzR6dPn9aMGTPU0tKicePGacuWLerRo0fMhwcAAIkpqkCRpBtvvFE33njjF+53OBx68MEH9eCDD3ZpMAAAcPHib/EAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDpRBcpPfvITORyOiFthYWF4f2trqyoqKpSVlaVevXqprKxMfr8/5kMDAIDEFvUrKFdccYWam5vDt+3bt4f3zZ49W5s3b9aGDRtUV1en48ePa/LkyTEdGAAAJL6UqB+QkiKPx3PO9kAgoNWrV2vdunW6/vrrJUlr1qzR0KFDtWPHDo0ePbrr0wIAgItC1K+gHDx4UDk5ORo0aJCmTp2qI0eOSJIaGhp05swZlZSUhI8tLCxUXl6e6uvrv/B8oVBIwWAw4gYAAC5uUQVKcXGx1q5dqy1btmjlypU6fPiwvvWtb+nkyZPy+XxKS0tTZmZmxGPcbrd8Pt8XnrOmpkYulyt8y83N7dRCAABA4ojqRzzjx48P//Pw4cNVXFys/Px8vfjii0pPT+/UANXV1aqqqgrfDwaDRAoAABe5Ln3MODMzU9/85jd16NAheTwetbW1qaWlJeIYv99/3vesnOV0OpWRkRFxAwAAF7cuBcqpU6f0+9//XtnZ2SoqKlJqaqpqa2vD+xsbG3XkyBF5vd4uDwoAAC4eUf2I55577tFNN92k/Px8HT9+XAsWLFBycrJuu+02uVwuTZ8+XVVVVerTp48yMjI0c+ZMeb1ePsEDAACiElWgHD16VLfddpv+9Kc/qW/fvho3bpx27Nihvn37SpIef/xxJSUlqaysTKFQSKWlpVqxYsUFGRwAACSuqAJl/fr1/+f+Hj16aPny5Vq+fHmXhgIAABc3/hYPAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOlH9sUAAX6PmZmnAgO6eIq7tbpbaJSU3S7rQ/1c2N1/gJwAuLgQKYKuODunYse6eIq5ln/2HDkn8XwnEFQIFsI3H090TJIzmZqm9Q0pOkrKzv/z4mOD6ATFBoAC22bOnuydIGNcO+PRFqP7Z0tGj3T0NgGjwJlkAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFinS4GyZMkSORwOVVZWhre1traqoqJCWVlZ6tWrl8rKyuT3+7s6JwAAuIh0OlB2796tf/u3f9Pw4cMjts+ePVubN2/Whg0bVFdXp+PHj2vy5MldHhQAAFw8OhUop06d0tSpU/XTn/5Ul1xySXh7IBDQ6tWr9dhjj+n6669XUVGR1qxZo//93//Vjh07YjY0AABIbJ0KlIqKCk2YMEElJSUR2xsaGnTmzJmI7YWFhcrLy1N9ff15zxUKhRQMBiNuAADg4pYS7QPWr1+vd955R7t37z5nn8/nU1pamjIzMyO2u91u+Xy+856vpqZGCxcujHYMAACQwKJ6BaWpqUmzZs3S888/rx49esRkgOrqagUCgfCtqakpJucFAADxK6pAaWho0IkTJzRy5EilpKQoJSVFdXV1WrZsmVJSUuR2u9XW1qaWlpaIx/n9fnk8nvOe0+l0KiMjI+IGAAAublH9iOeGG27Qvn37IrbdcccdKiws1H333afc3FylpqaqtrZWZWVlkqTGxkYdOXJEXq83dlMDAICEFlWg9O7dW1deeWXEtp49eyorKyu8ffr06aqqqlKfPn2UkZGhmTNnyuv1avTo0bGbGgAAJLSo3yT7ZR5//HElJSWprKxMoVBIpaWlWrFiRayfBgAAJDCHMcZ09xCfFQwG5XK5FAgEeD8KgC4ZMEA6dkzq3186erS7p8FXxXWLT7H+/s3f4gEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdaIKlJUrV2r48OHKyMhQRkaGvF6vXnvttfD+1tZWVVRUKCsrS7169VJZWZn8fn/MhwYAAIktqkAZMGCAlixZooaGBu3Zs0fXX3+9Jk6cqN/85jeSpNmzZ2vz5s3asGGD6urqdPz4cU2ePPmCDA4AABKXwxhjunKCPn366NFHH9XNN9+svn37at26dbr55pslSe+//76GDh2q+vp6jR49+iudLxgMyuVyKRAIKCMjoyujAbjIDRggHTsm9e8vHT3a3dPgq+K6xadYf/9O6ewD29vbtWHDBp0+fVper1cNDQ06c+aMSkpKwscUFhYqLy/v/wyUUCikUCgUvh8MBjs7EgCcV3Pzp9/0EB+am7t7Atgg6kDZt2+fvF6vWltb1atXL23cuFHDhg3T3r17lZaWpszMzIjj3W63fD7fF56vpqZGCxcujHpwAPiqOjo+/S9yAPEj6kC5/PLLtXfvXgUCAf3yl79UeXm56urqOj1AdXW1qqqqwveDwaByc3M7fT4AOMvj6e4J0BVcv4tb1IGSlpamyy67TJJUVFSk3bt368knn9Stt96qtrY2tbS0RLyK4vf75fk//i1zOp1yOp3RTw4AX2LPnu6eAEBndfn3oHR0dCgUCqmoqEipqamqra0N72tsbNSRI0fk9Xq7+jQAAOAiEtUrKNXV1Ro/frzy8vJ08uRJrVu3Tm+99Za2bt0ql8ul6dOnq6qqSn369FFGRoZmzpwpr9f7lT/BAwAAIEUZKCdOnNA//uM/qrm5WS6XS8OHD9fWrVv1N3/zN5Kkxx9/XElJSSorK1MoFFJpaalWrFhxQQYHAACJq8u/ByXW+D0oAADEn1h//+Zv8QAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOlEFSk1Nja699lr17t1b/fr106RJk9TY2BhxTGtrqyoqKpSVlaVevXqprKxMfr8/pkMDAIDEFlWg1NXVqaKiQjt27NC2bdt05swZfec739Hp06fDx8yePVubN2/Whg0bVFdXp+PHj2vy5MkxHxwAACQuhzHGdPbBf/jDH9SvXz/V1dXpr/7qrxQIBNS3b1+tW7dON998syTp/fff19ChQ1VfX6/Ro0d/6TmDwaBcLpcCgYAyMjI6OxoAAPgaxfr7d5fegxIIBCRJffr0kSQ1NDTozJkzKikpCR9TWFiovLw81dfXn/ccoVBIwWAw4gYAAC5unQ6Ujo4OVVZWauzYsbryyislST6fT2lpacrMzIw41u12y+fznfc8NTU1crlc4Vtubm5nRwIAAAmi04FSUVGh/fv3a/369V0aoLq6WoFAIHxramrq0vkAAED8S+nMg+666y69+uqrevvttzVgwIDwdo/Ho7a2NrW0tES8iuL3++XxeM57LqfTKafT2ZkxAABAgorqFRRjjO666y5t3LhRb7zxhgoKCiL2FxUVKTU1VbW1teFtjY2NOnLkiLxeb2wmBgAACS+qV1AqKiq0bt06vfzyy+rdu3f4fSUul0vp6elyuVyaPn26qqqq1KdPH2VkZGjmzJnyer1f6RM8AAAAUpQfM3Y4HOfdvmbNGt1+++2SPv1FbXfffbdeeOEFhUIhlZaWasWKFV/4I57P42PGAADEn1h//+7S70G5EAgUAADij1W/BwUAAOBCIFAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYJ2oA+Xtt9/WTTfdpJycHDkcDm3atClivzFG8+fPV3Z2ttLT01VSUqKDBw/Gal4AAHARiDpQTp8+rREjRmj58uXn3b906VItW7ZMq1at0s6dO9WzZ0+VlpaqtbW1y8MCAICLQ0q0Dxg/frzGjx9/3n3GGD3xxBP68Y9/rIkTJ0qSfv7zn8vtdmvTpk2aMmXKOY8JhUIKhULh+8FgMNqRAABAgonpe1AOHz4sn8+nkpKS8DaXy6Xi4mLV19ef9zE1NTVyuVzhW25ubixHAgAAcSimgeLz+SRJbrc7Yrvb7Q7v+7zq6moFAoHwrampKZYjAQCAOBT1j3hizel0yul0dvcYAADAIjF9BcXj8UiS/H5/xHa/3x/eBwAA8GViGigFBQXyeDyqra0NbwsGg9q5c6e8Xm8snwoAACSwqH/Ec+rUKR06dCh8//Dhw9q7d6/69OmjvLw8VVZW6qGHHtKQIUNUUFCgefPmKScnR5MmTYrl3AAAIIFFHSh79uzRX//1X4fvV1VVSZLKy8u1du1azZkzR6dPn9aMGTPU0tKicePGacuWLerRo0fspgYAAAnNYYwx3T3EZwWDQblcLgUCAWVkZHT3OAAA4CuI9fdv/hYPAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxzwQJl+fLlGjhwoHr06KHi4mLt2rXrQj0VAABIMBckUH7xi1+oqqpKCxYs0DvvvKMRI0aotLRUJ06cuBBPBwAAEozDGGNifdLi4mJde+21evrppyVJHR0dys3N1cyZMzV37tyIY0OhkEKhUPh+IBBQXl6empqalJGREevRAADABRAMBpWbm6uWlha5XK4uny8lBjNFaGtrU0NDg6qrq8PbkpKSVFJSovr6+nOOr6mp0cKFC8/ZnpubG+vRAADABfanP/3JzkD54x//qPb2drnd7ojtbrdb77///jnHV1dXq6qqKny/paVF+fn5OnLkSEwWaJOzdZmorw4l8vpYW3xibfGJtcWnsz8B6dOnT0zOF/NAiZbT6ZTT6Txnu8vlSriLd1ZGRkbCrk1K7PWxtvjE2uITa4tPSUmxeXtrzN8ke+mllyo5OVl+vz9iu9/vl8fjifXTAQCABBTzQElLS1NRUZFqa2vD2zo6OlRbWyuv1xvrpwMAAAnogvyIp6qqSuXl5Ro1apSuu+46PfHEEzp9+rTuuOOOL32s0+nUggULzvtjn3iXyGuTEnt9rC0+sbb4xNriU6zXdkE+ZixJTz/9tB599FH5fD5dffXVWrZsmYqLiy/EUwEAgARzwQIFAACgs/hbPAAAwDoECgAAsA6BAgAArEOgAAAA61gXKMuXL9fAgQPVo0cPFRcXa9euXd09UtTefvtt3XTTTcrJyZHD4dCmTZsi9htjNH/+fGVnZys9PV0lJSU6ePBg9wwbpZqaGl177bXq3bu3+vXrp0mTJqmxsTHimNbWVlVUVCgrK0u9evVSWVnZOb+4z0YrV67U8OHDw7/h0ev16rXXXgvvj9d1nc+SJUvkcDhUWVkZ3hav6/vJT34ih8MRcSssLAzvj9d1nXXs2DF973vfU1ZWltLT03XVVVdpz5494f3x/PVk4MCB51w7h8OhiooKSfF77drb2zVv3jwVFBQoPT1dgwcP1qJFi/TZz6TE83U7efKkKisrlZ+fr/T0dI0ZM0a7d+8O74/Z2oxF1q9fb9LS0sx//Md/mN/85jfmBz/4gcnMzDR+v7+7R4vKf//3f5sHHnjAvPTSS0aS2bhxY8T+JUuWGJfLZTZt2mTee+8983d/93emoKDAfPLJJ90zcBRKS0vNmjVrzP79+83evXvN3/7t35q8vDxz6tSp8DE//OEPTW5urqmtrTV79uwxo0ePNmPGjOnGqb+aV155xfzXf/2XOXDggGlsbDT333+/SU1NNfv37zfGxO+6Pm/Xrl1m4MCBZvjw4WbWrFnh7fG6vgULFpgrrrjCNDc3h29/+MMfwvvjdV3GGPPnP//Z5Ofnm9tvv93s3LnTfPDBB2br1q3m0KFD4WPi+evJiRMnIq7btm3bjCTz5ptvGmPi99otXrzYZGVlmVdffdUcPnzYbNiwwfTq1cs8+eST4WPi+bp997vfNcOGDTN1dXXm4MGDZsGCBSYjI8McPXrUGBO7tVkVKNddd52pqKgI329vbzc5OTmmpqamG6fqms8HSkdHh/F4PObRRx8Nb2tpaTFOp9O88MIL3TBh15w4ccJIMnV1dcaYT9eSmppqNmzYED7md7/7nZFk6uvru2vMTrvkkkvMv//7vyfMuk6ePGmGDBlitm3bZr797W+HAyWe17dgwQIzYsSI8+6L53UZY8x9991nxo0b94X7E+3ryaxZs8zgwYNNR0dHXF+7CRMmmGnTpkVsmzx5spk6daoxJr6v28cff2ySk5PNq6++GrF95MiR5oEHHojp2qz5EU9bW5saGhpUUlIS3paUlKSSkhLV19d342SxdfjwYfl8voh1ulwuFRcXx+U6A4GAJIX/emVDQ4POnDkTsb7CwkLl5eXF1fra29u1fv16nT59Wl6vN2HWVVFRoQkTJkSsQ4r/63bw4EHl5ORo0KBBmjp1qo4cOSIp/tf1yiuvaNSoUbrlllvUr18/XXPNNfrpT38a3p9IX0/a2tr03HPPadq0aXI4HHF97caMGaPa2lodOHBAkvTee+9p+/btGj9+vKT4vm5/+ctf1N7erh49ekRsT09P1/bt22O6tm7/a8Zn/fGPf1R7e7vcbnfEdrfbrffff7+bpoo9n88nSedd59l98aKjo0OVlZUaO3asrrzySkmfri8tLU2ZmZkRx8bL+vbt2yev16vW1lb16tVLGzdu1LBhw7R37964XpckrV+/Xu+8807Ez4rPiufrVlxcrLVr1+ryyy9Xc3OzFi5cqG9961vav39/XK9Lkj744AOtXLlSVVVVuv/++7V792796Ec/UlpamsrLyxPq68mmTZvU0tKi22+/XVJ8/zs5d+5cBYNBFRYWKjk5We3t7Vq8eLGmTp0qKb6/D/Tu3Vter1eLFi3S0KFD5Xa79cILL6i+vl6XXXZZTNdmTaAg/lRUVGj//v3avn17d48SM5dffrn27t2rQCCgX/7ylyovL1ddXV13j9VlTU1NmjVrlrZt23bOf/nEu7P/VSpJw4cPV3FxsfLz8/Xiiy8qPT29Gyfruo6ODo0aNUoPP/ywJOmaa67R/v37tWrVKpWXl3fzdLG1evVqjR8/Xjk5Od09Spe9+OKLev7557Vu3TpdccUV2rt3ryorK5WTk5MQ1+3ZZ5/VtGnT1L9/fyUnJ2vkyJG67bbb1NDQENPnseZHPJdeeqmSk5PPeYe23++Xx+Pppqli7+xa4n2dd911l1599VW9+eabGjBgQHi7x+NRW1ubWlpaIo6Pl/WlpaXpsssuU1FRkWpqajRixAg9+eSTcb+uhoYGnThxQiNHjlRKSopSUlJUV1enZcuWKSUlRW63O67X91mZmZn65je/qUOHDsX9dcvOztawYcMitg0dOjT8I6xE+Xry4Ycf6vXXX9f3v//98LZ4vnb33nuv5s6dqylTpuiqq67SP/zDP2j27NmqqamRFP/XbfDgwaqrq9OpU6fU1NSkXbt26cyZMxo0aFBM12ZNoKSlpamoqEi1tbXhbR0dHaqtrZXX6+3GyWKroKBAHo8nYp3BYFA7d+6Mi3UaY3TXXXdp48aNeuONN1RQUBCxv6ioSKmpqRHra2xs1JEjR+JifZ/X0dGhUCgU9+u64YYbtG/fPu3duzd8GzVqlKZOnRr+53he32edOnVKv//975WdnR33123s2LHnfIz/wIEDys/PlxT/X0/OWrNmjfr166cJEyaEt8Xztfv444+VlBT57TU5OVkdHR2SEue69ezZU9nZ2froo4+0detWTZw4MbZri8W7emNl/fr1xul0mrVr15rf/va3ZsaMGSYzM9P4fL7uHi0qJ0+eNO+++6559913jSTz2GOPmXfffdd8+OGHxphPP4KVmZlpXn75ZfPrX//aTJw4MW4+XvbP//zPxuVymbfeeivi44Eff/xx+Jgf/vCHJi8vz7zxxhtmz549xuv1Gq/X241TfzVz5841dXV15vDhw+bXv/61mTt3rnE4HOZXv/qVMSZ+1/VFPvspHmPid3133323eeutt8zhw4fN//zP/5iSkhJz6aWXmhMnThhj4nddxnz6kfCUlBSzePFic/DgQfP888+bb3zjG+a5554LHxPPX0+M+fTTmnl5eea+++47Z1+8Xrvy8nLTv3//8MeMX3rpJXPppZeaOXPmhI+J5+u2ZcsW89prr5kPPvjA/OpXvzIjRowwxcXFpq2tzRgTu7VZFSjGGPPUU0+ZvLw8k5aWZq677jqzY8eO7h4pam+++aaRdM6tvLzcGPPpR8zmzZtn3G63cTqd5oYbbjCNjY3dO/RXdL51STJr1qwJH/PJJ5+YO++801xyySXmG9/4hvn7v/9709zc3H1Df0XTpk0z+fn5Ji0tzfTt29fccMMN4TgxJn7X9UU+Hyjxur5bb73VZGdnm7S0NNO/f39z6623RvyekHhd11mbN282V155pXE6naawsNA888wzEfvj+euJMcZs3brVSDrvzPF67YLBoJk1a5bJy8szPXr0MIMGDTIPPPCACYVC4WPi+br94he/MIMGDTJpaWnG4/GYiooK09LSEt4fq7U5jPnMr7YDAACwgDXvQQEAADiLQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1/j8XOrP0I8ZpLAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# 주어진 바운딩 박스 데이터\n",
    "box1 = [50, 30, 70, 60]  # [x_min, y_min, x_max, y_max]\n",
    "box2 = [35, 45, 55, 75]\n",
    "\n",
    "# 그림 생성\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 첫 번째 바운딩 박스 추가\n",
    "rect1 = patches.Rectangle((box1[0], box1[1]), box1[2] - box1[0], box1[3] - box1[1], \n",
    "                          linewidth=2, edgecolor='blue', facecolor='none')\n",
    "ax.add_patch(rect1)\n",
    "\n",
    "# 두 번째 바운딩 박스 추가\n",
    "rect2 = patches.Rectangle((box2[0], box2[1]), box2[2] - box2[0], box2[3] - box2[1], \n",
    "                          linewidth=2, edgecolor='red', facecolor='none')\n",
    "ax.add_patch(rect2)\n",
    "\n",
    "# 축 범위 설정\n",
    "ax.set_xlim(0, 90)\n",
    "ax.set_ylim(0, 90)\n",
    "\n",
    "# 그림 표시\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_anchor_boxes(anchor_boxes, gt_boxes, match_iou = 0.5, ignore_iou = 0.4):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        print(\"iou_matrix:  \", iou_matrix)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        print(\"max_iou:  \", max_iou)\n",
    "\n",
    "\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis = 1)\n",
    "        print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        print(\"positive_mask:  \", positive_mask)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        print(\"negative_mask:  \", negative_mask)\n",
    "\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        print(\"ignore_mask:  \", ignore_mask)\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype = tf.float32),\n",
    "            tf.cast(ignore_mask, dtype = tf.float32),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(4, 4)\n",
      "iou_matrix:   tf.Tensor(\n",
      "[[1.         0.03100775 0.         0.        ]\n",
      " [0.03100775 1.         0.         0.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.06003535 0.        ]\n",
      " [0.         0.         0.09293901 0.        ]\n",
      " [0.         0.         0.11408175 0.        ]\n",
      " [0.16311106 0.         0.         0.        ]], shape=(8, 4), dtype=float32)\n",
      "max_iou:   tf.Tensor(\n",
      "[1.         1.         1.         0.         0.06003535 0.09293901\n",
      " 0.11408175 0.16311106], shape=(8,), dtype=float32)\n",
      "matched_gt_idx:   tf.Tensor([0 1 2 0 2 2 2 0], shape=(8,), dtype=int64)\n",
      "positive_mask:   tf.Tensor([ True  True  True False False False False False], shape=(8,), dtype=bool)\n",
      "negative_mask:   tf.Tensor([False False False  True  True  True  True  True], shape=(8,), dtype=bool)\n",
      "ignore_mask:   tf.Tensor([False False False False False False False False], shape=(8,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "anchor = np.array([\n",
    "                    [27.,  18.5,  8.,   7.],\n",
    "                    [18.5, 15.5, 11.,   7.],\n",
    "                    [ 6.,   4.,   8.,   6.],\n",
    "                    [ 0.,   0.,   0.,   0.],\n",
    "                    [ 1., 1., 4.242641, 8.485281 ],\n",
    "                    [ 1.,         1.,         5.3453927, 10.690784 ],\n",
    "                    [ 1.,         1.,         6.7347727, 13.469543 ],\n",
    "                    [30.,        22.,         9.899496,   4.949747 ]])\n",
    "\n",
    "gt_boxes = np.array([[27.,  18.5,  8.,   7., ],\n",
    "                     [18.5, 15.5, 11.,   7. ],\n",
    "                     [ 6.,   4.,   8.,   6. ],\n",
    "                     [ 0.,   0.,   0.,   0. ]])\n",
    "# print(gt_boxes.shape)\n",
    "a, b, c = match_anchor_boxes(tf.cast(anchor, tf.float32), tf.cast(gt_boxes, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    \n",
    "    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou = 0.5, ignore_iou = 0.4):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        print(\"iou_matrix:  \", iou_matrix.shape)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        print(\"max_iou:  \", max_iou.shape)\n",
    "\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis = 1)\n",
    "        print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "        print(\"max_iou:\", max_iou)\n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        print(\"positive_mask:  \", positive_mask)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        print(\"negative_mask:  \", negative_mask.shape)\n",
    "\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        print(\"ignore_mask:  \", ignore_mask.shape)\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype = tf.float32),\n",
    "            tf.cast(ignore_mask, dtype = tf.float32),\n",
    "        )\n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        print(\"_compute_box_target anchor_boxes: \", anchor_boxes)\n",
    "        print(\"_compute_box_target matched_gt_boxes : \", matched_gt_boxes)\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:])\n",
    "            ],\n",
    "            axis = -1,\n",
    "        )\n",
    "        print(\"box_target:  \", box_target)\n",
    "        box_target = box_target / self._box_variance\n",
    "        print(\"box_target:  \", box_target)\n",
    "        return box_target\n",
    "    \n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):        \n",
    "        print(\"image_shape:\", image_shape.shape)\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        # 앵커 박스 정규화\n",
    "        xmin = anchor_boxes[:, 0] / RES_WIDTH\n",
    "        ymin = anchor_boxes[:, 1] / RES_HEIGHT\n",
    "        xmax = anchor_boxes[:, 2] / RES_WIDTH\n",
    "        ymax = anchor_boxes[:, 3] / RES_HEIGHT\n",
    "\n",
    "        # 정규화된 좌표를 스택으로 결합\n",
    "        normalized_anchor = tf.stack([xmin, ymin, xmax, ymax], axis=-1)\n",
    "        \n",
    "        print(\"anchor_boxes  : \", normalized_anchor)\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        print(\"cls_ids\", cls_ids)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            normalized_anchor, gt_boxes\n",
    "        )\n",
    "        print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "        print(\"positive_mask:  \", positive_mask)\n",
    "        print(\"ignore_mask:  \", ignore_mask)\n",
    "\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "\n",
    "        print(\"matched_gt_boxes:  \", matched_gt_boxes)\n",
    "        \n",
    "        box_target = self._compute_box_target(normalized_anchor, matched_gt_boxes)\n",
    "        print(\"box_target:  \", box_target)\n",
    "\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        print(\"matched_gt_cls_ids:  \", matched_gt_cls_ids)\n",
    "        \n",
    "        cls_target = tf.where(tf.cast(positive_mask, tf.bool), matched_gt_cls_ids, -1.0)\n",
    "        cls_target = tf.where(tf.cast(ignore_mask, tf.bool), -2.0, cls_target)\n",
    "\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "        num_ones = tf.math.count_nonzero(tf.equal(cls_target, 1.0))\n",
    "        print(\"Number of 1.0 values in cls_target:\", num_ones)\n",
    "\n",
    "\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        print(\"label:  \", label)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):       \n",
    "        images_shape = tf.shape(batch_images)\n",
    "        print(\"images_shape:  \", images_shape)\n",
    "        batch_size = images_shape[0]\n",
    "        print(\"batch_size:  \", batch_size)\n",
    "\n",
    "        print(\"gt_boxes: \", gt_boxes)\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        print(\"labels:  \", labels)\n",
    "        # batch_size_val = batch_size.numpy()\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            print(\"label:  \", label)\n",
    "            labels = labels.write(i, label)\n",
    "        return batch_images, labels.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Eager execution: \", tf.executing_eagerly())\n",
    "if not tf.executing_eagerly():\n",
    "    tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 32, 1) (1, 4, 4) (1, 4)\n",
      "images_shape:   tf.Tensor([ 1 24 32  1], shape=(4,), dtype=int32)\n",
      "batch_size:   tf.Tensor(1, shape=(), dtype=int32)\n",
      "gt_boxes:  tf.Tensor(\n",
      "[[[0.171875 0.1875   0.21875  0.375   ]\n",
      "  [0.890625 0.375    0.21875  0.25    ]\n",
      "  [0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.      ]]], shape=(1, 4, 4), dtype=float32)\n",
      "labels:   <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f54ac73fd60>\n",
      "image_shape: (4,)\n",
      "anchor_boxes  :  tf.Tensor(\n",
      "[[0.015625   0.02083333 0.1331338  0.2958529 ]\n",
      " [0.015625   0.02083333 0.16773808 0.37275133]\n",
      " [0.015625   0.02083333 0.21133673 0.4696372 ]\n",
      " ...\n",
      " [0.9375     0.9166667  0.26879364 0.22399466]\n",
      " [0.9375     0.9166667  0.33865878 0.28221557]\n",
      " [0.9375     0.9166667  0.4266833  0.35556933]], shape=(9072, 4), dtype=float32)\n",
      "cls_ids tf.Tensor([1. 1. 0. 0.], shape=(4,), dtype=float32)\n",
      "(9072, 4)\n",
      "(4, 4)\n",
      "iou_matrix:   (9072, 4)\n",
      "max_iou:   (9072,)\n",
      "matched_gt_idx:   tf.Tensor([0 0 0 ... 0 0 0], shape=(9072,), dtype=int64)\n",
      "max_iou: tf.Tensor([0.02813981 0.05599736 0.09040865 ... 0.         0.         0.        ], shape=(9072,), dtype=float32)\n",
      "positive_mask:   tf.Tensor([False False False ... False False False], shape=(9072,), dtype=bool)\n",
      "negative_mask:   (9072,)\n",
      "ignore_mask:   (9072,)\n",
      "matched_gt_idx:   tf.Tensor([0 0 0 ... 0 0 0], shape=(9072,), dtype=int64)\n",
      "positive_mask:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(9072,), dtype=float32)\n",
      "ignore_mask:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(9072,), dtype=float32)\n",
      "matched_gt_boxes:   tf.Tensor(\n",
      "[[0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " ...\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]], shape=(9072, 4), dtype=float32)\n",
      "_compute_box_target anchor_boxes:  tf.Tensor(\n",
      "[[0.015625   0.02083333 0.1331338  0.2958529 ]\n",
      " [0.015625   0.02083333 0.16773808 0.37275133]\n",
      " [0.015625   0.02083333 0.21133673 0.4696372 ]\n",
      " ...\n",
      " [0.9375     0.9166667  0.26879364 0.22399466]\n",
      " [0.9375     0.9166667  0.33865878 0.28221557]\n",
      " [0.9375     0.9166667  0.4266833  0.35556933]], shape=(9072, 4), dtype=float32)\n",
      "_compute_box_target matched_gt_boxes :  tf.Tensor(\n",
      "[[0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " ...\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]\n",
      " [0.171875 0.1875   0.21875  0.375   ]], shape=(9072, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[ 1.1736313   0.56334305  0.49657494  0.2370637 ]\n",
      " [ 0.9315118   0.44712564  0.2655258   0.00601446]\n",
      " [ 0.73934144  0.35488388  0.03447683 -0.22503442]\n",
      " ...\n",
      " [-2.8483746  -3.255286   -0.20601445  0.51530385]\n",
      " [-2.2607565  -2.5837224  -0.43706352  0.28425482]\n",
      " [-1.7943636  -2.0507019  -0.6681125   0.05320574]], shape=(9072, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[ 1.1736313e+01  5.6334305e+00  2.4828746e+00  1.1853185e+00]\n",
      " [ 9.3151178e+00  4.4712563e+00  1.3276290e+00  3.0072315e-02]\n",
      " [ 7.3934145e+00  3.5488389e+00  1.7238416e-01 -1.1251720e+00]\n",
      " ...\n",
      " [-2.8483746e+01 -3.2552860e+01 -1.0300722e+00  2.5765193e+00]\n",
      " [-2.2607565e+01 -2.5837223e+01 -2.1853175e+00  1.4212741e+00]\n",
      " [-1.7943636e+01 -2.0507019e+01 -3.3405626e+00  2.6602870e-01]], shape=(9072, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[ 1.1736313e+01  5.6334305e+00  2.4828746e+00  1.1853185e+00]\n",
      " [ 9.3151178e+00  4.4712563e+00  1.3276290e+00  3.0072315e-02]\n",
      " [ 7.3934145e+00  3.5488389e+00  1.7238416e-01 -1.1251720e+00]\n",
      " ...\n",
      " [-2.8483746e+01 -3.2552860e+01 -1.0300722e+00  2.5765193e+00]\n",
      " [-2.2607565e+01 -2.5837223e+01 -2.1853175e+00  1.4212741e+00]\n",
      " [-1.7943636e+01 -2.0507019e+01 -3.3405626e+00  2.6602870e-01]], shape=(9072, 4), dtype=float32)\n",
      "matched_gt_cls_ids:   tf.Tensor([1. 1. 1. ... 1. 1. 1.], shape=(9072,), dtype=float32)\n",
      "cls_target:   tf.Tensor([-1. -1. -1. ... -1. -1. -1.], shape=(9072,), dtype=float32)\n",
      "cls_target:   tf.Tensor(\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]], shape=(9072, 1), dtype=float32)\n",
      "Number of 1.0 values in cls_target: tf.Tensor(215, shape=(), dtype=int64)\n",
      "label:   tf.Tensor(\n",
      "[[ 1.1736313e+01  5.6334305e+00  2.4828746e+00  1.1853185e+00\n",
      "  -1.0000000e+00]\n",
      " [ 9.3151178e+00  4.4712563e+00  1.3276290e+00  3.0072315e-02\n",
      "  -1.0000000e+00]\n",
      " [ 7.3934145e+00  3.5488389e+00  1.7238416e-01 -1.1251720e+00\n",
      "  -1.0000000e+00]\n",
      " ...\n",
      " [-2.8483746e+01 -3.2552860e+01 -1.0300722e+00  2.5765193e+00\n",
      "  -1.0000000e+00]\n",
      " [-2.2607565e+01 -2.5837223e+01 -2.1853175e+00  1.4212741e+00\n",
      "  -1.0000000e+00]\n",
      " [-1.7943636e+01 -2.0507019e+01 -3.3405626e+00  2.6602870e-01\n",
      "  -1.0000000e+00]], shape=(9072, 5), dtype=float32)\n",
      "label:   tf.Tensor(\n",
      "[[ 1.1736313e+01  5.6334305e+00  2.4828746e+00  1.1853185e+00\n",
      "  -1.0000000e+00]\n",
      " [ 9.3151178e+00  4.4712563e+00  1.3276290e+00  3.0072315e-02\n",
      "  -1.0000000e+00]\n",
      " [ 7.3934145e+00  3.5488389e+00  1.7238416e-01 -1.1251720e+00\n",
      "  -1.0000000e+00]\n",
      " ...\n",
      " [-2.8483746e+01 -3.2552860e+01 -1.0300722e+00  2.5765193e+00\n",
      "  -1.0000000e+00]\n",
      " [-2.2607565e+01 -2.5837223e+01 -2.1853175e+00  1.4212741e+00\n",
      "  -1.0000000e+00]\n",
      " [-1.7943636e+01 -2.0507019e+01 -3.3405626e+00  2.6602870e-01\n",
      "  -1.0000000e+00]], shape=(9072, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    img, box, label = preprocess_data(image, bbox, label)\n",
    "    print(img.shape, box.shape, label.shape)\n",
    "\n",
    "    label_encoder.encode_batch(img, box, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "autotune = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 32, 1)\n",
      "(1, 4, 4)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "for img, bbox, label in train_dataset.take(1):\n",
    "    print(img.shape)\n",
    "    print(bbox.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_shape:   Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "batch_size:   Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "gt_boxes:  Tensor(\"args_1:0\", shape=(1, None, 4), dtype=float32)\n",
      "labels:   <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f54ac771b50>\n",
      "image_shape: (4,)\n",
      "anchor_boxes  :  Tensor(\"while/stack_3:0\", shape=(None, 4), dtype=float32)\n",
      "cls_ids Tensor(\"while/Cast:0\", dtype=float32)\n",
      "(None, 4)\n",
      "(None, 4)\n",
      "iou_matrix:   (None, None)\n",
      "max_iou:   (None,)\n",
      "matched_gt_idx:   Tensor(\"while/ArgMax:0\", shape=(None,), dtype=int64)\n",
      "max_iou: Tensor(\"while/Max:0\", shape=(None,), dtype=float32)\n",
      "positive_mask:   Tensor(\"while/GreaterEqual:0\", shape=(None,), dtype=bool)\n",
      "negative_mask:   (None,)\n",
      "ignore_mask:   (None,)\n",
      "matched_gt_idx:   Tensor(\"while/ArgMax:0\", shape=(None,), dtype=int64)\n",
      "positive_mask:   Tensor(\"while/Cast_1:0\", shape=(None,), dtype=float32)\n",
      "ignore_mask:   Tensor(\"while/Cast_2:0\", shape=(None,), dtype=float32)\n",
      "matched_gt_boxes:   Tensor(\"while/GatherV2:0\", shape=(None, 4), dtype=float32)\n",
      "_compute_box_target anchor_boxes:  Tensor(\"while/stack_3:0\", shape=(None, 4), dtype=float32)\n",
      "_compute_box_target matched_gt_boxes :  Tensor(\"while/GatherV2:0\", shape=(None, 4), dtype=float32)\n",
      "box_target:   Tensor(\"while/concat_6:0\", shape=(None, 4), dtype=float32)\n",
      "box_target:   Tensor(\"while/truediv_17:0\", shape=(None, 4), dtype=float32)\n",
      "box_target:   Tensor(\"while/truediv_17:0\", shape=(None, 4), dtype=float32)\n",
      "matched_gt_cls_ids:   Tensor(\"while/GatherV2_1:0\", dtype=float32)\n",
      "cls_target:   Tensor(\"while/SelectV2_1:0\", dtype=float32)\n",
      "cls_target:   Tensor(\"while/ExpandDims_3:0\", dtype=float32)\n",
      "Number of 1.0 values in cls_target: Tensor(\"while/count_nonzero/Sum:0\", shape=(), dtype=int64)\n",
      "label:   Tensor(\"while/concat_7:0\", shape=(None, None), dtype=float32)\n",
      "label:   Tensor(\"while/concat_7:0\", shape=(None, None), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0 1.0\n",
      "(1, 9072, 5)\n",
      "Positive 개수: 241\n",
      "Negative 개수: 8491\n",
      "Ignore 개수: 340\n",
      "255.0 0.0\n",
      "(1, 9072, 5)\n",
      "Positive 개수: 132\n",
      "Negative 개수: 8655\n",
      "Ignore 개수: 285\n",
      "255.0 0.0\n",
      "(1, 9072, 5)\n",
      "Positive 개수: 448\n",
      "Negative 개수: 8072\n",
      "Ignore 개수: 552\n"
     ]
    }
   ],
   "source": [
    "positive_count = []\n",
    "negative_count = []\n",
    "ignore_count = []\n",
    "for batch in train_dataset.take(3):\n",
    "    images, labels = batch\n",
    "    print(np.array(images).max(), np.array(images).min())\n",
    "    print(labels.shape)\n",
    "\n",
    "    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[0, :, 4], 1.0), tf.int32))\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[0, :, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[0, :, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive 개수: 103\n",
      "Negative 개수: 8799\n",
      "Ignore 개수: 170\n",
      "Positive 103\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAGdCAYAAABZ+qqcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqcklEQVR4nO3dfXDc1X3v8c9vnyVZD5ZlSRZ+wAaDw4PdiRMc3QAlsQfb0zAQuBmg/GHSDNxQu1PiJinODRBIJqZkhqbpuHBn2uJmpoGETIEJt6ENJjYTapNrgy+hSVzbUWIbWzIW1rO02odz//BFiWIZtN+jw67s92tmZ+zV76vv0W/P/vajn367J3LOOQEAAAQUK/cAAADA2Y/AAQAAgiNwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACC4RLkH8PuKxaKOHj2q2tpaRVFU7uEAAIAzcM6pv79fbW1tisXe/RxGxQWOo0ePat68eeUeBgAAmKTDhw9r7ty577pNxQWO2tpaSdKCTfcqlsmUXN/wS7/+qb6CudbF/M7IFDL2+vTJvLk2ni2aayUpeXLYXDvaVO3Xuy9rrs3Vpc21icGcuVaSChn7U2+0zu9pW31kyFwbFe1zxSX8/oKbnVX68eAdsXz5VnBI9I961ccH7fXF6qS5NhqxHwslaXjeDHOt7+MVH7aPPV8dN9f6vgaMNth71xy1HwslabTWNlfyuRHtfuHrY6/d76biAsc7f0aJZTKmwBFP+fVPJMsXOJS01ycSHoGj4Bc4EnF7fTFhfxE51du+z1zCI3Ak7AcGSYoS9qdeMen3tE3E7XM8ijwCR9wvcBQ85kpMZQwcnkEr7jHHix4HxChuP6ZIUiJZvscrnvAISx7PbefxWElSIWnvnUj49S4m7eFU0qQugQh20eiWLVt0/vnnK5PJaMWKFfrpT38aqhUAAKhwQQLHd7/7XW3cuFH333+/Xn31VS1btkyrV6/W8ePHQ7QDAAAVLkjgeOSRR3THHXfo05/+tC655BI99thjqq6u1j/+4z+GaAcAACrclAeO0dFR7dmzR6tWrfptk1hMq1at0s6dO0/bPpvNqq+vb9wNAACcXaY8cJw4cUKFQkEtLS3j7m9paVFnZ+dp22/evFn19fVjN94SCwDA2afsnzS6adMm9fb2jt0OHz5c7iEBAIApNuVvi21qalI8HldXV9e4+7u6utTa2nra9ul0Wum0/e2JAACg8k35GY5UKqXly5dr27ZtY/cVi0Vt27ZN7e3tU90OAABMA0E++Gvjxo1at26dPvShD+mKK67QN7/5TQ0ODurTn/50iHYAAKDCBQkcN998s9566y3dd9996uzs1B/8wR/o+eefP+1CUgAAcG4I9tHmGzZs0IYNG0J9ewAAMI1U3Foq71j0rQNKxEpfB+DEJy7y6hsfsa8VUXXY7zNEcrNqzLW9i+zrFjTsty/mJUkuZZ9GqRN+vYtV9s//jw/Z14oopv3WUpHHsgfpbr/FwOSx3kOs175Qn0v7rdWQ6rXv86jgtzZHwffx9uE8xu5T6yl10j5PY1m/dVwKHseF4Sb78Swx4re/M2/b14DxOZ5J0uBi22tIYXTyz42yvy0WAACc/QgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAILlHuAZzJwEcWKpHMlFyX7it69c3NiJtrqwp+vV08MtfW/3rEXFvI+E2D5OFue3E+79Xbtc0y1yZODpprCzNrzLWSFBWcuTZ2st+rd7Fhhr14aNhem7A/tyQp3p+1F+cLXr2LTfZ9Vkz7Pb9iIx77LWb/ndKl7W195evK17yq2z5XnN8UV/959rlSTFZ59U4O2o5Jsdzk6zjDAQAAgiNwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4Cp2efpCKlKULH259po37cu0S1Ji/1Fz7fDy8716V79h751d3GKuTR23L9MuSYXmenNtvNtvqfXYcM5ePGyfK/Fc3t5Xkpx9eXoV/JZaj0Y9xp5O2Ws9l6fXqP2xjkZG/XoXa+y1idKPY+N720ujnMdcKXo0llSoz5hr8zV+cyXVZ58rmaP2Y1IxkzTXStJA2wx7b895lu61zZV8CXOMMxwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIJLlHsAU62zvcarPnnZhebaWW8MefXuu2Kuubaqc8RcG2VHzbWSFLmkubZY7/d4xfqHzbWuboa5NurtN9dKkpIe+6yl0at1NGifK66myqNxZK+VFBWK5lo3kvXqHR+yP0dc3u8wGxvyeLxi9n0e5fLmWklKpu1zPDHg1VpRzmOuJOPm2niP32tA0157bWzAPk8kafCCmaa6UuYYZzgAAEBwBA4AABAcgQMAAARH4AAAAMEROAAAQHAEDgAAEByBAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAER+AAAADBETgAAEBwBA4AABBcxS5PX/vrQSXiluWR7UuOS1L93rfMtaPnNXj1rtvbZa59e0WruXZmj32Jd8lvOecoV/DqrazHsuG19qXWoyH70tuS5DyW7i5U+/VO9tjX/nYx++8oLuM3bnnss2jI83erUb+l2r0U7UutRz6/U47m7LWS4sfeNte66oxXby8J+/HMVzFl7x3L+c3Rqs4hU12+MDLpbTnDAQAAgiNwAACA4AgcAAAguCkPHF/5ylcURdG425IlS6a6DQAAmEaCXDR66aWX6oUXXvhtk0TFXpsKAADeB0GSQCKRUGur/V0TAADg7BLkGo79+/erra1NixYt0m233aZDhw6dcdtsNqu+vr5xNwAAcHaZ8sCxYsUKbd26Vc8//7weffRRdXR06KqrrlJ/f/+E22/evFn19fVjt3nz5k31kAAAQJlNeeBYu3atPvWpT2np0qVavXq1/vVf/1U9PT363ve+N+H2mzZtUm9v79jt8OHDUz0kAABQZsGv5mxoaNBFF12kAwcOTPj1dDqtdDodehgAAKCMgn8Ox8DAgA4ePKg5c+aEbgUAACrUlAeOz3/+89qxY4d+/etf6z/+4z/0yU9+UvF4XLfeeutUtwIAANPElP9J5ciRI7r11lvV3d2t2bNn68orr9SuXbs0e/bsqW4FAACmiSkPHE8++eRUf0sAADDNVexHgHa21ymeLn2J4pb/Y1tidyoU0n5/oRqdN9NcW/WWfSnpYpXfsuHxo9324pTnkuVJ+xSOhu1L2/suYR0Vnb11z7BXb+exz3yWSpez/8yS5JL2fR75ftpxvmDv7ddZitmPKy5tf35FnsvTK++xXLrvEvEeS7VHw1lzbbG2ylwrSQNzS3/Ne0dtzj5HJSneP/ll5n9XrDD54yiLtwEAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACC5R7gGcyZyf9CoRHym5Lj8j5df4ZK+5tJhs8mo9mrLnv0I6Mtdm9hw110pS8bwWc22sf9Crd2Fmrbk2/nafudYl/Z46US5vr33bPkclKT+/2Vwby3qMe7D05/M4Pvs8bx+3JEVDzl7sOVe8RPbjgpzHzyxJ1VXm0mg469XaJeLm2lxrg7k2li+aayWp5qj9544Pjnr17l/SaKrL50akfZPbljMcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAIrmKXpx9urVYimSm5Lpb3W1I5WjzXXFuz/22v3vJYUjnqHbD3rZthr5W8lrF21aU/xr8r/laPvdhjyfKo6DfPfH7uKG6fJ5KUOHTcXOuGhs210cx6c60kuXTKXuxTK8ll0vbi7pN+vVtnm2ujQsFcW2ieaa717e2t401zqc+Loqv2mCeSopTHc7tY9OqdHLQ9XlF+8nWc4QAAAMEROAAAQHAEDgAAEByBAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAER+AAAADBETgAAEBwBA4AABAcgQMAAARH4AAAAMEROAAAQHAEDgAAEFyi3AM4ExeL5GJRyXWxXMGrb/LQW+ZaVz/Dq7eLSv9535Fb2GyuTXb2mmslSc7ZS+Nxv95VaXttV7+9ttXvsY6GRsy1rq7Gq7cGh8ylUXWVudaNZM21khQVPebZwKBXb6WS5tKoptqrtYvbjwsayZtLfX8bLWZS5lqX9jsuJOrr7L09HmuX9Bu35TVvrNb3WGp9fpVQxxkOAAAQHIEDAAAER+AAAADBETgAAEBwBA4AABAcgQMAAARH4AAAAMEROAAAQHAEDgAAEByBAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAEV7HL08/4eacSsdKXHndp+5LIkjRy8RxzbeqE3xLYo032Zcczv3jTXOtq7EuOS5IKRXut53LOPsuGF7P25dJjSb+njus+aa4tNtV79Y55LPOu6tKfk++IRnP2vpIUt/9+FDXYlyuXpNFW+z4vJv1+r4t7LDGfPG6fZ3Ie80RSbEa1ubYoe60kKbIv8x4N248LkUdfSYp77PPY0IhX71Sv7VgaK4xOfltTBwAAgBIQOAAAQHAEDgAAEFzJgeOll17Sddddp7a2NkVRpGeeeWbc151zuu+++zRnzhxVVVVp1apV2r9//1SNFwAATEMlB47BwUEtW7ZMW7ZsmfDrDz/8sL71rW/pscce0yuvvKKamhqtXr1aIyN+F7QAAIDpq+RL7deuXau1a9dO+DXnnL75zW/qy1/+sq6//npJ0re//W21tLTomWee0S233OI3WgAAMC1N6TUcHR0d6uzs1KpVq8buq6+v14oVK7Rz584Ja7LZrPr6+sbdAADA2WVKA0dnZ6ckqaWlZdz9LS0tY1/7fZs3b1Z9ff3Ybd68eVM5JAAAUAHK/i6VTZs2qbe3d+x2+PDhcg8JAABMsSkNHK2trZKkrq6ucfd3dXWNfe33pdNp1dXVjbsBAICzy5QGjoULF6q1tVXbtm0bu6+vr0+vvPKK2tvbp7IVAACYRkp+l8rAwIAOHDgw9v+Ojg7t3btXjY2Nmj9/vu6++2597Wtf0+LFi7Vw4ULde++9amtr0w033DCV4wYAANNIyYFj9+7d+tjHPjb2/40bN0qS1q1bp61bt+qLX/yiBgcHdeedd6qnp0dXXnmlnn/+eWUymakbNQAAmFZKDhzXXHON3LusaBdFkR588EE9+OCDXgMDAABnj7K/SwUAAJz9Sj7D8X4pNNUpipf+Z5hiMu7VN7PvmLk2u3jid+JMVmJg1Fw7tHSuubZq7yFzrXTqrJaVq5rl1VvvcrbtvUQJj+mfy9trJSlmn6dRruDX20ehaC51NVV+vZMej1fRPk8kKfKodwn780OSYsP2ueaK9scrqvZ8vGL232eLGb+Xpqix1l7scUzJ13leOuBxLPWVr0na6vKTPx5xhgMAAARH4AAAAMEROAAAQHAEDgAAEByBAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAER+AAAADBETgAAEBwBA4AABAcgQMAAARH4AAAAMFV7PL03UtrFU+VvtRv/UH7Eu+SFGudaa5N7tnv1Tu3fLG5tupQr7m2OK/ZXCtJsf4Rc61L+GXeaNC+dHesdoa98XDWXitJHku1RwNDXq19Fmp3gx69qz2X7i5hGezTnLQ/PyQpcbzbXts8y6u3uk/aa0dz5tJiS6O9r6TYgP24EBu1P68lycXsx5XIZ3n6Gr+X1Pho0VxbzNiWl39HIW3bZ4X45Os4wwEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAqdnn6mqN5JZKlL1Gcr4579U2/aV923C04z6t36tUD5tqoyb6UdOxtv6W7CwtazLXFlN/jFRv2WJK5ymO59Jzn8tlJ+1MvGrbPUV9udNRcG/NYKl2SXMJjrkSev1s5++Odb5rh1TrpMdeKXW+Za2Nv9ZhrJcnVVNl7D9nn2alvYH+8XRTZaxP2WkmKDdiXp48K9lpJilz4Os5wAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAILlHuAZxJsj+nRCJecl2ib8SvsaHnO2KDw16th69YbK5Nn7D3jjIpc60kJY73mmsHL2nx6u0jmR21F6eqvHpHI1lzrfN8vDTsMVdS9t4ulTTXSpKSHoer+hlerd0M++P95tXVXr2TA/b6xn1N5trUW37Hs9zMjLk2PpL36q0oMpcWk/bfwwdb/F5So7wz18Zy9lpJGq21vfblc5Ov4wwHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIKr2LfFAsC54LWvb5T9zfjnpoKkT3z0q+UeBkpE4ACAMnnt6xuVlmT/1IhzU0LScy/fS+iYZggcAFAmcZ0KG0Wd+q0d7y2uU9cCcFZo+iFwAECZFSQt/dIjE34tOWD/vo377J+mW6mfNPrDl+/l4sNpiscNAAAER+AAAADBETgAAEBwBA4AABBcxV40muwZUSJe+nK72Va/ZagLaXsGKybrvXonB+zXqefr0+ZaF7Nf3CVJA22N5tr4qN+SypJ9yfPUm0VzbaGhxlwrSbG3e821bs4sr95Rt32fRxn7PCvU+s2zKGt/fsRG7BdPSlLR4/EeWjy53mfaLj0ja+59aL79eFjVlTLXStLM/fYl5mO5yT03XXzqf2d2cY+l7T1fUX2WmI+N+r3PKZa3ve8nlp/8mDnDAQAAgiNwAACA4AgcAAAguJIDx0svvaTrrrtObW1tiqJIzzzzzLiv33777YqiaNxtzZo1UzVeAAAwDZUcOAYHB7Vs2TJt2bLljNusWbNGx44dG7s98cQTXoMEAADTW8nX1K5du1Zr1659123S6bRaW1vNgwIAAGeXINdwbN++Xc3Nzbr44ot11113qbu7+4zbZrNZ9fX1jbsBAICzy5QHjjVr1ujb3/62tm3bpr/6q7/Sjh07tHbtWhUKE79HePPmzaqvrx+7zZs3b6qHBAAAymzKP/jrlltuGfv35ZdfrqVLl+qCCy7Q9u3btXLlytO237RpkzZu3Dj2/76+PkIHAABnmeBvi120aJGampp04MCBCb+eTqdVV1c37gYAAM4uwQPHkSNH1N3drTlz5oRuBQAAKlTJf1IZGBgYd7aio6NDe/fuVWNjoxobG/XAAw/opptuUmtrqw4ePKgvfvGLuvDCC7V69eopHTgAAJg+Sg4cu3fv1sc+9rGx/79z/cW6dev06KOP6vXXX9c//dM/qaenR21tbbr22mv11a9+Vem0fdEnAAAwvZUcOK655ho5d+bV4f7t3/7Na0AAAODsw1oqAAAguCl/W+xUyddnpESm5LpE/6hX30K69J7vyJzw611M2vNfNFo01w7OrzLXSlJ05hNe78nFI6/eqd6cvXcibm9c8PihJUVxj6yfzfv1rq8117refq/ePmI99t7Ft0/6NZ/TaC5dMPeE13aL698y975q6T5zbU3M73j2P5+8zVwbH36Xl6aXfvvPo1dOfOyau81nntpfFp3nr/B956fMtQ0HR/x6L7D93IXs5Os4wwEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAqdnl6q9iQfblySUr12DNYzHPZ8MRJ+9gLP/8vc21D70XmWkk6ucy+dHfmbb99pqLHMvFRZC91fsvTe/Ue9ZvjbnDYXhyzj7tQbV96W5LU0mAujY34Ld3t8kVz7W9+Pdtru4VLu829L00fNdfOjvstT1/7Qfu4TxxpmNR2QwsmPn68fekMc+9E1v7cLlTZnx+SFHkcz4ab/J5fkXGKRyUMmTMcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAjurHtbLACg8r38R19X3PN7HPgfX5ySsZwLCpI+/t+/UdYxEDgAAO+rl//o60pL8vnUiuRUDeYckZD04ve/UNbQQeAAALyv4joVNoo69Zt3qZKS/D7+7twS16nrJ3zPKPkicAAAyqIg6SP/+0un3f9unzT6u39GufB/PTzhNk2v2F9afT5pdKjF77LIxKC9d1X3mT8qdMf3v1ARF2xWwhgAAMBZjsABAACCI3AAAIDgCBwAACC4ir1odLg5rUQyXXJdvM7vzVKZziFzbfztPq/e2UWTW8Z6Qs0fNJcWPZbelqQZh+1Lf8cH/a41L2Y8pnDSXuvifstQu5oqr3ovRcv7Ak6J6uvMtcmf/cpcK0lRTY251jU2ePX2+dUs3ju5eXam7Y4MNph7t3ksMd9T9Pt99MMth8zbvTg0uWN/unF4wvt7L7YvT5/qsf/cyQFz6Skeh5Wh2ZMb92S3C4EzHAAAIDgCBwAACI7AAQAAgiNwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACI7AAQAAgkuUewBnkjkxqkSi9Dw02Jb26hsrVJlrM0NZr94+ionIXJsayHv1jvJFe+3wqFdvl47bi3P2nzuK2/e3JCnp8dQrOr/eHlwmZa6NGmf69T7Zay+uzvj1fu2X5tqWRR+a3HY/nfh59GbPPHPv77deaq6NRfbntST9eqDRvN2FzScmVXum7VKtnZOqn8hIIWmu/dVL55trJUkeh5VYdnLFI7Mm3i7TbTuuRPnJ13GGAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAER+AAAADBETgAAEBwBA4AABAcgQMAAARH4AAAAMEROAAAQHAEDgAAEByBAwAABEfgAAAAwVXs8vSDc9OKJ0tfaj456Lekcrpr0FxbqLcvbS9JuRr7w5E5MWKujf2my1wrSZptX3bc+SzTLnkt1R4N2feZin7zzEvksYa1JDeSNdfGfPZZvmCvlaSUfdlwl055tY6WLTHX1hyd3P4+03aZt+Pm3o/G/shcG3k+XKOXDU1qu/3Hmk+771MfeHVStUvr35zw/ttmvjKp+olcmrIfxxf95jPmWkly9sOZ0ocnN8eLaY8mnjjDAQAAgiNwAACA4AgcAAAguJICx+bNm/XhD39YtbW1am5u1g033KB9+/aN22ZkZETr16/XrFmzNGPGDN10003q6vK8RgAAAExrJQWOHTt2aP369dq1a5d+9KMfKZfL6dprr9Xg4G8vtPzc5z6nH/zgB3rqqae0Y8cOHT16VDfeeOOUDxwAAEwfJb1F4Pnnnx/3/61bt6q5uVl79uzR1Vdfrd7eXv3DP/yDvvOd7+jjH/+4JOnxxx/XBz7wAe3atUsf+chHpm7kAABg2vB6T2Jvb68kqbGxUZK0Z88e5XI5rVq1amybJUuWaP78+dq5c+eEgSObzSqb/e3bwfr6+nyGBADTTlzSD3/y5an/xi9M/becar+8tbSf2/4mYZSbOXAUi0Xdfffd+uhHP6rLLrtMktTZ2alUKqWGhoZx27a0tKizs3PC77N582Y98MAD1mEAwLRV0KmDcEzn5hX81k9WcTq17zC9mOf4+vXr9cYbb+jJJ5/0GsCmTZvU29s7djt8+LDX9wOA6eITV35NWUm5c/Amj9qspK/s5drA6cZ0hmPDhg167rnn9NJLL2nu3Llj97e2tmp0dFQ9PT3jznJ0dXWptbV1wu+VTqeVTpf+iaIAcDb4xJVfe9evFzL2PyIc+2/2Y2vITxr93T+jLHni9J9/sp80iumlpDMczjlt2LBBTz/9tF588UUtXLhw3NeXL1+uZDKpbdu2jd23b98+HTp0SO3t7VMzYgAAMO2UdIZj/fr1+s53vqNnn31WtbW1Y9dl1NfXq6qqSvX19frMZz6jjRs3qrGxUXV1dfqzP/sztbe38w4VAADOYSUFjkcffVSSdM0114y7//HHH9ftt98uSfrrv/5rxWIx3XTTTcpms1q9erX+7u/+bkoGCwAApqeSAoebxFJ2mUxGW7Zs0ZYtW8yDAgAAZ5dz8Z1YAADgfeb1wV8h9S6MKZ4uPQ/VH/TrG+u2f/BYVFvt1Ttqyphrh9qqzLW1/Y3mWknSaO69tzmDfFu9V+soV7QXJzw+Qihl/QSBU6KRUXOtS/s9baOMx7vCcnlzaaF5pr2vpJjPPtv3K6/eumyxuTR6ea9X6/QlF5lrm1+1/07ZP9dvnuV/NbljUmyC7Z5NX+7V+7+am821K2Z2mGsTR1PmWklSZC9N9U6u+Ezb5Y0vX4X45AfNGQ4AABAcgQMAAARH4AAAAMEROAAAQHAEDgAAEByBAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAER+AAAADBETgAAEBwBA4AABAcgQMAAARXscvTF9NOyriS64Zm+2WomgtbzLWJ/qxX7+qDJ821ueZac20x4zkNUvb6YsLv8UpkC+ZaV2Vfpj1fP7mlt88kUSiaa4sZvyWwY7Psy8S7on3c8WMnzLWSpGTSXBqryni1ztXZ93nyA/al7SWpMMM+T+PD9ser+ri9VpLyVfFJbZc5cfry5rFt9V69DyTs9a/PsT9emW6P9eUlZbpLf817Ryw/ucerumvi7QbOsx2LSzkkcIYDAAAER+AAAADBETgAAEBwBA4AABAcgQMAAARH4AAAAMEROAAAQHAEDgAAEByBAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAER+AAAADBETgAAEBwiXIP4EyWXrVfyZpUyXV7X1ji1dfFInNt/wW1Xr2TA9Xm2ni2aO/bNWKulSQl4ubS+JDfFIz3eY7dyKXKmNUTfr2LtRlzbWwkb64dba0310pSosfjsW70e26mfvWWuXb0/Nl+vd88aa7Nzrb3TvbbH2tJKqQnd1wopE+/b+b+nFfvYsJ+HC8m7Mek/sV++6zqLfuxNDnkvLZLDNv6RtnJb8sZDgAAEByBAwAABEfgAAAAwRE4AABAcAQOAAAQHIEDAAAEV7FviwUAnN3ikv7vQxvLPYyznv3NtlOLwAEAeF8VdOrFJyZOs79fnE7t93IicAAA3lcfvOcRvfrQxor5zftcUJC08qZvlHUMBA4AwPvug/c8csavNb1Rvk8a7bmgfJ802rzTHsFSA/ZPm36/cDYLAAAER+AAAADBETgAAEBwFXcNh3OnVrLLDY6a6gsjfquH5vMlLH33+7U5v2uAI496l7f//S5esP/MkqTI/nfHfN7+t1ZJks/Y3eRWV5xIPu/31Ik8xl0seO6zov3njhXsf6Mu62Mtv96xosdxIe93TPLqnbP3jnkcUySpkPV4fuU8r+Fw9se7kLU/t4vDnivsjnocS3O+j5etd2H01BxzkzieRm4yW72Pjhw5onnz5pV7GAAAYJIOHz6suXPnvus2FRc4isWijh49qtraWkXR6Sm1r69P8+bN0+HDh1VXV1eGEU4/7LPSsc9Kxz4rHfusdOyz0oXcZ8459ff3q62tTbHYu1+lUXF/UonFYu+ZkiSprq6OyVYi9lnp2GelY5+Vjn1WOvZZ6ULts/r6+kltx0WjAAAgOAIHAAAIbtoFjnQ6rfvvv1/pdLrcQ5k22GelY5+Vjn1WOvZZ6dhnpauUfVZxF40CAICzz7Q7wwEAAKYfAgcAAAiOwAEAAIIjcAAAgOCmXeDYsmWLzj//fGUyGa1YsUI//elPyz2kivWVr3xFURSNuy1ZsqTcw6ooL730kq677jq1tbUpiiI988wz477unNN9992nOXPmqKqqSqtWrdL+/fvLM9gK8V777Pbbbz9t3q1Zs6Y8g60Amzdv1oc//GHV1taqublZN9xwg/bt2zdum5GREa1fv16zZs3SjBkzdNNNN6mrq6tMIy6/yeyza6655rR59tnPfrZMIy6/Rx99VEuXLh37cK/29nb98Ic/HPt6JcyxaRU4vvvd72rjxo26//779eqrr2rZsmVavXq1jh8/Xu6hVaxLL71Ux44dG7v95Cc/KfeQKsrg4KCWLVumLVu2TPj1hx9+WN/61rf02GOP6ZVXXlFNTY1Wr16tEc9FAqez99pnkrRmzZpx8+6JJ554H0dYWXbs2KH169dr165d+tGPfqRcLqdrr71Wg4ODY9t87nOf0w9+8AM99dRT2rFjh44ePaobb7yxjKMur8nsM0m64447xs2zhx9+uEwjLr+5c+fqoYce0p49e7R79259/OMf1/XXX6///M//lFQhc8xNI1dccYVbv3792P8LhYJra2tzmzdvLuOoKtf999/vli1bVu5hTBuS3NNPPz32/2Kx6FpbW903vvGNsft6enpcOp12TzzxRBlGWHl+f58559y6devc9ddfX5bxTAfHjx93ktyOHTucc6fmVDKZdE899dTYNr/4xS+cJLdz585yDbOi/P4+c865P/zDP3R//ud/Xr5BTQMzZ850f//3f18xc2zanOEYHR3Vnj17tGrVqrH7YrGYVq1apZ07d5ZxZJVt//79amtr06JFi3Tbbbfp0KFD5R7StNHR0aHOzs5xc66+vl4rVqxgzr2H7du3q7m5WRdffLHuuusudXd3l3tIFaO3t1eS1NjYKEnas2ePcrncuHm2ZMkSzZ8/n3n2//3+PnvHP//zP6upqUmXXXaZNm3apKGhoXIMr+IUCgU9+eSTGhwcVHt7e8XMsYpbvO1MTpw4oUKhoJaWlnH3t7S06Je//GWZRlXZVqxYoa1bt+riiy/WsWPH9MADD+iqq67SG2+8odra2nIPr+J1dnZK0oRz7p2v4XRr1qzRjTfeqIULF+rgwYP60pe+pLVr12rnzp2Kx+PlHl5ZFYtF3X333froRz+qyy67TNKpeZZKpdTQ0DBuW+bZKRPtM0n64z/+Yy1YsEBtbW16/fXX9Zd/+Zfat2+f/uVf/qWMoy2vn/3sZ2pvb9fIyIhmzJihp59+Wpdccon27t1bEXNs2gQOlG7t2rVj/166dKlWrFihBQsW6Hvf+54+85nPlHFkOJvdcsstY/++/PLLtXTpUl1wwQXavn27Vq5cWcaRld/69ev1xhtvcC1VCc60z+68886xf19++eWaM2eOVq5cqYMHD+qCCy54v4dZES6++GLt3btXvb29+v73v69169Zpx44d5R7WmGnzJ5WmpibF4/HTrqrt6upSa2trmUY1vTQ0NOiiiy7SgQMHyj2UaeGdecWc87No0SI1NTWd8/Nuw4YNeu655/TjH/9Yc+fOHbu/tbVVo6Oj6unpGbc98+zM+2wiK1askKRzep6lUildeOGFWr58uTZv3qxly5bpb/7mbypmjk2bwJFKpbR8+XJt27Zt7L5isaht27apvb29jCObPgYGBnTw4EHNmTOn3EOZFhYuXKjW1tZxc66vr0+vvPIKc64ER44cUXd39zk775xz2rBhg55++mm9+OKLWrhw4bivL1++XMlkctw827dvnw4dOnTOzrP32mcT2bt3rySds/NsIsViUdlstnLm2Pt2eeoUePLJJ106nXZbt251P//5z92dd97pGhoaXGdnZ7mHVpH+4i/+wm3fvt11dHS4l19+2a1atco1NTW548ePl3toFaO/v9+99tpr7rXXXnOS3COPPOJee+0195vf/MY559xDDz3kGhoa3LPPPutef/11d/3117uFCxe64eHhMo+8fN5tn/X397vPf/7zbufOna6jo8O98MIL7oMf/KBbvHixGxkZKffQy+Kuu+5y9fX1bvv27e7YsWNjt6GhobFtPvvZz7r58+e7F1980e3evdu1t7e79vb2Mo66vN5rnx04cMA9+OCDbvfu3a6jo8M9++yzbtGiRe7qq68u88jL55577nE7duxwHR0d7vXXX3f33HOPi6LI/fu//7tzrjLm2LQKHM4597d/+7du/vz5LpVKuSuuuMLt2rWr3EOqWDfffLObM2eOS6VS7rzzznM333yzO3DgQLmHVVF+/OMfO0mn3datW+ecO/XW2Hvvvde1tLS4dDrtVq5c6fbt21feQZfZu+2zoaEhd+2117rZs2e7ZDLpFixY4O64445z+peCifaVJPf444+PbTM8POz+9E//1M2cOdNVV1e7T37yk+7YsWPlG3SZvdc+O3TokLv66qtdY2OjS6fT7sILL3Rf+MIXXG9vb3kHXkZ/8id/4hYsWOBSqZSbPXu2W7ly5VjYcK4y5hjL0wMAgOCmzTUcAABg+iJwAACA4AgcAAAgOAIHAAAIjsABAACCI3AAAIDgCBwAACA4AgcAAAiOwAEAAIIjcAAAgOAIHAAAIDgCBwAACO7/AXAZK+Q1Zn4bAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_predictions(labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    decoded_boxes = []\n",
    "    label_idx = 0\n",
    "    for label in labels:\n",
    "        # if label[4] == 1.0:\n",
    "        #     print(\"label:\", label)\n",
    "        # elif label[4] == -1.0:\n",
    "        #     print(\"label:\", label)\n",
    "        dx, dy, dw, dh = label[:4]\n",
    "        anchor = anchors[label_idx]\n",
    "        anchor_x, anchor_y, anchor_w, anchor_h = anchor\n",
    "        cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "        cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "        width = np.exp(dw * box_variance[2]) * anchor_w\n",
    "        height = np.exp(dh * box_variance[3]) * anchor_h\n",
    "        x_min = cx - width / 2\n",
    "        y_min = cy - height / 2\n",
    "        decoded_box = [x_min, y_min, width, height]\n",
    "        # print(np.array(decoded_box))\n",
    "        if label[4] == 1.0:\n",
    "            decoded_boxes.append(decoded_box)\n",
    "        label_idx += 1\n",
    "        # if len(np.array(decoded_boxes)) > 1: \n",
    "            # break\n",
    "    print(\"Positive\",len(np.array(decoded_boxes)))\n",
    "    return decoded_boxes    \n",
    "    # print(np.array(decoded_boxes))\n",
    "    \n",
    "\n",
    "# 바운딩 박스 그리기 함수\n",
    "def draw_positive_bounding_boxes(image, decoded_boxes):\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    # print(len(decoded_boxes))\n",
    "    i = 0\n",
    "    for box in decoded_boxes:\n",
    "        i+=1\n",
    "        # print(box)\n",
    "        x_min, y_min, width, height = box\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    # print(i)\n",
    "    plt.show()\n",
    "\n",
    "# 앵커 박스 생성\n",
    "anchor_box = AnchorBox()\n",
    "anchors = anchor_box.get_anchors(24, 32)\n",
    "# train_dataset에서 첫 번째 배치를 가져오고, 바운딩 박스 그리기\n",
    "for batch in train_dataset.take(1):\n",
    "    image = batch[0][0].numpy()\n",
    "    labels = batch[1][0].numpy()  # 여기서 labels는 [오프셋x, 오프셋y, 스케일w, 스케일h, 클래스, 앵커 박스 인덱스]를 포함한다고 가정\n",
    "    # print(labels)\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, 4], 1.0), tf.int32))\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n",
    "\n",
    "    # 오프셋 디코딩 및 바운딩 박스 그리기\n",
    "    decoded_boxes = decode_predictions(labels, anchors)\n",
    "    draw_positive_bounding_boxes(image, decoded_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = layers.DepthwiseConv2D(kernel_size=kernel_size, padding='same' if padding else 'valid', depth_multiplier=1, strides=stride)\n",
    "        self.pointwise = layers.Conv2D(out_channels, kernel_size=1, strides=1)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "class DepthwiseConv(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseConv, self).__init__()\n",
    "        self.depthwise = DepthwiseSeparableConv(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.silu = layers.Activation('silu')\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.silu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = layers.Conv2D(out_channels, kernel_size, strides=stride, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.silu = layers.Activation('silu')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return self.silu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(layers.Layer):\n",
    "    def __init__(self, in_out_channels, mid_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv_0 = Conv(in_out_channels, mid_channels, kernel_size=1, stride=stride, padding=0)\n",
    "        self.conv_1 = Conv(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv_3 = Conv(mid_channels, in_out_channels, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def call(self, x):\n",
    "        identity = x\n",
    "        out = self.conv_0(x)\n",
    "        out = self.conv_1(out)\n",
    "        out = self.conv_3(out)\n",
    "        out += identity\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CSPDenseLayer(layers.Layer):\n",
    "#     def __init__(self, in_out_channels, bottleneck_mid_channels, out_channels):\n",
    "#         super(CSPDenseLayer, self).__init__()\n",
    "#         self.conv_0 = DepthwiseConv(in_out_channels, kernel_size=3, stride=1)\n",
    "#         self.conv_1 = DepthwiseConv(in_out_channels, kernel_size=3, stride=1)\n",
    "#         self.conv_2 = DepthwiseConv(bottleneck_mid_channels, kernel_size=1, stride=1)\n",
    "#         # self.bottleneck = Bottleneck(in_out_channels, bottleneck_mid_channels)\n",
    "#         self.conv_3 = DepthwiseConv(out_channels, kernel_size=3, stride=1)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x1, x2 = tf.split(x, num_or_size_splits=2, axis=-1)\n",
    "#         x1 = self.conv_0(x1)\n",
    "#         x2 = self.conv_1(x2)\n",
    "#         out = self.conv_2(x1)\n",
    "#         out = tf.concat([out, x2], axis=-1)\n",
    "#         out = self.conv_3(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSPDenseLayer(layers.Layer):\n",
    "    def __init__(self, in_out_channels, bottleneck_mid_channels, out_channels):\n",
    "        super(CSPDenseLayer, self).__init__()\n",
    "        self.conv_0 = Conv(in_out_channels // 2, in_out_channels, stride=1)\n",
    "        self.conv_1 = Conv(in_out_channels // 2, in_out_channels, stride=1)\n",
    "        self.bottleneck = Bottleneck(in_out_channels, bottleneck_mid_channels)\n",
    "        self.conv_3 = Conv(in_out_channels * 2, out_channels, stride=1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x1, x2 = tf.split(x, num_or_size_splits=2, axis=-1)\n",
    "        x1 = self.conv_0(x1)\n",
    "        x2 = self.conv_1(x2)\n",
    "        out = self.bottleneck(x1)\n",
    "        out = tf.concat([out, x2], axis=-1)\n",
    "        out = self.conv_3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(layers.Layer):\n",
    "    def __init__(self, in_channels, reduction_ratio=16, pool_types=['avg', 'max'], kernel_size=3):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.pool_types = pool_types\n",
    "        self.conv = layers.Conv2D(1, kernel_size=kernel_size, strides=1, padding='same', use_bias=False, kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        self.sigmoid = layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        pooled_features = []\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type == 'avg':\n",
    "                pooled = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
    "            elif pool_type == 'max':\n",
    "                pooled = tf.reduce_max(x, axis=[1, 2], keepdims=True)\n",
    "            pooled_features.append(pooled)\n",
    "        \n",
    "        concat = tf.concat(pooled_features, axis=-1)\n",
    "        attention = self.conv(concat)\n",
    "        attention = self.sigmoid(attention)\n",
    "        return x * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(layers.Layer):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = layers.Conv2D(1, kernel_size=kernel_size, strides=1, padding='same', use_bias=False, kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        self.sigmoid = layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        avg_out = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        max_out = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "        x = tf.concat([avg_out, max_out], axis=-1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(layers.Layer):\n",
    "    def __init__(self, in_channels, reduction_ratio=16, pool_types=['avg', 'max'], kernel_size=3):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction_ratio, pool_types, kernel_size)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x) * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "class SPPFast(layers.Layer):\n",
    "    def __init__(self, filters: int, pool_kernel_sizes: List[int] = [1, 2, 4], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_kernel_sizes = pool_kernel_sizes\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.conv = layers.Conv2D(filters, kernel_size=1, strides=1, padding='same', use_bias=False, kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        height, width = tf.shape(inputs)[1], tf.shape(inputs)[2]\n",
    "\n",
    "        # 글로벌 평균 풀링과 업샘플링\n",
    "        global_features = self.global_pool(inputs)\n",
    "        global_features = tf.expand_dims(tf.expand_dims(global_features, 1), 1)\n",
    "        global_features = tf.image.resize(global_features, [height, width])\n",
    "\n",
    "        # 다양한 크기의 MaxPooling\n",
    "        pooled_outputs = [\n",
    "            layers.MaxPooling2D(pool_size=kernel_size, strides=1, padding='SAME')(inputs)\n",
    "            for kernel_size in self.pool_kernel_sizes\n",
    "        ]\n",
    "\n",
    "        # 업샘플링 및 컨캐터네이션\n",
    "        pooled_outputs = [\n",
    "            tf.image.resize(pooled, [height, width])\n",
    "            for pooled in pooled_outputs\n",
    "        ]\n",
    "        pooled_outputs.append(global_features)\n",
    "        pooled_outputs.append(inputs)\n",
    "        spp_output = tf.concat(pooled_outputs, axis=-1)\n",
    "\n",
    "        # 컨볼루션 적용\n",
    "        spp_output = self.conv(spp_output)\n",
    "\n",
    "        return spp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SPPF(layers.Layer):\n",
    "#     def __init__(self, out_channels, kernel_size=3, stride=1, padding='SAME'):\n",
    "#         super(SPPF, self).__init__()\n",
    "#         self.conv = layers.Conv2D(filters=out_channels, kernel_size=kernel_size, strides=stride, padding=padding, kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "#         self.maxpool = layers.MaxPooling2D(pool_size=2, strides=1, padding='SAME')\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         x = self.conv(inputs)\n",
    "        \n",
    "#         pool1 = self.maxpool(x)\n",
    "#         pool2 = self.maxpool(pool1)\n",
    "        \n",
    "#         concatenated = tf.concat([x, pool1, pool2], axis=-1)\n",
    "#         return concatenated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(layers.Layer):\n",
    "    def __init__(self, size, interpolation = 'bilinear'):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.upsample = layers.UpSampling2D(size=size, interpolation = interpolation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.upsample(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# class BackBone(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(BackBone, self).__init__()\n",
    "#         self.upsample = Upsample(size=(4, 4), interpolation='bilinear')\n",
    "\n",
    "#         self.conv1 = Conv(in_channels=1, out_channels=4, kernel_size=6, stride=2, padding=2)\n",
    "        \n",
    "#         self.conv2 = Conv(in_channels=4, out_channels=8, kernel_size=3, stride=2)\n",
    "#         self.csp1 = CSPDenseLayer(8, 4, 8)\n",
    "#         self.cbam1 = CBAM(16)\n",
    "        \n",
    "#         self.conv3 = Conv(in_channels=8, out_channels=16, kernel_size=3, stride=2)\n",
    "#         self.csp2 = CSPDenseLayer(16, 8, 16)\n",
    "#         self.cbam2 = CBAM(32)\n",
    "        \n",
    "#         self.conv4 = Conv(in_channels=16, out_channels=32, kernel_size=3, stride=2)\n",
    "#         self.csp3 = CSPDenseLayer(32, 16, 32)\n",
    "#         self.cbam3 = CBAM(32)\n",
    "#         self.sppf = SPPF(out_channels=32, kernel_size=3, stride=1)\n",
    "#         self.conv5 = Conv(in_channels = 32, out_channels = 32, kernel_size=3, stride=1)\n",
    "\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         x = self.upsample(inputs)\n",
    "#         x = self.conv1(x)   \n",
    "#         x = self.conv2(x)   # 24, 32\n",
    "#         p3 = self.csp1(x)   # 24, 32\n",
    "#         p3 = self.cbam1(p3) # 24, 32\n",
    "\n",
    "#         x = self.conv3(p3) # 6, 8\n",
    "#         p4 = self.csp2(x)  # 12, 16\n",
    "#         p4 = self.cbam2(p4)\n",
    "\n",
    "#         x = self.conv4(p4) \n",
    "#         p5 = self.csp3(x)  \n",
    "#         p5 = self.cbam3(p5)\n",
    "#         p5 = self.sppf(p5)\n",
    "#         p5 = self.conv5(p5) # 6, 8\n",
    "        \n",
    "#         return p3, p4, p5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeckLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(NeckLayer, self).__init__()\n",
    "#         self.conv_c10 = Conv(in_channels=32, out_channels=64, kernel_size=1, stride=1)\n",
    "#         self.upsample1 = layers.Conv2DTranspose(filters = 32, kernel_size = 1, strides=(2, 2), padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "#         self.csp = CSPDenseLayer(32, 16, 32)\n",
    "#         self.conv_c14 = Conv(in_channels=32, out_channels=64, kernel_size=1, stride=1)\n",
    "#         self.upsample2 = layers.Conv2DTranspose(filters = 32, kernel_size = 1, strides=(2, 2), padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "\n",
    "#     def call(self, p3, p4, p5):\n",
    "#         c10 = self.conv_c10(p5) # 15, 20 \n",
    "#         x = self.upsample1(c10) # 30, 40\n",
    "#         x = layers.concatenate([x, p4])\n",
    "#         x = self.csp(x) \n",
    "#         c14 = self.conv_c14(x) \n",
    "#         x = self.upsample2(c14) # 60, 80\n",
    "#         x = layers.concatenate([x, p3])\n",
    "#         return x, c14, c10\n",
    "    \n",
    "# # • x=tf.Tensor(shape=(None, 48, 64, 16), dtype=float32)\n",
    "# # • c14=tf.Tensor(shape=(None, 24, 32, 25), dtype=float32)\n",
    "# # • c10=tf.Tensor(shape=(None, 12, 16, 128), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HeadLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(HeadLayer, self).__init__()\n",
    "#         self.csp_dense1 = CSPDenseLayer(64, 32, 64)\n",
    "#         self.conv1 = layers.Conv2D(64, 1, strides=2, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "#         self.cbam1 = CBAM(64)\n",
    "#         self.csp_dense2 = CSPDenseLayer(64, 32, 64)\n",
    "#         self.conv2 = layers.Conv2D(64, 1, strides=2, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "#         self.cbam2 = CBAM(64)\n",
    "#         self.csp_dense3 = CSPDenseLayer(64, 32, 64)\n",
    "\n",
    "#         # self.channel_adjust1 = tf.keras.layers.Conv2D(64, 1, padding='same', use_bias=False) \n",
    "#         # self.channel_adjust2 = tf.keras.layers.Conv2D(64, 1, padding='same', use_bias=False) \n",
    "#         # self.channel_adjust3 = tf.keras.layers.Conv2D(64, 1, padding='same', use_bias=False) \n",
    "        \n",
    "#     def call(self, x, c14, c10):\n",
    "#         x = self.cbam1(x)\n",
    "#         out1 = self.csp_dense1(x)\n",
    "#         # out1_adj = self.channel_adjust1(out1)\n",
    "#         x = self.conv1(out1)\n",
    "#         x = layers.concatenate([x, c14])\n",
    "#         x = self.cbam2(x)\n",
    "#         out2 = self.csp_dense2(x)\n",
    "#         # out2_adj = self.channel_adjust2(out2)\n",
    "#         x = self.conv2(out2)\n",
    "#         x = layers.concatenate([x, c10])\n",
    "#         out3 = self.csp_dense3(x)\n",
    "#         # out3_adj = self.channel_adjust3(out3)\n",
    "#         return out1, out2, out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(tf.keras.Model):\n",
    "#     def __init__(self, num_classes=1, num_anchors_per_location=9):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.backbone = BackBone()\n",
    "#         self.neck = NeckLayer()\n",
    "#         self.head = HeadLayer()\n",
    "#         self.backbone.trainable = True\n",
    "#         self.neck.trainable = True\n",
    "#         self.head.trainable = True\n",
    "\n",
    "#         self.prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         # 각 위치(픽셀)에서 예측해야 하는 앵커 박스의 수\n",
    "#         self.num_anchors_per_location = num_anchors_per_location\n",
    "\n",
    "#         self.classification_conv1 = layers.Conv2D(128, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer = self.prior_probability)\n",
    "#         self.classification_conv2 = layers.Conv2D(128, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer = self.prior_probability)\n",
    "     \n",
    "#         self.regression_conv1 = layers.Conv2D(128, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer = self.prior_probability)\n",
    "#         self.regression_conv2 = layers.Conv2D(128, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer = self.prior_probability)\n",
    "    \n",
    "    \n",
    "#         # 분류 헤드\n",
    "#         self.classification_head = tf.keras.Sequential([\n",
    "#             self.classification_conv1,\n",
    "#             self.classification_conv2,\n",
    "#             layers.Conv2D(self.num_anchors_per_location * num_classes, 3, padding=\"same\", kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer = self.prior_probability)\n",
    "#         ])\n",
    "\n",
    "#         # 회귀 헤드\n",
    "#         self.regression_head = tf.keras.Sequential([\n",
    "#             self.regression_conv1,\n",
    "#             self.regression_conv2,\n",
    "#             layers.Conv2D(self.num_anchors_per_location * 4, 3, padding=\"same\", kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer = self.prior_probability)\n",
    "#         ])\n",
    "\n",
    "#         self.classification_head.trainable = True\n",
    "#         self.regression_head.trainable = True\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         p3, p4, p5 = self.backbone(inputs)\n",
    "#         x, c14, c10 = self.neck(p3, p4, p5)\n",
    "#         out1, out2, out3 = self.head(x, c14, c10)\n",
    "\n",
    "#         cls_outputs = []\n",
    "#         reg_outputs = []\n",
    "#         N = tf.shape(inputs)[0]\n",
    "#         for feature in [out1, out2, out3]:\n",
    "#             # print(feature.shape)\n",
    "#             # 첫 번째 feature 맵 (None, 12, 16, 32)의 경우: 12x16 위치 각각에 9개의 앵커 박스 = 12x16x9 = 1728\n",
    "#             # 두 번째 feature 맵 (None, 6, 8, 32)의 경우: 6x8 위치 각각에 9개의 앵커 박스 = 6x8x9 = 432\n",
    "#             # 세 번째 feature 맵 (None, 3, 4, 32)의 경우: 3x4 위치 각각에 9개의 앵커 박스 = 3x4x9 = 108\n",
    "#             cls_output = self.classification_head(feature)\n",
    "#             reg_output = self.regression_head(feature)\n",
    "#             reg_output = tf.reshape(reg_output, [N, -1, 4])\n",
    "            \n",
    "#             cls_output = tf.reshape(cls_output, [N, -1, self.num_classes])\n",
    "#             cls_outputs.append(cls_output)\n",
    "#             reg_outputs.append(reg_output)\n",
    "\n",
    "#         # 결과 결합\n",
    "#         reg_outputs = tf.concat(reg_outputs, axis=1)\n",
    "#         cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "#         # 최종 출력\n",
    "#         final_output = tf.concat([reg_outputs, cls_outputs], axis=-1)\n",
    "#         # print(final_output.shape)\n",
    "#         return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackBone(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(BackBone, self).__init__()\n",
    "        self.conv1 = Conv(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.csp1 = CSPDenseLayer(8, 4, 8)\n",
    "        self.cbam1 = CBAM(8)\n",
    "        \n",
    "        self.conv2 = Conv(in_channels=8, out_channels=16, kernel_size=3, stride=2)\n",
    "        self.csp2 = CSPDenseLayer(16, 8, 16)\n",
    "        self.cbam2 = CBAM(16)\n",
    "        self.sppf = SPPFast(32)\n",
    "\n",
    "        self.conv3 = Conv(in_channels=32, out_channels=32, kernel_size=3, stride=2)\n",
    "        self.csp3 = CSPDenseLayer(32, 16, 32)\n",
    "        self.cbam3 = CBAM(32)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        p1 = self.csp1(x)  # 24, 32\n",
    "        p1 = self.cbam1(p1)\n",
    "        \n",
    "        x = self.conv2(p1)  # 12, 16\n",
    "        p2 = self.csp2(x)\n",
    "        p2 = self.cbam2(p2)\n",
    "        p2 = self.sppf(p2)\n",
    "        \n",
    "        x = self.conv3(p2)  # 6, 8\n",
    "        p3 = self.csp3(x)\n",
    "        p3 = self.cbam3(p3)\n",
    "        # p3 = self.sppf(p3)\n",
    "        return p1, p2, p3\n",
    "\n",
    "class NeckLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(NeckLayer, self).__init__()\n",
    "        self.conv_c3 = Conv(in_channels=32, out_channels=64, kernel_size=1, stride=1)\n",
    "        self.upsample1 = layers.Conv2DTranspose(filters=32, kernel_size=1, strides=(2, 2), padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        self.csp = CSPDenseLayer(32, 16, 32)\n",
    "        self.conv_c2 = Conv(in_channels=32, out_channels=64, kernel_size=1, stride=1)\n",
    "        self.upsample2 = layers.Conv2DTranspose(filters=16, kernel_size=1, strides=(2, 2), padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "\n",
    "    def call(self, p1, p2, p3):\n",
    "        c3 = self.conv_c3(p3)  #6, 8\n",
    "        x = self.upsample1(c3) #12, 16\n",
    "        x = layers.concatenate([x, p2])\n",
    "        x = self.csp(x)\n",
    "        c2 = self.conv_c2(x)\n",
    "        x = self.upsample2(c2) # 24, 32\n",
    "        x = layers.concatenate([x, p1])\n",
    "        return x, c2, c3\n",
    "\n",
    "class HeadLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(HeadLayer, self).__init__()\n",
    "        self.csp_dense1 = CSPDenseLayer(16, 8, 32)\n",
    "        self.conv1 = layers.Conv2D(16, 1, strides=2, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        self.cbam1 = CBAM(32)\n",
    "        self.csp_dense2 = CSPDenseLayer(32, 16, 32)\n",
    "        self.conv2 = layers.Conv2D(32, 1, strides=2, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        self.cbam2 = CBAM(64)\n",
    "        self.csp_dense3 = CSPDenseLayer(64, 32, 32)\n",
    "\n",
    "    def call(self, x, c2, c3):\n",
    "        out1 = self.csp_dense1(x) # 24, 32\n",
    "        x = self.conv1(out1)  \n",
    "        x = layers.concatenate([x, c2]) \n",
    "        # x = self.cbam1(x)\n",
    "        out2 = self.csp_dense2(x)\n",
    "        x = self.conv2(out2)\n",
    "        x = layers.concatenate([x, c3])\n",
    "        # x = self.cbam2(x)\n",
    "        out3 = self.csp_dense3(x)\n",
    "        return out1, out2, out3\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes=1, num_anchors_per_location=9):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.backbone = BackBone()\n",
    "        self.neck = NeckLayer()\n",
    "        self.head = HeadLayer()\n",
    "\n",
    "        self.prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors_per_location = num_anchors_per_location\n",
    "        \n",
    "        self.classification_head = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer=self.prior_probability),\n",
    "            layers.Conv2D(64, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer=self.prior_probability),\n",
    "            layers.Conv2D(self.num_anchors_per_location * num_classes, 3, padding=\"same\", kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer=self.prior_probability),\n",
    "            # layers.Activation('selu')\n",
    "        ])\n",
    "        \n",
    "        self.regression_head = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer=self.prior_probability),\n",
    "            layers.Conv2D(64, 1, strides=1, padding='same', kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer=self.prior_probability),\n",
    "            layers.Conv2D(self.num_anchors_per_location * 4, 3, padding=\"same\", kernel_initializer=tf.keras.initializers.HeNormal(), bias_initializer=self.prior_probability),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        p1, p2, p3 = self.backbone(inputs)\n",
    "        x, c2, c3 = self.neck(p1, p2, p3)\n",
    "        out1, out2, out3 = self.head(x, c2, c3)\n",
    "\n",
    "        cls_outputs = []\n",
    "        reg_outputs = []\n",
    "        N = tf.shape(inputs)[0]\n",
    "\n",
    "        for _, feature in enumerate([out1, out2, out3]):\n",
    "            cls_output = self.classification_head(feature)\n",
    "            reg_output = self.regression_head(feature)\n",
    "            \n",
    "            H, W = feature.shape[1], feature.shape[2]\n",
    "            num_anchors = H * W * self.num_anchors_per_location\n",
    "\n",
    "            reg_output = tf.reshape(reg_output, [N, num_anchors, 4])\n",
    "            cls_output = tf.reshape(cls_output, [N, num_anchors, self.num_classes])\n",
    "\n",
    "            cls_outputs.append(cls_output)\n",
    "            reg_outputs.append(reg_output)\n",
    "\n",
    "        reg_outputs = tf.concat(reg_outputs, axis=1)\n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        # clipped_outputs = tf.clip_by_value(cls_outputs, -2, 1)\n",
    "        final_output = tf.concat([reg_outputs, cls_outputs], axis=-1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " back_bone_9 (BackBone)      multiple                  56398     \n",
      "                                                                 \n",
      " neck_layer_9 (NeckLayer)    multiple                  48848     \n",
      "                                                                 \n",
      " head_layer_9 (HeadLayer)    multiple                  168080    \n",
      "                                                                 \n",
      " sequential_18 (Sequential)  (None, None, None, 9)     8361      \n",
      "                                                                 \n",
      " sequential_19 (Sequential)  (None, None, None, 36)    23940     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 305627 (1.17 MB)\n",
      "Trainable params: 303291 (1.16 MB)\n",
      "Non-trainable params: 2336 (9.12 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(num_classes=1)\n",
    "model.trainable = True\n",
    "model.build(input_shape=(None, 24, 32, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BoxLoss(tf.losses.Loss):  \n",
    "#     def __init__(self, delta):\n",
    "#         super(BoxLoss, self).__init__(\n",
    "#             reduction=\"none\", name=\"BoxLoss\"\n",
    "#         )\n",
    "#         self._delta = delta\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         difference = y_true - y_pred\n",
    "#         absolute_difference = tf.abs(difference)\n",
    "#         squared_difference = difference ** 2\n",
    "#         loss = tf.where(\n",
    "#             tf.less_equal(absolute_difference, self._delta),  # 여기를 수정\n",
    "#             0.5 * squared_difference,\n",
    "#             absolute_difference - 0.5\n",
    "#         )\n",
    "\n",
    "#         return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClassificationLoss(tf.losses.Loss):   \n",
    "#     def __init__(self, alpha, gamma):\n",
    "#         super(ClassificationLoss, self).__init__(\n",
    "#             reduction=\"none\", name=\"ClassificationLoss\"\n",
    "#         )\n",
    "#         self._alpha = alpha\n",
    "#         self._gamma = gamma\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#             labels=y_true, logits=y_pred)\n",
    "        \n",
    "#         probs = tf.nn.sigmoid(y_pred)\n",
    "#         alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "#         pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        \n",
    "#         loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "\n",
    "#         return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Loss(tf.losses.Loss):    \n",
    "#     def __init__(self, num_classes=1, alpha=0.75, gamma=2.0, delta=1.0):\n",
    "#         super(Loss, self).__init__(reduction=\"auto\", name=\"Loss\")\n",
    "#         self._cls_loss = ClassificationLoss(alpha, gamma)\n",
    "#         self._box_loss = BoxLoss(delta)\n",
    "#         self._num_classes = num_classes\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "#         print(y_pred.shape)\n",
    "#         # 바운딩 박스 레이블과 예측값\n",
    "#         box_labels = y_true[:, :, :4]\n",
    "#         box_predictions = y_pred[:, :, :4]\n",
    "#         # 클래스 레이블과 예측값\n",
    "#         # cls_labels = tf.one_hot(\n",
    "#         #     tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "#         #     depth=self._num_classes,\n",
    "#         #     dtype=tf.float32,\n",
    "#         # )\n",
    "#         # print(cls_labels)\n",
    "#         cls_labels = y_true[:, :, 4:]\n",
    "#         cls_predictions = y_pred[:, :, 4:]\n",
    "#         # print(cls_predictions)\n",
    "#         # cls_true = y_true[:, :, 4:]\n",
    "#         # positive와 ignore 마스크\n",
    "#         positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "#         ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "\n",
    "#         cls_loss = self._cls_loss(cls_labels, cls_predictions)\n",
    "#         box_loss = self._box_loss(box_labels, box_predictions)\n",
    "\n",
    "#         # Positive 예시에 대한 추가 가중치 적용\n",
    "#         # positive_weight_multiplier = 10.0\n",
    "#         # cls_loss = tf.where(tf.equal(positive_mask, 1.0), cls_loss * positive_weight_multiplier, cls_loss)\n",
    "\n",
    "#         # Ignore 예시에 대한 손실 0으로 설정\n",
    "#         cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, cls_loss)\n",
    "#         box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "\n",
    "#         # 손실 정규화\n",
    "#         normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "#         cls_loss = tf.math.divide_no_nan(tf.reduce_sum(cls_loss, axis=-1), normalizer)\n",
    "#         box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        \n",
    "#         # 최종 손실 계산\n",
    "#         loss = cls_loss + box_loss\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxLoss(tf.losses.Loss):\n",
    "    def __init__(self, delta):\n",
    "        super(BoxLoss, self).__init__(reduction=\"none\", name=\"BoxLoss\")\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less_equal(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5 * self._delta\n",
    "        )\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "class ClassificationLoss(tf.losses.Loss):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(ClassificationLoss, self).__init__(reduction=\"none\", name=\"ClassificationLoss\")\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred = tf.clip_by_value(y_pred, -2, 1)  # 로짓 값을 -2와 1 사이로 제한\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        modified_pt = tf.clip_by_value(1.0 - pt + 1e-4, 0, 1)  # 1e-4를 추가하고 0과 1 사이로 값을 제한합니다.\n",
    "        loss = alpha * tf.math.pow(modified_pt, self._gamma) * cross_entropy\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ClassificationLoss(tf.losses.Loss):\n",
    "#     def __init__(self, alpha=0.25, gamma=2.0):\n",
    "#         super(ClassificationLoss, self).__init__(reduction=\"none\", name=\"ClassificationLoss\")\n",
    "#         self._alpha = alpha\n",
    "#         self._gamma = gamma\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # Ignore에 해당하는 인덱스 식별\n",
    "#         ignore_mask = tf.cast(tf.equal(y_true, -2.0), tf.float32)\n",
    "        \n",
    "#         # Ignore에 해당하는 부분은 0으로 만들어 손실 계산에서 제외\n",
    "#         y_true = tf.maximum(y_true, 0)\n",
    "        \n",
    "#         cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "#         probs = tf.nn.sigmoid(y_pred)\n",
    "#         alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "#         pt = tf.clip_by_value(probs, 1e-8, 1.0)\n",
    "#         pt = tf.where(tf.equal(y_true, 1.0), pt, 1 - pt)\n",
    "#         loss = alpha * tf.math.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "#         loss = tf.maximum(loss, 0.0)\n",
    "        \n",
    "#         # Ignore에 해당하는 부분은 손실을 0으로 설정\n",
    "#         loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, loss)\n",
    "        \n",
    "#         return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ClassificationLoss(tf.losses.Loss):\n",
    "#     def __init__(self, alpha, gamma):\n",
    "#         super(ClassificationLoss, self).__init__(reduction=\"none\", name=\"ClassificationLoss\")\n",
    "#         self._alpha = alpha\n",
    "#         self._gamma = gamma\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         y_pred = tf.math.log(1 + tf.exp(y_pred))  # Log 적용\n",
    "#         cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "#         probs = tf.nn.sigmoid(y_pred)\n",
    "#         alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "#         pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "#         loss = alpha * tf.math.pow(1.0 - pt + 1e-6, self._gamma) * cross_entropy\n",
    "#         return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "class CIoULoss(tf.losses.Loss):\n",
    "    def __init__(self, delta=1.0, anchors=None, name=\"CIoULoss\"):\n",
    "        super(CIoULoss, self).__init__(reduction=\"none\", name=name)\n",
    "        self._delta = delta\n",
    "        self.anchors = anchors\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Decode true and predicted boxes\n",
    "        true_boxes = self.decode_predictions(y_true, self.anchors)\n",
    "        pred_boxes = self.decode_predictions(y_pred, self.anchors)\n",
    "\n",
    "        # Calculate IoU\n",
    "        iou = self.iou(true_boxes, pred_boxes)\n",
    "\n",
    "        # Calculate center distances\n",
    "        center_true = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) / 2\n",
    "        center_pred = (pred_boxes[..., 0:2] + pred_boxes[..., 2:4]) / 2\n",
    "        center_distance = tf.reduce_sum(tf.square(center_true - center_pred), axis=-1)\n",
    "\n",
    "        # Calculate diagonal lengths\n",
    "        diag_true = tf.square(true_boxes[..., 2:4] - true_boxes[..., 0:2])\n",
    "        diag_pred = tf.square(pred_boxes[..., 2:4] - pred_boxes[..., 0:2])\n",
    "        diag_sum = diag_true + diag_pred\n",
    "\n",
    "        # Calculate aspect ratio\n",
    "        aspect_ratio_true = diag_true[..., 0] / diag_true[..., 1]\n",
    "        aspect_ratio_pred = diag_pred[..., 0] / diag_pred[..., 1]\n",
    "        aspect_ratio = tf.square(tf.maximum(aspect_ratio_true / aspect_ratio_pred, aspect_ratio_pred / aspect_ratio_true))\n",
    "\n",
    "        # Calculate CIOU\n",
    "        ciou = iou - (center_distance / tf.reduce_sum(diag_sum, axis=-1)) - (aspect_ratio / tf.reduce_sum(diag_sum, axis=-1))\n",
    "        ciou_loss = 1 - tf.clip_by_value(ciou, 0.0, 1.0 - self._delta)\n",
    "\n",
    "        return ciou_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(y_true, y_pred):\n",
    "        # Calculate intersection\n",
    "        x1 = tf.maximum(y_true[..., 0], y_pred[..., 0])\n",
    "        y1 = tf.maximum(y_true[..., 1], y_pred[..., 1])\n",
    "        x2 = tf.minimum(y_true[..., 2], y_pred[..., 2])\n",
    "        y2 = tf.minimum(y_true[..., 3], y_pred[..., 3])\n",
    "        \n",
    "        intersection = tf.maximum(0.0, x2 - x1) * tf.maximum(0.0, y2 - y1)\n",
    "        \n",
    "        # Calculate union\n",
    "        area_true = (y_true[..., 2] - y_true[..., 0]) * (y_true[..., 3] - y_true[..., 1])\n",
    "        area_pred = (y_pred[..., 2] - y_pred[..., 0]) * (y_pred[..., 3] - y_pred[..., 1])\n",
    "        union = area_true + area_pred - intersection\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = intersection / (union + 1e-6)\n",
    "        \n",
    "        return iou\n",
    "\n",
    "    def decode_predictions(self, labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "        anchor_x = anchors[..., 0]\n",
    "        anchor_y = anchors[..., 1]\n",
    "        anchor_w = anchors[..., 2]\n",
    "        anchor_h = anchors[..., 3]\n",
    "\n",
    "        cx = labels[..., 0] * box_variance[0] * anchor_w + anchor_x\n",
    "        cy = labels[..., 1] * box_variance[1] * anchor_h + anchor_y\n",
    "        width = tf.exp(labels[..., 2] * box_variance[2]) * anchor_w\n",
    "        height = tf.exp(labels[..., 3] * box_variance[3]) * anchor_h\n",
    "\n",
    "        x_min = cx - width / 2\n",
    "        y_min = cy - height / 2\n",
    "        x_max = x_min + width\n",
    "        y_max = y_min + height\n",
    "\n",
    "        decoded_boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "        return decoded_boxes\n",
    "\n",
    "class Loss(tf.losses.Loss):\n",
    "    def __init__(self, num_classes=1, alpha=0.5, gamma=2.0, delta=2.0, ciou_delta=1.0, anchors=None):\n",
    "        super(Loss, self).__init__(reduction=\"auto\", name=\"Loss\")\n",
    "        self._cls_loss = ClassificationLoss(alpha, gamma)\n",
    "        # self._cls_loss = ClassificationLoss()\n",
    "        self._box_loss = BoxLoss(delta)\n",
    "        self._ciou_loss = CIoULoss(ciou_delta, anchors)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[..., :4]\n",
    "        box_predictions = y_pred[..., :4]\n",
    "        cls_labels = y_true[..., 4]\n",
    "        cls_predictions = y_pred[..., 4]\n",
    "\n",
    "        positive_mask = tf.cast(tf.math.greater(y_true[..., 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.math.equal(y_true[..., 4], -2.0), dtype=tf.float32)\n",
    "\n",
    "        cls_loss = self._cls_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        # ciou_loss = self._ciou_loss(box_labels, box_predictions)\n",
    "\n",
    "        cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, cls_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        # ciou_loss = tf.where(tf.equal(positive_mask, 1.0), ciou_loss, 0.0)\n",
    "\n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1) + 1e-6\n",
    "        cls_loss = tf.math.divide_no_nan(tf.reduce_sum(cls_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        # ciou_loss = tf.math.divide_no_nan(tf.reduce_sum(ciou_loss, axis=-1), normalizer)\n",
    "\n",
    "        cls_weight = 1.0\n",
    "        box_weight = 1.5\n",
    "        ciou_weight = 0.5\n",
    "\n",
    "\n",
    "        # 최종 손실 계산\n",
    "        # final_loss = cls_weight * cls_loss + box_weight * box_loss  # + ciou_weight * ciou_loss\n",
    "        # loss = cls_weight * cls_loss + box_weight * box_loss + ciou_weight * ciou_loss\n",
    "        # loss = cls_weight * cls_loss + box_weight * box_loss\n",
    "        # cls_loss = tf.clip_by_value(cls_loss, clip_value_min=0.0, clip_value_max=1e6)\n",
    "        return cls_loss\n",
    "        # return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BoxLoss(tf.losses.Loss):\n",
    "#     def __init__(self, delta):\n",
    "#         super(BoxLoss, self).__init__(reduction=\"none\", name=\"BoxLoss\")\n",
    "#         self._delta = delta\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         difference = y_true - y_pred\n",
    "#         absolute_difference = tf.abs(difference)\n",
    "#         squared_difference = difference ** 2\n",
    "#         loss = tf.where(\n",
    "#             tf.less(absolute_difference, self._delta),\n",
    "#             0.5 * squared_difference,\n",
    "#             absolute_difference - 0.5 * self._delta\n",
    "#         )\n",
    "#         return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "# class ClassificationLoss(tf.losses.Loss):\n",
    "#     def __init__(self, alpha, gamma):\n",
    "#         super(ClassificationLoss, self).__init__(reduction=\"none\", name=\"ClassificationLoss\")\n",
    "#         self._alpha = alpha\n",
    "#         self._gamma = gamma\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "#         probs = tf.nn.sigmoid(y_pred)\n",
    "#         alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "#         pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "#         focal_loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "#         return tf.reduce_sum(focal_loss, axis=-1)\n",
    "\n",
    "# class Loss(tf.losses.Loss):\n",
    "#     def __init__(self, num_classes=1, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "#         super(Loss, self).__init__(reduction=\"auto\", name=\"Loss\")\n",
    "#         self._cls_loss = ClassificationLoss(alpha, gamma)\n",
    "#         self._box_loss = BoxLoss(delta)\n",
    "#         self._num_classes = num_classes\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "#         box_labels = y_true[:, :, :4]\n",
    "#         box_predictions = y_pred[:, :, :4]\n",
    "#         cls_labels = y_true[:, :, 4:]\n",
    "#         cls_predictions = y_pred[:, :, 4:]\n",
    "\n",
    "#         positive_mask = tf.cast(tf.math.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "#         ignore_mask = tf.cast(tf.math.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "\n",
    "#         cls_loss = self._cls_loss(cls_labels, cls_predictions)\n",
    "#         box_loss = self._box_loss(box_labels, box_predictions)\n",
    "\n",
    "#         cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, cls_loss)\n",
    "#         box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "\n",
    "#         normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "#         cls_loss = tf.math.divide_no_nan(tf.reduce_sum(cls_loss, axis=-1), normalizer)\n",
    "#         box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        \n",
    "#         loss = cls_loss + box_loss\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAveragePrecision(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, anchors, iou_threshold=0.5, **kwargs):\n",
    "        super(MeanAveragePrecision, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', shape=(num_classes,))\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros', shape=(num_classes,))\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros', shape=(num_classes,))\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # 형상 검증 코드 추가\n",
    "        assert y_true.shape[-1] == (self.num_classes + 1) * 4\n",
    "        assert y_pred.shape[-1] == (self.num_classes + 1) * 4\n",
    "\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "        for class_id in tf.range(1, self.num_classes):  # 배경 클래스(0) 제외\n",
    "            true_boxes = y_true[..., class_id * 4:(class_id + 1) * 4]\n",
    "            pred_offsets = y_pred[..., class_id * 4:(class_id + 1) * 4]\n",
    "            \n",
    "            pred_boxes = self.decode_predictions(pred_offsets, self.anchors)\n",
    "    \n",
    "            iou = self.calculate_iou(true_boxes, pred_boxes)\n",
    "            detections_mask = iou >= self.iou_threshold\n",
    "    \n",
    "            tp = tf.reduce_sum(tf.cast(detections_mask, tf.float32), axis=[0, 1])\n",
    "            pred_areas = (pred_boxes[..., 2] - pred_boxes[..., 0]) * (pred_boxes[..., 3] - pred_boxes[..., 1])\n",
    "            fp = tf.reduce_sum(tf.cast(pred_areas > 0, tf.float32), axis=[0, 1]) - tp\n",
    "            true_areas = (true_boxes[..., 2] - true_boxes[..., 0]) * (true_boxes[..., 3] - true_boxes[..., 1])\n",
    "            fn = tf.reduce_sum(tf.cast(true_areas > 0, tf.float32), axis=[0, 1]) - tp\n",
    "    \n",
    "            self.true_positives[class_id].assign(self.true_positives[class_id] + tp)\n",
    "            self.false_positives[class_id].assign(self.false_positives[class_id] + fp)\n",
    "            self.false_negatives[class_id].assign(self.false_negatives[class_id] + fn)\n",
    "\n",
    "\n",
    "\n",
    "    def decode_predictions(self, labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "        anchor_x = anchors[..., 0]\n",
    "        anchor_y = anchors[..., 1]\n",
    "        anchor_w = anchors[..., 2]\n",
    "        anchor_h = anchors[..., 3]\n",
    "    \n",
    "        cx = labels[..., 0] * box_variance[0] * anchor_w + anchor_x\n",
    "        cy = labels[..., 1] * box_variance[1] * anchor_h + anchor_y\n",
    "        width = tf.exp(labels[..., 2] * box_variance[2]) * anchor_w\n",
    "        height = tf.exp(labels[..., 3] * box_variance[3]) * anchor_h\n",
    "    \n",
    "        x_min = cx - width / 2\n",
    "        y_min = cy - height / 2\n",
    "        x_max = x_min + width\n",
    "        y_max = y_min + height\n",
    "    \n",
    "        decoded_boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "        return decoded_boxes\n",
    "\n",
    "    def result(self):\n",
    "        precisions = tf.where(self.true_positives + self.false_positives > 0,\n",
    "                              self.true_positives / (self.true_positives + self.false_positives),\n",
    "                              tf.zeros_like(self.true_positives))\n",
    "        recalls = tf.where(self.true_positives + self.false_negatives > 0,\n",
    "                           self.true_positives / (self.true_positives + self.false_negatives),\n",
    "                           tf.zeros_like(self.true_positives))\n",
    "        \n",
    "        all_precisions = tf.stack(precisions, axis=0)\n",
    "        all_recalls = tf.stack(recalls, axis=0)\n",
    "        \n",
    "        sorted_indices = tf.argsort(all_recalls, direction='DESCENDING')\n",
    "        all_precisions_sorted = tf.gather(all_precisions, sorted_indices)\n",
    "        all_recalls_sorted = tf.gather(all_recalls, sorted_indices)\n",
    "        \n",
    "        # Calculate the revised precisions to sweep over recall thresholds\n",
    "        max_precisions = tf.scan(lambda a, x: tf.maximum(a, x), all_precisions_sorted, initializer=0.0, reverse=True)\n",
    "        \n",
    "        # Calculate the area under the precision-recall curve using numerical integration\n",
    "        recall_delta = tf.concat([[0.0], all_recalls_sorted[:-1] - all_recalls_sorted[1:]], axis=0)\n",
    "        average_precision = tf.reduce_sum(max_precisions * recall_delta)\n",
    "        \n",
    "        return average_precision\n",
    "\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
    "\n",
    "    def calculate_iou(self, true_boxes, pred_boxes):\n",
    "        x1 = tf.maximum(true_boxes[..., 0], pred_boxes[..., 0])\n",
    "        y1 = tf.maximum(true_boxes[..., 1], pred_boxes[..., 1])\n",
    "        x2 = tf.minimum(true_boxes[..., 2], pred_boxes[..., 2])\n",
    "        y2 = tf.minimum(true_boxes[..., 3], pred_boxes[..., 3])\n",
    "\n",
    "        intersection_area = tf.maximum(0.0, x2 - x1) * tf.maximum(0.0, y2 - y1)\n",
    "        pred_areas = (pred_boxes[..., 2] - pred_boxes[..., 0]) * (pred_boxes[..., 3] - pred_boxes[..., 1])\n",
    "        true_areas = (true_boxes[..., 2] - true_boxes[..., 0]) * (true_boxes[..., 3] - true_boxes[..., 1])\n",
    "\n",
    "        union_area = true_areas + pred_areas - intersection_area\n",
    "\n",
    "        iou = intersection_area / (union_area + tf.keras.backend.epsilon())\n",
    "        return iou\n",
    "\n",
    "\n",
    "# class IntersectionOverUnion(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, num_classes, anchors, **kwargs):\n",
    "#         super(IntersectionOverUnion, self).__init__(**kwargs)\n",
    "#         self.num_classes = num_classes\n",
    "#         self.anchors = anchors\n",
    "#         self.intersection_over_union = self.add_weight(name='iou', initializer='zeros', shape=(num_classes,))\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "#         batch_size = tf.shape(y_true)[0]  # 배치 크기 가져오기\n",
    "    \n",
    "#         for batch in tf.range(batch_size):  # 배치 별로 반복\n",
    "#             for i in range(self.num_classes):\n",
    "#                 true_boxes = y_true[batch, ..., i * 4:(i + 1) * 4]\n",
    "#                 pred_offsets = y_pred[batch, ..., i * 4:(i + 1) * 4]\n",
    "                \n",
    "#                 pred_boxes = self.decode_predictions(pred_offsets, self.anchors)\n",
    "    \n",
    "#                 iou = self.calculate_iou(true_boxes, pred_boxes)\n",
    "#                 self.intersection_over_union[i].assign(self.intersection_over_union[i] + tf.reduce_mean(iou))\n",
    "                \n",
    "#     def decode_predictions(self, labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "#         anchor_x = anchors[..., 0]\n",
    "#         anchor_y = anchors[..., 1]\n",
    "#         anchor_w = anchors[..., 2]\n",
    "#         anchor_h = anchors[..., 3]\n",
    "    \n",
    "#         cx = labels[..., 0] * box_variance[0] * anchor_w + anchor_x\n",
    "#         cy = labels[..., 1] * box_variance[1] * anchor_h + anchor_y\n",
    "#         width = tf.exp(labels[..., 2] * box_variance[2]) * anchor_w\n",
    "#         height = tf.exp(labels[..., 3] * box_variance[3]) * anchor_h\n",
    "    \n",
    "#         x_min = cx - width / 2\n",
    "#         y_min = cy - height / 2\n",
    "#         x_max = x_min + width\n",
    "#         y_max = y_min + height\n",
    "    \n",
    "#         decoded_boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "#         return decoded_boxes\n",
    "\n",
    "#     def result(self):\n",
    "#         return self.intersection_over_union\n",
    "\n",
    "#     def reset_state(self):\n",
    "#         self.intersection_over_union.assign(tf.zeros_like(self.intersection_over_union))\n",
    "\n",
    "#     def calculate_iou(self, true_boxes, pred_boxes):\n",
    "#         x1 = tf.maximum(true_boxes[..., 0], pred_boxes[..., 0])\n",
    "#         y1 = tf.maximum(true_boxes[..., 1], pred_boxes[..., 1])\n",
    "#         x2 = tf.minimum(true_boxes[..., 2], pred_boxes[..., 2])\n",
    "#         y2 = tf.minimum(true_boxes[..., 3], pred_boxes[..., 3])\n",
    "\n",
    "#         intersection_area = tf.maximum(0.0, x2 - x1) * tf.maximum(0.0, y2 - y1)\n",
    "#         true_area = (true_boxes[..., 2] - true_boxes[..., 0]) * (true_boxes[..., 3] - true_boxes[..., 1])\n",
    "#         pred_area = (pred_boxes[..., 2] - pred_boxes[..., 0]) * (pred_boxes[..., 3] - pred_boxes[..., 1])\n",
    "#         union_area = true_area + pred_area - intersection_area\n",
    "\n",
    "#         iou = intersection_area / (union_area + tf.keras.backend.epsilon())\n",
    "#         return iou "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntersectionOverUnion(tf.keras.metrics.Metric):\n",
    "    def __init__(self, anchors, **kwargs):\n",
    "        super(IntersectionOverUnion, self).__init__(**kwargs)\n",
    "        self.anchors = tf.convert_to_tensor(anchors, dtype=tf.float32)\n",
    "        self.box_variance = tf.constant([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "        self.total_iou = self.add_weight(name='total_iou', initializer='zeros')\n",
    "        self.num_batches = self.add_weight(name='num_batches', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Decode predictions\n",
    "        pred_boxes = self.decode_boxes(y_pred[:, :, :4])\n",
    "        \n",
    "        # Filter predictions based on confidence score\n",
    "        confidence_scores = y_pred[..., 4]\n",
    "        mask = confidence_scores > 0.5\n",
    "        \n",
    "        # Use tf.map_fn to process each batch separately\n",
    "        def compute_iou_for_batch(args):\n",
    "            y_true_batch, pred_boxes_batch, mask_batch = args\n",
    "            filtered_boxes = tf.boolean_mask(pred_boxes_batch, mask_batch)\n",
    "            if tf.shape(filtered_boxes)[0] == 0:\n",
    "                return 0.0  # Avoid empty tensor issues\n",
    "            iou = self.calculate_iou(y_true_batch, filtered_boxes)\n",
    "            return tf.reduce_mean(iou)\n",
    "\n",
    "        # Apply compute_iou_for_batch to each batch\n",
    "        mean_iou_per_batch = tf.map_fn(compute_iou_for_batch, (y_true, pred_boxes, mask), dtype=tf.float32)\n",
    "        \n",
    "        # Update metric state\n",
    "        self.total_iou.assign_add(tf.reduce_sum(mean_iou_per_batch))\n",
    "        self.num_batches.assign_add(tf.cast(tf.size(mean_iou_per_batch), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_iou / self.num_batches\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_iou.assign(0)\n",
    "        self.num_batches.assign(0)\n",
    "\n",
    "    def decode_boxes(self, predictions):\n",
    "        dx, dy, dw, dh = predictions[..., 0], predictions[..., 1], predictions[..., 2], predictions[..., 3]\n",
    "        anchor_x, anchor_y, anchor_w, anchor_h = self.anchors[..., 0], self.anchors[..., 1], self.anchors[..., 2], self.anchors[..., 3]\n",
    "\n",
    "        cx = dx * self.box_variance[0] * anchor_w + anchor_x\n",
    "        cy = dy * self.box_variance[1] * anchor_h + anchor_y\n",
    "        width = tf.exp(dw * self.box_variance[2]) * anchor_w\n",
    "        height = tf.exp(dh * self.box_variance[3]) * anchor_h\n",
    "\n",
    "        x_min = cx - width / 2\n",
    "        y_min = cy - height / 2\n",
    "        x_max = x_min + width\n",
    "        y_max = y_min + height\n",
    "\n",
    "        return tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "\n",
    "    def calculate_iou(self, true_boxes, pred_boxes):\n",
    "        # Expand dimensions for broadcasting\n",
    "        true_boxes = tf.expand_dims(true_boxes, axis=1)  # Shape: [batch_size, 1, 4]\n",
    "        pred_boxes = tf.expand_dims(pred_boxes, axis=0)  # Shape: [1, num_pred_boxes, 4]\n",
    "    \n",
    "        # Calculate intersection coordinates\n",
    "        inter_x1 = tf.maximum(true_boxes[..., 0], pred_boxes[..., 0])\n",
    "        inter_y1 = tf.maximum(true_boxes[..., 1], pred_boxes[..., 1])\n",
    "        inter_x2 = tf.minimum(true_boxes[..., 2], pred_boxes[..., 2])\n",
    "        inter_y2 = tf.minimum(true_boxes[..., 3], pred_boxes[..., 3])\n",
    "    \n",
    "        # Calculate intersection and union areas\n",
    "        inter_area = tf.maximum(inter_x2 - inter_x1, 0) * tf.maximum(inter_y2 - inter_y1, 0)\n",
    "        true_area = (true_boxes[..., 2] - true_boxes[..., 0]) * (true_boxes[..., 3] - true_boxes[..., 1])\n",
    "        pred_area = (pred_boxes[..., 2] - pred_boxes[..., 0]) * (pred_boxes[..., 3] - pred_boxes[..., 1])\n",
    "    \n",
    "        union_area = true_area + pred_area - inter_area\n",
    "        iou = inter_area / (union_area + tf.keras.backend.epsilon())\n",
    "    \n",
    "        return iou\n",
    "\n",
    "\n",
    "    def compute_iou_for_batch(self, args):\n",
    "        y_true_batch, pred_boxes_batch, mask_batch = args\n",
    "        filtered_boxes = tf.boolean_mask(pred_boxes_batch, mask_batch)\n",
    "        if tf.shape(filtered_boxes)[0] == 0:\n",
    "            return 0.0  # Avoid empty tensor issues\n",
    "        return self.calculate_iou(y_true_batch, filtered_boxes)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class MeanAveragePrecision(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, num_classes, anchors, box_variance=[0.1, 0.1, 0.2, 0.2], iou_threshold=0.5, **kwargs):\n",
    "#         super(MeanAveragePrecision, self).__init__(**kwargs)\n",
    "#         self.num_classes = num_classes\n",
    "#         self.anchors = anchors\n",
    "#         self.box_variance = box_variance  # 속성 추가\n",
    "#         self.iou_threshold = iou_threshold\n",
    "#         self.true_positives = self.add_weight(name='tp', initializer='zeros', shape=(num_classes,))\n",
    "#         self.false_positives = self.add_weight(name='fp', initializer='zeros', shape=(num_classes,))\n",
    "#         self.false_negatives = self.add_weight(name='fn', initializer='zeros', shape=(num_classes,))\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         for i in range(self.num_classes):\n",
    "#             true_boxes = y_true[..., i * 4:(i + 1) * 4]\n",
    "#             pred_offsets = y_pred[..., i * 4:(i + 1) * 4]\n",
    "#             pred_boxes = self.decode_predictions(pred_offsets, self.anchors[i])\n",
    "\n",
    "#             iou = self.calculate_iou(true_boxes, pred_boxes)\n",
    "#             mask = tf.greater_equal(iou, self.iou_threshold)\n",
    "\n",
    "#             true_positives = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "#             false_positives = tf.reduce_sum(tf.cast(tf.logical_not(mask), tf.float32)) - true_positives\n",
    "#             false_negatives = tf.reduce_sum(1 - tf.cast(mask, tf.float32))\n",
    "\n",
    "#             # Update using scatter add\n",
    "#             indices = [[i]]\n",
    "#             self.true_positives.assign(tf.tensor_scatter_nd_add(self.true_positives, indices, [true_positives]))\n",
    "#             self.false_positives.assign(tf.tensor_scatter_nd_add(self.false_positives, indices, [false_positives]))\n",
    "#             self.false_negatives.assign(tf.tensor_scatter_nd_add(self.false_negatives, indices, [false_negatives]))\n",
    "\n",
    "#     def result(self):\n",
    "#         precisions = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
    "#         recalls = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "#         return tf.reduce_mean(precisions), tf.reduce_mean(recalls)\n",
    "\n",
    "#     def reset_state(self):\n",
    "#         self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "#         self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
    "#         self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
    "\n",
    "#     def calculate_iou(self, true_boxes, pred_boxes):\n",
    "#         inter_x1 = tf.maximum(true_boxes[..., 0], pred_boxes[..., 0])\n",
    "#         inter_y1 = tf.maximum(true_boxes[..., 1], pred_boxes[..., 1])\n",
    "#         inter_x2 = tf.minimum(true_boxes[..., 2], pred_boxes[..., 2])\n",
    "#         inter_y2 = tf.minimum(true_boxes[..., 3], pred_boxes[..., 3])\n",
    "#         inter_area = tf.maximum(inter_x2 - inter_x1, 0) * tf.maximum(inter_y2 - inter_y1, 0)\n",
    "\n",
    "#         true_area = (true_boxes[..., 2] - true_boxes[..., 0]) * (true_boxes[..., 3] - true_boxes[..., 1])\n",
    "#         pred_area = (pred_boxes[..., 2] - pred_boxes[..., 0]) * (pred_boxes[..., 3] - pred_boxes[..., 1])\n",
    "#         union_area = true_area + pred_area - inter_area\n",
    "\n",
    "#         return inter_area / (union_area + tf.keras.backend.epsilon())\n",
    "\n",
    "\n",
    "#     def decode_boxes(self, predictions, anchors):\n",
    "#         box_variance = [0.1, 0.1, 0.2, 0.2]\n",
    "#         dx = predictions[..., 0]\n",
    "#         dy = predictions[..., 1]\n",
    "#         dw = predictions[..., 2]\n",
    "#         dh = predictions[..., 3]\n",
    "#         anchor_x = anchors[..., 0]\n",
    "#         anchor_y = anchors[..., 1]\n",
    "#         anchor_w = anchors[..., 2]\n",
    "#         anchor_h = anchors[..., 3]\n",
    "\n",
    "#         cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "#         cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "#         width = tf.exp(dw * box_variance[2]) * anchor_w\n",
    "#         height = tf.exp(dh * box_variance[3]) * anchor_h\n",
    "\n",
    "#         x_min = cx - width / 2\n",
    "#         y_min = cy - height / 2\n",
    "#         x_max = x_min + width\n",
    "#         y_max = y_min + height\n",
    "\n",
    "#         boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "#         return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BoxLoss(tf.losses.Loss):\n",
    "#     def __init__(self, delta):\n",
    "#         super(BoxLoss, self).__init__(reduction=\"none\", name=\"BoxLoss\")\n",
    "#         self._delta = delta\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         difference = y_true - y_pred\n",
    "#         absolute_difference = tf.abs(difference)\n",
    "#         squared_difference = difference ** 2\n",
    "#         loss = tf.where(\n",
    "#             tf.less_equal(absolute_difference, self._delta),\n",
    "#             0.5 * squared_difference,\n",
    "#             absolute_difference - 0.5 * self._delta\n",
    "#         )\n",
    "#         return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "# class ClassificationLoss(tf.losses.Loss):\n",
    "#     def __init__(self, alpha, gamma):\n",
    "#         super(ClassificationLoss, self).__init__(reduction=\"none\", name=\"ClassificationLoss\")\n",
    "#         self._alpha = alpha\n",
    "#         self._gamma = gamma\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "#         probs = tf.nn.sigmoid(y_pred)\n",
    "#         alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "#         pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "#         loss = alpha * tf.math.pow(1.0 - pt + 1e-6, self._gamma) * cross_entropy\n",
    "#         return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "# class CIoULoss(tf.losses.Loss):\n",
    "#     def __init__(self, delta=1.0, anchors=None, name=\"CIoULoss\"):\n",
    "#         super(CIoULoss, self).__init__(reduction=\"none\", name=name)\n",
    "#         self._delta = delta\n",
    "#         self.anchors = anchors\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # Decode true and predicted boxes\n",
    "#         true_boxes = self.decode_predictions(y_true, self.anchors)\n",
    "#         pred_boxes = self.decode_predictions(y_pred, self.anchors)\n",
    "\n",
    "#         # Calculate IoU\n",
    "#         iou = self.iou(true_boxes, pred_boxes)\n",
    "\n",
    "#         # Calculate center distances\n",
    "#         center_true = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) / 2\n",
    "#         center_pred = (pred_boxes[..., 0:2] + pred_boxes[..., 2:4]) / 2\n",
    "#         center_distance = tf.reduce_sum(tf.square(center_true - center_pred), axis=-1)\n",
    "\n",
    "#         # Calculate diagonal lengths\n",
    "#         diag_true = tf.square(true_boxes[..., 2:4] - true_boxes[..., 0:2])\n",
    "#         diag_pred = tf.square(pred_boxes[..., 2:4] - pred_boxes[..., 0:2])\n",
    "#         diag_sum = diag_true + diag_pred\n",
    "\n",
    "#         # Calculate aspect ratio\n",
    "#         aspect_ratio_true = diag_true[..., 0] / diag_true[..., 1]\n",
    "#         aspect_ratio_pred = diag_pred[..., 0] / diag_pred[..., 1]\n",
    "#         aspect_ratio = tf.square(tf.maximum(aspect_ratio_true / aspect_ratio_pred, aspect_ratio_pred / aspect_ratio_true))\n",
    "\n",
    "#         # Calculate CIOU\n",
    "#         ciou = iou - (center_distance / tf.reduce_sum(diag_sum, axis=-1)) - (aspect_ratio / tf.reduce_sum(diag_sum, axis=-1))\n",
    "#         ciou_loss = 1 - tf.clip_by_value(ciou, 0.0, 1.0 - self._delta)\n",
    "\n",
    "#         return ciou_loss\n",
    "\n",
    "#     @staticmethod\n",
    "#     def iou(y_true, y_pred):\n",
    "#         # Calculate intersection\n",
    "#         x1 = tf.maximum(y_true[..., 0], y_pred[..., 0])\n",
    "#         y1 = tf.maximum(y_true[..., 1], y_pred[..., 1])\n",
    "#         x2 = tf.minimum(y_true[..., 2], y_pred[..., 2])\n",
    "#         y2 = tf.minimum(y_true[..., 3], y_pred[..., 3])\n",
    "        \n",
    "#         intersection = tf.maximum(0.0, x2 - x1) * tf.maximum(0.0, y2 - y1)\n",
    "        \n",
    "#         # Calculate union\n",
    "#         area_true = (y_true[..., 2] - y_true[..., 0]) * (y_true[..., 3] - y_true[..., 1])\n",
    "#         area_pred = (y_pred[..., 2] - y_pred[..., 0]) * (y_pred[..., 3] - y_pred[..., 1])\n",
    "#         union = area_true + area_pred - intersection\n",
    "        \n",
    "#         # Calculate IoU\n",
    "#         iou = intersection / (union + 1e-6)\n",
    "        \n",
    "#         return iou\n",
    "\n",
    "#     def decode_predictions(self, labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "#         anchor_x = anchors[..., 0]\n",
    "#         anchor_y = anchors[..., 1]\n",
    "#         anchor_w = anchors[..., 2]\n",
    "#         anchor_h = anchors[..., 3]\n",
    "\n",
    "#         cx = labels[..., 0] * box_variance[0] * anchor_w + anchor_x\n",
    "#         cy = labels[..., 1] * box_variance[1] * anchor_h + anchor_y\n",
    "#         width = tf.exp(labels[..., 2] * box_variance[2]) * anchor_w\n",
    "#         height = tf.exp(labels[..., 3] * box_variance[3]) * anchor_h\n",
    "\n",
    "#         x_min = cx - width / 2\n",
    "#         y_min = cy - height / 2\n",
    "#         x_max = x_min + width\n",
    "#         y_max = y_min + height\n",
    "\n",
    "#         decoded_boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "#         return decoded_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_learning_rate = 0.0002\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=initial_learning_rate,\n",
    "#     decay_steps=1000,\n",
    "#     decay_rate=0.96,\n",
    "#     staircase=True)\n",
    "\n",
    "initial_learning_rate = 0.005\n",
    "decay_steps = 30\n",
    "decay_rate = 0.96\n",
    "staircase = True\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return initial_learning_rate * (decay_rate ** (epoch // decay_steps))\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "anchor_box = AnchorBox()\n",
    "anchors = anchor_box.get_anchors(24, 32)\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "model = CustomModel(num_classes)\n",
    "loss_fn = Loss(num_classes = 1, anchors=anchors)\n",
    "# optimizer = tf.optimizers.Adam(learning_rate = lr_schedule)\n",
    "\n",
    "anchor_box = AnchorBox()\n",
    "anchors = anchor_box.get_anchors(24, 32)\n",
    "\n",
    "# map_metric = MeanAveragePrecision(num_classes=num_classes, anchors=anchors)\n",
    "iou_metric = IntersectionOverUnion(anchors=anchors)\n",
    "\n",
    "# anchor_box = AnchorBox()\n",
    "# anchors = anchor_box.get_anchors(24, 32)\n",
    "# iou_metric = MultiBoxIoUMetric(anchors=anchors)\n",
    "# 모델 컴파일\n",
    "# model.compile(optimizer=optimizer, \n",
    "#               loss=[loss_fn],\n",
    "#               metrics=['accuracy', iou_metric])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=[loss_fn],\n",
    "              metrics=[Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (9, 24, 32, 1)\n",
      "Labels shape: (9, 9072, 5)\n",
      "Images max: 255.0\n",
      "Images min: 0.0\n",
      "Labels shape: (9, 9072, 4)\n"
     ]
    }
   ],
   "source": [
    "# 가정: train_dataset은 이미 tf.data.Dataset 객체로 생성되어 있음\n",
    "new_batch_size = 9\n",
    "\n",
    "# 기존 데이터셋에서 배치 사이즈를 새로운 값으로 변경\n",
    "train_dataset = train_dataset.unbatch()  # 먼저, 기존 배치를 해제\n",
    "train_dataset = train_dataset.batch(new_batch_size, drop_remainder=True)  # 새로운 배치 사이즈로 재배치\n",
    "\n",
    "# 배치 사이즈 변경 후 데이터셋을 확인하기 위한 코드\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Images max: {tf.reduce_max(images)}\")\n",
    "    print(f\"Images min: {tf.reduce_min(images)}\")\n",
    "    # print(\"labels[:, :, :4]: \", labels[:, :, :4])\n",
    "    print(\"Labels shape:\", labels[:, :, :4].shape)\n",
    "    # print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 15:47:27.096330: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n",
      "2024-04-08 15:47:27.103697: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradient_tape/Loss/SelectV2_1 defined at (most recent call last):\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/tmp/ipykernel_2188983/1750089067.py\", line 3, in <module>\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1130, in train_step\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nrequired broadcastable shapes\n\t [[{{node gradient_tape/Loss/SelectV2_1}}]] [Op:__inference_train_function_2561592]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[406], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# image, label,\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# batch_size= 12,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/Loss/SelectV2_1 defined at (most recent call last):\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/tmp/ipykernel_2188983/1750089067.py\", line 3, in <module>\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1130, in train_step\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/home/gpuadmin/anaconda3/envs/tensor/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nrequired broadcastable shapes\n\t [[{{node gradient_tape/Loss/SelectV2_1}}]] [Op:__inference_train_function_2561592]"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "\n",
    "model.fit(\n",
    "    # image, label,\n",
    "    train_dataset,\n",
    "    # batch_size= 12,\n",
    "    epochs=epochs,\n",
    "    callbacks=lr_callback,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# model.save('ObjectDetection/model_v7', save_format=\"tf\")\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model('ObjectDetection/model_v7')\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# # 입력과 출력의 데이터 타입을 설정\n",
    "# converter.inference_input_type = tf.float32\n",
    "# converter.inference_output_type = tf.float32\n",
    "\n",
    "# tflite_model = converter.convert()\n",
    "# open('ObjectDetection/tflite/model_v7.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img, label in val_dataset.take(10):\n",
    "#     # predictions = inference_model.predict(tf.expand_dims(img[0], axis=0))  # img에 첫 번째 차원을 추가\n",
    "#     predictions = model.predict(tf.expand_dims(img[0], axis=0))  # img에 첫 번째 차원을 추가\n",
    "#     print(predictions[0, :10, :])\n",
    "#     # positive_count = tf.reduce_sum(tf.cast(tf.equal(predictions[:, :, 4], 1.0), tf.int32))\n",
    "#     positive_count = tf.reduce_sum(tf.cast(tf.greater(predictions[:, :, 4], -1.0), tf.int32))\n",
    "#     # ignore_count = tf.reduce_sum(tf.cast(tf.less(predictions[:, :, 4], -2.0), tf.int32))\n",
    "\n",
    "#     print(\"Positive 개수:\", positive_count.numpy())\n",
    "#     # print(\"Negative 개수:\", negative_count.numpy())\n",
    "#     # print(\"Ignore 개수:\", ignore_count.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # decode_predictions 함수 정의\n",
    "# def decode_predictions(labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "#     decoded_boxes = []\n",
    "#     for label_idx, label in enumerate(labels):\n",
    "#         if label[4] > -1.0:  # 양성 레이블 조건 확인\n",
    "#             dx, dy, dw, dh = label[:4]\n",
    "#             anchor = anchors[label_idx]\n",
    "#             anchor_x, anchor_y, anchor_w, anchor_h = anchor\n",
    "#             cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "#             cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "#             width = np.exp(dw * box_variance[2]) * anchor_w\n",
    "#             height = np.exp(dh * box_variance[3]) * anchor_h\n",
    "#             x_min = cx - width / 2\n",
    "#             y_min = cy - height / 2\n",
    "#             decoded_box = [x_min, y_min, width, height]\n",
    "#             decoded_boxes.append(decoded_box)\n",
    "#     return decoded_boxes\n",
    "\n",
    "# # draw_positive_bounding_boxes 함수 정의\n",
    "# def draw_positive_bounding_boxes(image, decoded_boxes):\n",
    "#     plt.imshow(image)\n",
    "#     ax = plt.gca()\n",
    "#     for box in decoded_boxes:\n",
    "#         x_min, y_min, width, height = box\n",
    "#         rect = patches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "#     plt.show()\n",
    "\n",
    "# # 앵커 박스 및 디코딩 로직 사용 예시\n",
    "# # 주의: AnchorBox 클래스의 구현과 train_dataset의 정의가 필요합니다.\n",
    "\n",
    "# # 예를 들어, 앵커 박스 생성 및 train_dataset에서의 사용 예제는 다음과 같습니다:\n",
    "# anchor_box = AnchorBox()\n",
    "# anchors = anchor_box.get_anchors(24, 32)  # 앵커 박스 생성 예시, 실제 사용 시에는 해당 구현에 맞게 조정 필요\n",
    "\n",
    "# for batch in train_dataset.take(1):\n",
    "#     image = batch[0][0].numpy()\n",
    "#     labels = batch[1][0].numpy()\n",
    "#     decoded_boxes = decode_predictions(labels, anchors)\n",
    "#     draw_positive_bounding_boxes(image, decoded_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가정: train_dataset은 이미 tf.data.Dataset 객체로 생성되어 있음\n",
    "# new_batch_size = 9\n",
    "\n",
    "# # 기존 데이터셋에서 배치 사이즈를 새로운 값으로 변경\n",
    "# val_dataset = val_dataset.unbatch()  # 먼저, 기존 배치를 해제\n",
    "# val_dataset = val_dataset.batch(new_batch_size, drop_remainder=True)  # 새로운 배치 사이즈로 재배치\n",
    "\n",
    "# 배치 사이즈 변경 후 데이터셋을 확인하기 위한 코드\n",
    "for images, _, _ in val_dataset.take(1):\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Images max: {tf.reduce_max(images)}\")\n",
    "    print(f\"Images min: {tf.reduce_min(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "\n",
    "    x_overlap = max(0, min(x2, x4) - max(x1, x3))\n",
    "    y_overlap = max(0, min(y2, y4) - max(y1, y3))\n",
    "\n",
    "    intersection = x_overlap * y_overlap\n",
    "    area1 = (x2 - x1) * (y2 - y1)\n",
    "    area2 = (x4 - x3) * (y4 - y3)\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "def decode_predictions(predictions, anchors, box_variance=[0.1, 0.1, 0.2, 0.2], iou_threshold=0.5, score_threshold=0.5, top_n=100):\n",
    "    decoded_boxes = []\n",
    "    scores = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        score = prediction[-1]\n",
    "        if score > score_threshold:\n",
    "            scores.append((score, i)) \n",
    "\n",
    "    # 점수에 따라 내림차순 정렬\n",
    "    scores.sort(reverse=True)\n",
    "\n",
    "    # 상위 N개 선택\n",
    "    scores = scores[:top_n]\n",
    "\n",
    "    # NMS 적용\n",
    "    while scores:\n",
    "        score, i = scores.pop(0)\n",
    "        prediction = predictions[i]\n",
    "        dx, dy, dw, dh = prediction[:4]\n",
    "        anchor = anchors[i]\n",
    "        anchor_x, anchor_y, anchor_w, anchor_h = anchor\n",
    "        cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "        cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "        width = np.exp(dw * box_variance[2]) * anchor_w\n",
    "        height = np.exp(dh * box_variance[3]) * anchor_h\n",
    "        x_min = cx - width / 2\n",
    "        y_min = cy - height / 2\n",
    "        decoded_box = [x_min, y_min, x_min + width, y_min + height, score]\n",
    "        keep = True\n",
    "        for other_box in decoded_boxes:\n",
    "            if iou(decoded_box[:4], other_box[:4]) >= iou_threshold:\n",
    "                keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            decoded_boxes.append(decoded_box)\n",
    "\n",
    "    return decoded_boxes\n",
    "\n",
    "def draw_bounding_boxes(image, boxes, class_names):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='gray')  # 이미지가 grayscale인 경우 cmap='gray'를 추가합니다.\n",
    "    ax = plt.gca()\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max, score = box\n",
    "        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # 박스 위에 클래스 이름과 확률 표시\n",
    "        class_name = class_names[0]  # 예시로 'person' 클래스를 사용합니다.\n",
    "        text = f'{class_name}: {score :.2f}'\n",
    "        # text = f'{class_name}'\n",
    "        ax.text(x_min, y_min, text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# AnchorBox 클래스와 get_anchors 함수가 필요합니다.\n",
    "anchor_box = AnchorBox()\n",
    "anchors = anchor_box.get_anchors(24, 32)  # 앵커 박스 생성 예시\n",
    "\n",
    "class_names = ['person']  # 클래스 이름 리스트\n",
    "\n",
    "for img, _, _ in val_dataset.take(1):    \n",
    "    predictions = model.predict(tf.expand_dims(img[0], axis=0))[0]  # 첫 번째 이미지에 대한 예측 결과\n",
    "    decoded_boxes = decode_predictions(predictions, anchors)  # 예측된 바운딩 박스 디코딩\n",
    "    print(np.array(decoded_boxes))\n",
    "    draw_bounding_boxes(img[0].numpy(), decoded_boxes, class_names)  # 디코딩된 바운딩 박스를 이미지에 그리기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_box = AnchorBox()\n",
    "anchors = anchor_box.get_anchors(24, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodePredictions(tf.keras.layers.Layer):\n",
    "    def __init__(self, confidence_threshold=0.7, iou_threshold=0.5, top_k=4, num_classes=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.top_k = top_k\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        predictions = inputs\n",
    "\n",
    "        # 앵커 박스 생성\n",
    "        anchor_box = AnchorBox()\n",
    "        anchors = anchor_box.get_anchors(24, 32)\n",
    "\n",
    "        # 스코어 필터링\n",
    "        scores = predictions[..., -1]\n",
    "        score_mask = scores > self.confidence_threshold\n",
    "        filtered_predictions = tf.boolean_mask(predictions, score_mask)\n",
    "        filtered_anchors = tf.boolean_mask(anchors, tf.reshape(score_mask, [-1]))\n",
    "\n",
    "        # NMS 적용\n",
    "        boxes = self.decode_boxes(filtered_predictions, filtered_anchors)\n",
    "        nms_indices = tf.image.non_max_suppression(boxes, tf.boolean_mask(scores, score_mask), max_output_size=self.top_k, iou_threshold=self.iou_threshold)\n",
    "        decoded_boxes = tf.gather(boxes, nms_indices)\n",
    "        decoded_scores = tf.gather(tf.boolean_mask(scores, score_mask), nms_indices)\n",
    "\n",
    "        # 바운딩 박스와 스코어 합치기\n",
    "        decoded_predictions = tf.concat([decoded_boxes, tf.expand_dims(decoded_scores, axis=-1)], axis=-1)\n",
    "\n",
    "        return decoded_predictions\n",
    "\n",
    "    def decode_boxes(self, predictions, anchors):\n",
    "        box_variance = [0.1, 0.1, 0.2, 0.2]\n",
    "        dx = predictions[..., 0]\n",
    "        dy = predictions[..., 1]\n",
    "        dw = predictions[..., 2]\n",
    "        dh = predictions[..., 3]\n",
    "        anchor_x = anchors[..., 0]\n",
    "        anchor_y = anchors[..., 1]\n",
    "        anchor_w = anchors[..., 2]\n",
    "        anchor_h = anchors[..., 3]\n",
    "\n",
    "        cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "        cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "        width = tf.exp(dw * box_variance[2]) * anchor_w\n",
    "        height = tf.exp(dh * box_variance[3]) * anchor_h\n",
    "\n",
    "        x_min = cx - width / 2\n",
    "        y_min = cy - height / 2\n",
    "        x_max = x_min + width\n",
    "        y_max = y_min + height\n",
    "\n",
    "        boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "        return boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecodePredictions(tf.keras.layers.Layer):\n",
    "#     def __init__(self, confidence_threshold=0.7, num_classes=1, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.confidence_threshold = confidence_threshold\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         predictions = inputs\n",
    "\n",
    "#         # 앵커 박스 생성\n",
    "#         anchor_box = AnchorBox()\n",
    "#         anchors = anchor_box.get_anchors(24, 32)\n",
    "\n",
    "#         # 스코어 필터링\n",
    "#         scores = predictions[..., -1]\n",
    "#         score_mask = tf.greater(scores, self.confidence_threshold)\n",
    "#         filtered_predictions = tf.boolean_mask(predictions, score_mask)\n",
    "#         filtered_anchors = tf.boolean_mask(anchors, tf.reshape(score_mask, [-1]))\n",
    "\n",
    "#         # 바운딩 박스 디코딩\n",
    "#         boxes = self.decode_boxes(filtered_predictions, filtered_anchors)\n",
    "#         scores = tf.boolean_mask(scores, score_mask)\n",
    "\n",
    "#         # 바운딩 박스와 스코어 합치기\n",
    "#         decoded_predictions = tf.concat([boxes, tf.expand_dims(scores, axis=-1)], axis=-1)\n",
    "\n",
    "#         return decoded_predictions\n",
    "\n",
    "#     def decode_boxes(self, predictions, anchors):\n",
    "#         box_variance = [0.1, 0.1, 0.2, 0.2]\n",
    "#         dx = predictions[..., 0]\n",
    "#         dy = predictions[..., 1]\n",
    "#         dw = predictions[..., 2]\n",
    "#         dh = predictions[..., 3]\n",
    "#         anchor_x = anchors[..., 0]\n",
    "#         anchor_y = anchors[..., 1]\n",
    "#         anchor_w = anchors[..., 2]\n",
    "#         anchor_h = anchors[..., 3]\n",
    "\n",
    "#         cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "#         cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "#         width = tf.exp(dw * box_variance[2]) * anchor_w\n",
    "#         height = tf.exp(dh * box_variance[3]) * anchor_h\n",
    "\n",
    "#         x_min = cx - width / 2\n",
    "#         y_min = cy - height / 2\n",
    "#         x_max = x_min + width\n",
    "#         y_max = y_min + height\n",
    "\n",
    "#         boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)\n",
    "#         return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 모델 생성\n",
    "image = tf.keras.Input(shape=[24, 32, 1], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "detections = DecodePredictions(confidence_threshold=0.5)(predictions)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['person']  # 클래스 이름 리스트\n",
    "\n",
    "for img, _, _ in val_dataset.take(1):    \n",
    "    predictions = inference_model.predict(tf.expand_dims(img[0], axis=0))  # 첫 번째 이미지에 대한 예측 결과\n",
    "    print(predictions)\n",
    "    draw_bounding_boxes(img[0].numpy(), predictions, class_names)  # 디코딩된 바운딩 박스를 이미지에 그리기\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "custom_objects = {\n",
    "    'DepthwiseSeparableConv': DepthwiseSeparableConv,\n",
    "    'DepthwiseConv': DepthwiseConv,\n",
    "    'Conv': Conv,\n",
    "    'Bottleneck': Bottleneck,\n",
    "    'CSPDenseLayer': CSPDenseLayer,\n",
    "    'ChannelAttention': ChannelAttention,\n",
    "    'SpatialAttention': SpatialAttention,\n",
    "    'CBAM': CBAM,\n",
    "    'SPPF': SPPF,\n",
    "    'BackBone': BackBone,\n",
    "    'NeckLayer': NeckLayer,\n",
    "    'HeadLayer': HeadLayer,\n",
    "    'CustomModel': CustomModel,\n",
    "    'DecodePredictions': DecodePredictions\n",
    "}\n",
    "\n",
    "export_path = 'ObjectDetection/model_v7.pb'\n",
    "inference_model.save(export_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# custom_objects = {\n",
    "#     'DepthwiseSeparableConv': DepthwiseSeparableConv,\n",
    "#     'DepthwiseConv': DepthwiseConv,\n",
    "#     'Conv': Conv,\n",
    "#     'Bottleneck': Bottleneck,\n",
    "#     'CSPDenseLayer': CSPDenseLayer,\n",
    "#     'ChannelAttention': ChannelAttention,\n",
    "#     'SpatialAttention': SpatialAttention,\n",
    "#     'CBAM': CBAM,\n",
    "#     'SPPF': SPPF,\n",
    "#     'BackBone': BackBone,\n",
    "#     'NeckLayer': NeckLayer,\n",
    "#     'HeadLayer': HeadLayer,\n",
    "#     'CustomModel': CustomModel,\n",
    "#     'DecodePredictions': DecodePredictions\n",
    "# }\n",
    "\n",
    "# export_path = 'ObjectDetection/model_v6.pb'\n",
    "# loaded_model = tf.keras.models.load_model(export_path, custom_objects=custom_objects, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saved_model_dir = 'ObjectDetection/model_v7.pb'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# 입력과 출력의 데이터 타입을 설정\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open('ObjectDetection/tflite/model_v7.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Lite 모델 로드\n",
    "tflite_model_path = 'ObjectDetection/tflite/model_v7.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "class_names = ['person']  # 클래스 이름 리스트\n",
    "\n",
    "def draw_bounding_boxes(image, boxes, class_names):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='gray')  # 이미지가 grayscale인 경우 cmap='gray'를 추가합니다.\n",
    "    ax = plt.gca()\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max, score = box\n",
    "        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # 박스 위에 클래스 이름과 확률 표시\n",
    "        class_name = class_names[0]  # 예시로 'person' 클래스를 사용합니다.\n",
    "        # text = f'{class_name}: {score / 4:.2f}'\n",
    "        text = f'{class_name}'\n",
    "        ax.text(x_min, y_min, text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "for img, _, _ in val_dataset.take(1):\n",
    "    # 입력 이미지 전처리\n",
    "    input_data = tf.expand_dims(img[0], axis=0)\n",
    "    input_data = input_data.numpy()\n",
    "    \n",
    "    # 모델 추론\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # print(predictions)\n",
    "    predictions[:, -1] / 3\n",
    "    \n",
    "    scores = predictions[:, -1] / 3\n",
    "\n",
    "    # scores가 0.5 이상인 인덱스를 찾습니다.\n",
    "    indices = tf.where(scores >= 0.5)\n",
    "\n",
    "    # 해당 인덱스에 해당하는 predictions만 필터링합니다.\n",
    "    filtered_predictions = tf.gather(predictions, indices[:, 0])\n",
    "\n",
    "    # 결과 후처리 및 시각화\n",
    "    draw_bounding_boxes(img[0].numpy(), filtered_predictions, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# import tensorflow as tf\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def iou(box1, box2):\n",
    "#     x1, y1, x2, y2 = box1\n",
    "#     x3, y3, x4, y4 = box2\n",
    "    \n",
    "#     x_overlap = max(0, min(x2, x4) - max(x1, x3))\n",
    "#     y_overlap = max(0, min(y2, y4) - max(y1, y3))\n",
    "    \n",
    "#     intersection = x_overlap * y_overlap\n",
    "#     area1 = (x2 - x1) * (y2 - y1)\n",
    "#     area2 = (x4 - x3) * (y4 - y3)\n",
    "#     union = area1 + area2 - intersection\n",
    "    \n",
    "#     return intersection / union\n",
    "\n",
    "# def decode_predictions(predictions, anchors, box_variance=[0.1, 0.1, 0.2, 0.2], iou_threshold=0.5, score_threshold=0.9, top_n=9000):\n",
    "#     decoded_boxes = []\n",
    "#     scores = []\n",
    "    \n",
    "#     for i, prediction in enumerate(predictions):\n",
    "#         score = prediction[-1]\n",
    "#         if score > score_threshold:\n",
    "#             scores.append((score, i))\n",
    "    \n",
    "#     # 점수에 따라 내림차순 정렬\n",
    "#     scores.sort(reverse=True)\n",
    "    \n",
    "#     # 상위 N개 선택\n",
    "#     scores = scores[:top_n]\n",
    "    \n",
    "#     # NMS 적용\n",
    "#     for score, i in scores:\n",
    "#         prediction = predictions[i]\n",
    "#         dx, dy, dw, dh = prediction[:4]\n",
    "#         anchor = anchors[i]\n",
    "#         anchor_x, anchor_y, anchor_w, anchor_h = anchor\n",
    "        \n",
    "#         cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "#         cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "#         width = np.exp(dw * box_variance[2]) * anchor_w\n",
    "#         height = np.exp(dh * box_variance[3]) * anchor_h\n",
    "        \n",
    "#         x_min = cx - width / 2\n",
    "#         y_min = cy - height / 2\n",
    "#         decoded_box = [x_min, y_min, x_min + width, y_min + height]\n",
    "        \n",
    "#         keep = True\n",
    "#         for other_box in decoded_boxes:\n",
    "#             if iou(decoded_box, other_box) >= iou_threshold:\n",
    "#                 keep = False\n",
    "#                 break\n",
    "        \n",
    "#         if keep:\n",
    "#             decoded_boxes.append(decoded_box)\n",
    "    \n",
    "#     return decoded_boxes\n",
    "\n",
    "# def draw_bounding_boxes(image, boxes):\n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.imshow(image)\n",
    "#     ax = plt.gca()\n",
    "#     for box in boxes:\n",
    "#         x_min, y_min, x_max, y_max = box\n",
    "#         rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # AnchorBox 클래스와 get_anchors 함수가 필요합니다.\n",
    "\n",
    "# anchor_box = AnchorBox()\n",
    "# anchors = anchor_box.get_anchors(24, 32)  # 앵커 박스 생성 예시\n",
    "\n",
    "# for img, _, _ in val_dataset:    \n",
    "#     predictions = model.predict(tf.expand_dims(img[0], axis=0))[0]  # 첫 번째 이미지에 대한 예측 결과\n",
    "#     decoded_boxes = decode_predictions(predictions, anchors)  # 예측된 바운딩 박스 디코딩\n",
    "#     draw_bounding_boxes(img[0].numpy(), decoded_boxes)  # 디코딩된 바운딩 박스를 이미지에 그리기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "    \n",
    "    x_overlap = max(0, min(x2, x4) - max(x1, x3))\n",
    "    y_overlap = max(0, min(y2, y4) - max(y1, y3))\n",
    "    \n",
    "    intersection = x_overlap * y_overlap\n",
    "    area1 = (x2 - x1) * (y2 - y1)\n",
    "    area2 = (x4 - x3) * (y4 - y3)\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def decode_predictions(predictions, anchors, box_variance=[0.1, 0.1, 0.2, 0.2], iou_threshold=0.5, score_threshold=0.9, top_n=9000):\n",
    "    decoded_boxes = []\n",
    "    scores = []\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        score = prediction[-1]\n",
    "        if score > score_threshold:\n",
    "            scores.append((score, i))\n",
    "    \n",
    "    # 점수에 따라 내림차순 정렬\n",
    "    scores.sort(reverse=True)\n",
    "    \n",
    "    # 상위 N개 선택\n",
    "    scores = scores[:top_n]\n",
    "    \n",
    "    # NMS 적용\n",
    "    for score, i in scores:\n",
    "        prediction = predictions[i]\n",
    "        dx, dy, dw, dh = prediction[:4]\n",
    "        anchor = anchors[i]\n",
    "        anchor_x, anchor_y, anchor_w, anchor_h = anchor\n",
    "        \n",
    "        cx = dx * box_variance[0] * anchor_w + anchor_x\n",
    "        cy = dy * box_variance[1] * anchor_h + anchor_y\n",
    "        width = np.exp(dw * box_variance[2]) * anchor_w\n",
    "        height = np.exp(dh * box_variance[3]) * anchor_h\n",
    "        \n",
    "        x_min = cx - width / 2\n",
    "        y_min = cy - height / 2\n",
    "        decoded_box = [x_min, y_min, x_min + width, y_min + height]\n",
    "        \n",
    "        keep = True\n",
    "        for other_box in decoded_boxes:\n",
    "            if iou(decoded_box, other_box) >= iou_threshold:\n",
    "                keep = False\n",
    "                break\n",
    "        \n",
    "        if keep:\n",
    "            decoded_boxes.append(decoded_box)\n",
    "    \n",
    "    return decoded_boxes\n",
    "\n",
    "def draw_bounding_boxes(image, boxes, save_path=None):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# AnchorBox 클래스와 get_anchors 함수가 필요합니다.\n",
    "\n",
    "anchor_box = AnchorBox()\n",
    "anchors = anchor_box.get_anchors(24, 32)  # 앵커 박스 생성 예시\n",
    "\n",
    "for i, (img, _, _) in enumerate(val_dataset):    \n",
    "    predictions = model.predict(tf.expand_dims(img[0], axis=0))[0]  # 첫 번째 이미지에 대한 예측 결과\n",
    "    decoded_boxes = decode_predictions(predictions, anchors)  # 예측된 바운딩 박스 디코딩\n",
    "    save_path = f\"prediction_img/output_{i}.png\"  # 이미지 저장 경로 지정\n",
    "    draw_bounding_boxes(img[0].numpy(), decoded_boxes, save_path=save_path)  # 디코딩된 바운딩 박스를 이미지에 그리고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
