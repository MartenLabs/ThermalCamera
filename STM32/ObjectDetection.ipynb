{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# class Logger(object):\n",
    "#     def __init__(self, filename=\"tensorflow_logs.log\"):\n",
    "#         self.terminal = sys.stdout\n",
    "#         self.log = open(filename, \"a\")\n",
    "\n",
    "#     def write(self, message):\n",
    "#         self.terminal.write(message)\n",
    "#         self.log.write(message)\n",
    "\n",
    "#     def flush(self):\n",
    "#         # 이 메소드는 flush() 인터페이스를 유지하기 위한 것입니다.\n",
    "#         pass\n",
    "\n",
    "# sys.stdout = Logger(\"tensorflow_logs.log\")\n",
    "\n",
    "# # 테스트 출력\n",
    "# print(\"This will be written to both console and file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "datasets = np.load('npz/ObjectDetection.npz', allow_pickle=True)\n",
    "images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']\n",
    "\n",
    "max_label_length = 4\n",
    "labels = []\n",
    "for num in numbers:\n",
    "    cls = [1] * num if num != 0 else [0]\n",
    "    cls += [0] * (max_label_length - len(cls))\n",
    "    labels.append(cls)\n",
    "\n",
    "# labels = np.array(labels)\n",
    "expected_image_shape = images[9000].shape\n",
    "expected_bbox_shape = bboxes[9000].shape\n",
    "expected_label_length = bboxes[9000].shape\n",
    "\n",
    "non_zero_indices = np.where(numbers > 2)[0]\n",
    "\n",
    "images_filtered = images[non_zero_indices]\n",
    "bboxes_filtered = bboxes[non_zero_indices]\n",
    "labels_filtered = np.array(labels)[non_zero_indices]\n",
    "numbers_filtered = numbers[non_zero_indices]\n",
    "# labels = np.array(labels)\n",
    "\n",
    "\n",
    "for i in range(len(images_filtered)):\n",
    "    image = images_filtered[i]\n",
    "    bbox = bboxes_filtered[i]\n",
    "    label = labels_filtered[i]\n",
    "\n",
    "    # 이미지 형태와 데이터 타입 검증\n",
    "    if image.shape != expected_image_shape or image.dtype != np.uint8:\n",
    "        print(f\"이미지 {i}의 형태 또는 데이터 타입이 잘못되었습니다: 형태={image.shape}, 데이터 타입={image.dtype}\")\n",
    "\n",
    "    # 바운딩 박스 형태와 데이터 타입 검증\n",
    "    if bbox.shape != expected_bbox_shape or bbox.dtype != np.float64:\n",
    "        print(f\"바운딩 박스 {i}의 형태 또는 데이터 타입이 잘못되었습니다: 형태={bbox.shape}, 데이터 타입={bbox.dtype}\")\n",
    "\n",
    "    # # 레이블 길이와 데이터 타입 검증\n",
    "    # if len(label) != expected_label_length or not all(isinstance(x, int) for x in label):\n",
    "    #     print(f\"레이블 {i}의 길이 또는 데이터 타입이 잘못되었습니다: 길이={len(label)}, 레이블={label}\")\n",
    "\n",
    "# print(images.shape, numbers.shape, bboxes.shape, len(labels))\n",
    "\n",
    "# print(images.max(), images.min())\n",
    "# print(bboxes[9000:9010])\n",
    "# print(labels[9000:9010])\n",
    "\n",
    "\n",
    "dataset = {\n",
    "    'images' : images_filtered,\n",
    "    'bboxes' : bboxes_filtered,\n",
    "    'cls' : labels_filtered\n",
    "}\n",
    "\n",
    "print(dataset['images'].shape)\n",
    "print(dataset['bboxes'].shape)\n",
    "print(dataset['cls'])\n",
    "non_zero_indices = np.where(numbers_filtered == 0)[0]\n",
    "print(\"non_zero_indices: \", non_zero_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels[9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "images = dataset['images']\n",
    "# numbers =dataset['numbers']\n",
    "bboxes = dataset['bboxes']\n",
    "cls = dataset['cls']\n",
    "\n",
    "boxes = bboxes[0]\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.axis('off')\n",
    "image = images[0]\n",
    "print(image.shape)\n",
    "print(image.shape[0])\n",
    "print(image.shape[0])\n",
    "plt.imshow(images[0])\n",
    "ax = plt.gca()\n",
    "boxes = tf.stack([\n",
    "\tboxes[:, 0] * images.shape[2],\n",
    "\tboxes[:, 1] * images.shape[1],\n",
    "\tboxes[:, 2] * images.shape[2],\n",
    "\tboxes[:, 3] * images.shape[1]], axis = -1\n",
    ")\n",
    "\n",
    "print(boxes)\n",
    "\n",
    "for box in boxes:\n",
    "\txmin, ymin = box[:2]\n",
    "\tw, h = box[2:] - box[:2]\n",
    "\tpatch = plt.Rectangle(\n",
    "\t\t[xmin, ymin], w, h, fill = False, edgecolor = [1, 0, 0], linewidth = 2\n",
    "\t)\n",
    "\tax.add_patch(patch)\n",
    "plt.show()\n",
    "print(cls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE_WIDTH = images.shape[2]\n",
    "IMG_SIZE_HEIGHT = images.shape[1]\n",
    "N_DATA = images.shape[0]\n",
    "N_TRAIN = int(images.shape[0] * 0.7)\n",
    "N_VAL = images.shape[0] - N_TRAIN\n",
    "LOG_DIR = 'ObjectDetectionLog'\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'tfrecord/ObjectDetection')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "print(\"IMG_SIZE_WIDTH:  \", IMG_SIZE_WIDTH)\n",
    "print(\"IMG_SIZE_HEIGHT: \", IMG_SIZE_HEIGHT)\n",
    "print(\"N_DATA:          \", N_DATA)\n",
    "print(\"N_TRAIN:         \", N_TRAIN)\n",
    "\n",
    "\n",
    "\n",
    "shuffle_list = list(range(N_DATA))\n",
    "random.shuffle(shuffle_list)\n",
    "\n",
    "train_idx_list = shuffle_list[:N_TRAIN]\n",
    "val_idx_list = shuffle_list[N_TRAIN:]\n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'tfrecord/ObjectDetection')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "writer_train = tf.io.TFRecordWriter(tfr_train_dir)\n",
    "writer_val = tf.io.TFRecordWriter(tfr_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.max(), images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list = tf.train.FloatList(value = value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list = tf.train.Int64List(value = [value]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in train_idx_list:\n",
    "    bbox = bboxes[idx]\n",
    "    xmin, ymin, xmax, ymax = bbox[:, 0], bbox[:, 1], bbox[:, 2], bbox[:, 3]\n",
    "    bbox = np.stack([xmin, ymin, xmax, ymax], axis=-1).flatten()\n",
    "\n",
    "    image = images[idx]\n",
    "    bimage = image.tobytes()\n",
    "\n",
    "    number = numbers[idx]\n",
    "    class_id = cls[idx]\n",
    "    # print(len(cls))\n",
    "    serialized_cls = tf.io.serialize_tensor(tf.constant(class_id, dtype=tf.int32)).numpy()\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image': _bytes_feature(bimage),\n",
    "        'bbox': _float_feature(bbox),\n",
    "        'label': _bytes_feature(serialized_cls),\n",
    "        # 'number': _int64_feature(number)\n",
    "    }))\n",
    "    \n",
    "    writer_train.write(example.SerializeToString())\n",
    "writer_train.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "RES_HEIGHT =24\n",
    "RES_WIDTH = 32\n",
    "# N_EPOCHS = 100\n",
    "# N_BATCH = 8\n",
    "# LR = 0.0005\n",
    "\n",
    "\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    features = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'bbox': tf.io.VarLenFeature(tf.float32),  \n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'number': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "\n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, [RES_HEIGHT, RES_WIDTH, 1])\n",
    "    image = image / tf.reduce_max(image)\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    # image = image / tf.reduce_max(image)\n",
    "\n",
    "    bbox = tf.sparse.to_dense(parsed_features['bbox']) \n",
    "    bbox = tf.cast(bbox, tf.float32)\n",
    "    # num_boxes = tf.shape(bbox)[0] // 4\n",
    "    bbox = tf.reshape(bbox, [-1, 4])\n",
    "\n",
    "    serialized_cls = parsed_features['label']\n",
    "    label = tf.io.parse_tensor(serialized_cls, out_type=tf.int32)\n",
    "    \n",
    "    # number = tf.cast(parsed_features['number'], tf.int64)\n",
    "    return image, bbox, label\n",
    "\n",
    "# cur_dir = os.getcwd()\n",
    "# tfr_dir = os.path.join(cur_dir, 'tfrecord/ObjectDetection')\n",
    "# tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(AUTOTUNE).batch(N_BATCH, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
    "val_dataset = val_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    image = image.numpy()\n",
    "    print(image.shape)\n",
    "    # print(label)\n",
    "    # # plt.axis('off')\n",
    "    # plt.imshow(image)\n",
    "    # ax = plt.gca()  \n",
    "    # print(bbox)\n",
    "\n",
    "    # boxes = tf.stack(\n",
    "    # \t[\n",
    "    # \t bbox[:,0] * image.shape[1],\n",
    "    # \t bbox[:,1] * image.shape[0],\n",
    "    # \t bbox[:,2] * image.shape[1],\n",
    "    # \t bbox[:,3] * image.shape[0]\n",
    "    # \t], axis = -1\n",
    "    # )\n",
    "    # print(image.shape)\n",
    "    # print(boxes)\n",
    "    # for box in boxes:\n",
    "    #     xmin, ymin = box[:2]\n",
    "    #     w, h = box[2:] - box[:2]\n",
    "    #     patch = plt.Rectangle(\n",
    "    #         [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "    #     )\n",
    "    #     ax.add_patch(patch)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):    \n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_corners(boxes):\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    print(bbox)\n",
    "    print(convert_to_xywh(bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_image(image, min_side=96, max_side=128):\n",
    "    \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype = tf.float32)\n",
    "    # print(f\"image_shape: {image_shape}\")\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    # print(f\"ratio: {ratio}\")\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "      ratio = max_side / tf.reduce_max(image_shape)\n",
    "    # print(f\"ratio: {ratio}\")\n",
    "\n",
    "    new_image_shape = ratio * image_shape\n",
    "    # print(f\"new_image_shape: {new_image_shape}\")\n",
    "\n",
    "    image = tf.image.resize(image, \n",
    "                            tf.cast(new_image_shape, dtype=tf.int32), \n",
    "                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # print(f\"image: {image.shape}\")\n",
    "\n",
    "    return image, new_image_shape, ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image, gt_boxes, cls_ids):\n",
    "    # image = sample[\"images\"]\n",
    "    # bbox = sample[\"bboxes\"]\n",
    "    cls_ids = tf.cast(cls_ids, dtype = tf.int32)\n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "    \n",
    "    bbox = tf.stack([\n",
    "        gt_boxes[:, 0] * image_shape[1],\n",
    "        gt_boxes[:, 1] * image_shape[0],\n",
    "        gt_boxes[:, 2] * image_shape[1],\n",
    "        gt_boxes[:, 3] * image_shape[0]],\n",
    "        axis = -1\n",
    "    )\n",
    "    \n",
    "    bbox = convert_to_xywh(bbox)\n",
    "    print(image.shape)\n",
    "    print(bbox.shape)\n",
    "    print(cls_ids.shape)\n",
    "    return image, bbox, cls_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    resize_and_pad_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    print(label)\n",
    "    img, bbox, class_id = preprocess_data(image, bbox, label)\n",
    "    print(img.shape) # (72, 96, 1)\n",
    "\n",
    "    anchor_img = np.zeros((*img.shape[:2], 3), dtype=np.uint8)\n",
    "    print(anchor_img.shape)\n",
    "\n",
    "    print(bbox)\n",
    "\n",
    "    strides = [4, 8, 16, 32]\n",
    "    colors = {\n",
    "        4: [0, 255, 0],  # 초록색\n",
    "        8: [0, 0, 255],  # 파란색\n",
    "        16: [255, 0, 0],   # 빨간색\n",
    "        32:[255, 255, 255],  # 노란색\n",
    "        # 64:[255, 255, 0],  # 노란색\n",
    "    }\n",
    "\n",
    "    for stride in strides:\n",
    "        color = colors[stride]\n",
    "        for y in range(0, anchor_img.shape[0], stride):\n",
    "            for x in range(0, anchor_img.shape[1], stride):\n",
    "                anchor_img[y, x, :] = color\n",
    "\n",
    "    # 이미지 표시\n",
    "    plt.imshow(img, alpha=1)  \n",
    "    plt.imshow(anchor_img, alpha=0.5) \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    img, box, label = preprocess_data(image, bbox, label)\n",
    "    print(label)\n",
    "    print(box)\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))\n",
    "    # 이미지 시각화\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    print(\"width: \", width)\n",
    "    print(\"height: \", height)\n",
    "    boxes = tf.stack(\n",
    "        [\n",
    "            (box[:, 0] - 0.5 * box[:, 2])  ,  # xmin = x_center - width/2\n",
    "            (box[:, 1] - 0.5 * box[:, 3])  ,  # ymin = y_center - height/2\n",
    "            (box[:, 0] + 0.5 * box[:, 2])  ,  # xmax = x_center + width/2\n",
    "            (box[:, 1] + 0.5 * box[:, 3])     # ymax = y_center + height/2\n",
    "        ], axis=-1\n",
    "    )\n",
    "    print(\"bbox: \", boxes)\n",
    "    # 각 바운딩 박스에 대해 반복하여 그리기\n",
    "    for box in boxes[2:3]:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        # print(\"xmin, ymin: \", xmin, ymin)\n",
    "        print(box)\n",
    "        w, h = xmax - xmin, ymax - ymin\n",
    "        patch = plt.Rectangle(\n",
    "            [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    plt.show()\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((7, 24192, 1) vs (None, 24624, 1)).\n",
    "\n",
    "class AnchorBox:\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.5, 0.75, 1.0, 1.25]        \n",
    "        self.scales = [2** x for x in [1/3, 2/3]]\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(2, 8)]\n",
    "        self._areas = [x ** 2 for x in [8.0, 16.0, 32.0, 64.0, 128.0, 256.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        anchor_dims_all = []\n",
    "\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios: \n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis = -1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales: \n",
    "                    anchor_dims.append(scale * dims) \n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis = -2))\n",
    "        return anchor_dims_all \n",
    "    \n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        rx = tf.range(feature_width, dtype = tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype = tf.float32) + 0.5\n",
    "\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis = -1) * self._strides[level - 2]\n",
    "        # print(f\"centers: {centers}\")\n",
    "        centers = tf.expand_dims(centers, axis = -2)\n",
    "        # print(f\"centers: {centers}\")\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        # print(f\"centers: {centers}\")\n",
    "\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 2], [feature_height, feature_width, 1, 1] \n",
    "        )\n",
    "        # print(f\"dims: {dims}\")\n",
    "\n",
    "        \n",
    "        anchors = tf.concat([centers, dims], axis=-1) \n",
    "        # print(f\"anchors: {anchors}\")\n",
    "\n",
    "        # print(f\"{tf.reshape(anchors, [feature_height * feature_width * self._num_anchors, 4]).shape}\")\n",
    "\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i), # 올림\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i\n",
    "            )\n",
    "            for i in range(2, 8)\n",
    "        ]\n",
    "\n",
    "        anchors = tf.concat(anchors, axis=0)\n",
    "\n",
    "        # 앵커 박스의 좌표를 이미지 크기 내로 제한\n",
    "        anchors = tf.clip_by_value(anchors, 0, [image_height, image_width, image_height, image_width])\n",
    "        return tf.concat(anchors, axis=0)\n",
    "        # return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = AnchorBox()\n",
    "anchor = anchors.get_anchors(96, 128)\n",
    "\n",
    "has_negative_values = tf.reduce_any(tf.less(anchor, 0))\n",
    "print(\"Anchor 음수 값:\", has_negative_values.numpy())\n",
    "\n",
    "# print(anchor)\n",
    "print(anchor.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bounding_boxes(data, num_samples):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    # print(img.shape)\n",
    "    data_np = data.numpy()\n",
    "\n",
    "    if len(data) > num_samples:\n",
    "        sampled_indices = np.random.choice(len(data), num_samples, replace=False)\n",
    "        sample_data = data_np[sampled_indices]\n",
    "    else : \n",
    "        sample_data = data_np\n",
    "    print(sample_data)\n",
    "    for center_x, center_y, width, height in sample_data:\n",
    "        top_left_x = center_x - width / 2\n",
    "        top_left_y = center_y - height / 2\n",
    "\n",
    "        rect = patches.Rectangle((top_left_x, top_left_y), width, height, linewidth=0.8, edgecolor='white', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "draw_bounding_boxes(anchor, 1)\n",
    "\n",
    "\n",
    "# 24192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    print(\"boxes1: \", boxes1)\n",
    "    print(\"boxes1_corners: \", boxes1_corners)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    print(\"boxes2: \", boxes2)\n",
    "    print(\"boxes2_corners: \", boxes2_corners)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    print(\"lu: \", lu)\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])  \n",
    "    print(\"rd: \", rd)\n",
    "\n",
    "    \n",
    "\n",
    "    intersection = tf.maximum(rd - lu, 0.0)\n",
    "    print(\"intersection: \", intersection)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    print(\"intersection_area: \", intersection_area)\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8)\n",
    "    print(\"union_area: \", union_area)\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[62. 54. 78. 70.]], shape=(1, 4), dtype=float64)\n",
      "tf.Tensor([[60. 52. 92. 92.]], shape=(1, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "b1 = np.array([[70, 62, 16, 16]]) \n",
    "b1c = convert_to_corners(b1)\n",
    "print(b1c)\n",
    "\n",
    "b2 = np.array([[76, 72, 32, 40]])\n",
    "b2c = convert_to_corners(b2)\n",
    "print(b2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# # 주어진 바운딩 박스 데이터\n",
    "# box1 = [62, 54, 78, 70]  # [x_min, y_min, x_max, y_max]\n",
    "# box2 = [60, 52, 82, 82]\n",
    "\n",
    "# # 그림 생성\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # 첫 번째 바운딩 박스 추가\n",
    "# rect1 = patches.Rectangle((box1[0], box1[1]), box1[2] - box1[0], box1[3] - box1[1], \n",
    "#                           linewidth=2, edgecolor='blue', facecolor='none')\n",
    "# ax.add_patch(rect1)\n",
    "\n",
    "# # 두 번째 바운딩 박스 추가\n",
    "# rect2 = patches.Rectangle((box2[0], box2[1]), box2[2] - box2[0], box2[3] - box2[1], \n",
    "#                           linewidth=2, edgecolor='red', facecolor='none')\n",
    "# ax.add_patch(rect2)\n",
    "\n",
    "# # 축 범위 설정\n",
    "# ax.set_xlim(0, 90)\n",
    "# ax.set_ylim(0, 90)\n",
    "\n",
    "# # 그림 표시\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA = np.array([[70, 62, 16, 16]])\n",
    "# GT = np.array([[76, 72, 32, 40]])\n",
    "\n",
    "# print(compute_iou(GA, GT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    \n",
    "    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou = 0.4, ignore_iou = 0.3):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        # print(\"iou_matrix:  \", iou_matrix)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        # print(\"max_iou:  \", max_iou)\n",
    "\n",
    "\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis = 1)\n",
    "        # print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "\n",
    "    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        # print(\"positive_mask:  \", positive_mask)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        # print(\"negative_mask:  \", negative_mask)\n",
    "\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        # print(\"ignore_mask:  \", ignore_mask)\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype = tf.float32),\n",
    "            tf.cast(ignore_mask, dtype = tf.float32),\n",
    "        )\n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:])\n",
    "            ],\n",
    "            axis = -1,\n",
    "        )\n",
    "        # print(\"box_target:  \", box_target)\n",
    "        box_target = box_target / self._box_variance\n",
    "        # print(\"box_target:  \", box_target)\n",
    "        return box_target\n",
    "    \n",
    "\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        # print(\"anchor_boxes  : \", anchor_boxes)\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        # print(\"cls_ids\", cls_ids)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        # print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "        # print(\"positive_mask:  \", positive_mask)\n",
    "        # print(\"ignore_mask:  \", ignore_mask)\n",
    "\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "\n",
    "        # print(\"matched_gt_boxes:  \", matched_gt_boxes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        # print(\"box_target:  \", box_target)\n",
    "\n",
    "\n",
    "\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        # print(\"matched_gt_cls_ids:  \", matched_gt_cls_ids)\n",
    "        \n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
    "        )\n",
    "        # print(\"cls_target:  \", cls_target)\n",
    "\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        # print(\"cls_target:  \", cls_target)\n",
    "\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        # print(\"cls_target:  \", cls_target)\n",
    "\n",
    "\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        # print(\"label:  \", label)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):       \n",
    "        images_shape = tf.shape(batch_images)\n",
    "        # print(\"images_shape:  \", images_shape)\n",
    "        batch_size = images_shape[0]\n",
    "        # print(\"batch_size:  \", batch_size)\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        # print(\"labels:  \", labels)\n",
    "        # batch_size_val = batch_size.numpy()\n",
    "        for i in range(1):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            # print(\"label:  \", label)\n",
    "            labels = labels.write(i, label)\n",
    "        return batch_images, labels.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Eager execution: \", tf.executing_eagerly())\n",
    "if not tf.executing_eagerly():\n",
    "    tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for image, bbox, label in train_dataset.take(1):\n",
    "#     img, box, label = preprocess_data(image, bbox, label)\n",
    "#     img = np.expand_dims(img, axis = 0)\n",
    "#     box = np.expand_dims(box, axis = 0)\n",
    "#     label = np.expand_dims(label, axis = 0)\n",
    "\n",
    "#     print(img.shape, box.shape, label.shape)\n",
    "#     label_encoder.encode_batch(img, box, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_axis(img, box, label):\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    box = np.expand_dims(box, axis = 0)\n",
    "    label = np.expand_dims(label, axis = 0)\n",
    "    return img, box, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 1)\n",
      "(None, 4)\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "autotune = tf.data.AUTOTUNE\n",
    "num_classes = 1\n",
    "batch_size = 7\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# train_dataset = train_dataset.shuffle(batch_size * 8)\n",
    "\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "    batch_size=batch_size, \n",
    "    padded_shapes = ([96, 128, 1], [4, 4], [4]),\n",
    "    padding_values=(0.0, 1e-8, -2), \n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "\n",
    "# val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# val_dataset = val_dataset.padded_batch(\n",
    "#     batch_size=batch_size, \n",
    "#     padded_shapes = ([96, 128, 1], [4, 4], [4]),\n",
    "#     padding_values=(0.0, 1e-8, 0), \n",
    "#     drop_remainder=True\n",
    "# )\n",
    "# val_dataset = val_dataset.map(\n",
    "#     label_encoder.encode_batch, num_parallel_calls=autotune\n",
    "# )\n",
    "\n",
    "# val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes1:  Tensor(\"concat_7/concat:0\", shape=(8200, 4), dtype=float32)\n",
      "boxes1_corners:  Tensor(\"concat_8:0\", shape=(8200, 4), dtype=float32)\n",
      "boxes2:  Tensor(\"strided_slice_1:0\", shape=(4, 4), dtype=float32)\n",
      "boxes2_corners:  Tensor(\"concat_9:0\", shape=(4, 4), dtype=float32)\n",
      "lu:  Tensor(\"Maximum:0\", shape=(8200, 4, 2), dtype=float32)\n",
      "rd:  Tensor(\"Minimum:0\", shape=(8200, 4, 2), dtype=float32)\n",
      "intersection:  Tensor(\"Maximum_1:0\", shape=(8200, 4, 2), dtype=float32)\n",
      "intersection_area:  Tensor(\"mul_18:0\", shape=(8200, 4), dtype=float32)\n",
      "union_area:  Tensor(\"Maximum_2:0\", shape=(8200, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
    ")\n",
    "# train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 8200, 5)\n",
      "Positive 개수: 52\n",
      "Negative 개수: 8048\n",
      "Ignore 개수: 100\n",
      "(7, 8200, 5)\n",
      "Positive 개수: 137\n",
      "Negative 개수: 7866\n",
      "Ignore 개수: 197\n",
      "(7, 8200, 5)\n",
      "Positive 개수: 80\n",
      "Negative 개수: 8023\n",
      "Ignore 개수: 97\n",
      "(7, 8200, 5)\n",
      "Positive 개수: 66\n",
      "Negative 개수: 8003\n",
      "Ignore 개수: 131\n",
      "(7, 8200, 5)\n",
      "Positive 개수: 94\n",
      "Negative 개수: 7997\n",
      "Ignore 개수: 109\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "ignore_count = []\n",
    "for batch in train_dataset.take(5):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    print(labels.shape)\n",
    "\n",
    "    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 1.0), tf.int32))\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "#     print(\"원본 레이블 shape:\", labels.shape)\n",
    "\n",
    "#     # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "#     mask = tf.not_equal(labels[:, :, 4], -2.0)\n",
    "#     filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "#     # 필터링된 레이블을 원하는 크기로 조절\n",
    "#     # 예: 36288개로 조절\n",
    "#     desired_count = 36288\n",
    "#     filtered_labels = filtered_labels[:desired_count, :]\n",
    "#     print(\"조정된 레이블 shape:\", filtered_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def cut_labels(images, labels):\n",
    "# #     # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "# #     mask = tf.not_equal(labels[:, 4], -2.0)\n",
    "# #     filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "# #     # 필터링된 레이블을 원하는 크기로 조절\n",
    "# #     desired_count = 36288\n",
    "# #     # 주의: 여기서 filtered_labels의 크기가 desired_count보다 작을 수 있으므로, 적절한 처리가 필요합니다.\n",
    "# #     if tf.shape(filtered_labels)[0] > desired_count:\n",
    "# #         filtered_labels = filtered_labels[:desired_count, :]\n",
    "\n",
    "# #     return images, filtered_labels\n",
    "\n",
    "\n",
    "def cut_labels(images, labels):\n",
    "    # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "    mask = tf.not_equal(labels[:, 4], -2.0)\n",
    "    filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "    # 필터링된 레이블을 원하는 크기로 조절\n",
    "    desired_count = 8064\n",
    "    current_count = tf.shape(filtered_labels)[0]\n",
    "\n",
    "    # 필요한 경우 레이블을 패딩\n",
    "    if current_count < desired_count:\n",
    "        # 누락된 개수만큼 ignore 상태(-2.0)로 패딩\n",
    "        padding_count = desired_count - current_count\n",
    "        padding = tf.fill([padding_count, 5], -2.0)  # -2.0으로 채워진 텐서 생성\n",
    "        filtered_labels = tf.concat([filtered_labels, padding], axis=0)\n",
    "    else:\n",
    "        filtered_labels = filtered_labels[:desired_count, :]\n",
    "\n",
    "    return images, filtered_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # 원본 train_dataset에 process_labels 함수 적용\n",
    "train_dataset = train_dataset.unbatch().map(cut_labels).batch(batch_size)\n",
    "# # # val_dataset = val_dataset.unbatch().map(cut_labels).batch(batch_size)\n",
    "\n",
    "# # # buffer_size = 1000\n",
    "# # # 필요한 경우 배치 크기, 셔플, 반복 등을 적용\n",
    "# # # train_dataset = new_train_dataset.batch(batch_size).shuffle(buffer_size).repeat()\n",
    "# # # val_dataset = new_val_dataset.batch(batch_size).shuffle(buffer_size).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(106.63389, shape=(), dtype=float32) tf.Tensor(-68.77227, shape=(), dtype=float32)\n",
      "Positive 개수: 52\n",
      "Negative 개수: 8012\n",
      "Ignore 개수: 0\n",
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(33.67386, shape=(), dtype=float32) tf.Tensor(-103.82774, shape=(), dtype=float32)\n",
      "Positive 개수: 137\n",
      "Negative 개수: 7866\n",
      "Ignore 개수: 61\n",
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(129.08313, shape=(), dtype=float32) tf.Tensor(-84.30149, shape=(), dtype=float32)\n",
      "Positive 개수: 80\n",
      "Negative 개수: 7984\n",
      "Ignore 개수: 0\n",
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(115.05237, shape=(), dtype=float32) tf.Tensor(-84.30149, shape=(), dtype=float32)\n",
      "Positive 개수: 66\n",
      "Negative 개수: 7998\n",
      "Ignore 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "ignore_count = []\n",
    "for batch in train_dataset.take(4):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    labels[:, :, 4:]\n",
    "    print(tf.reduce_max(images), tf.reduce_min(images))\n",
    "    print(tf.reduce_max(labels), tf.reduce_min(labels))\n",
    "\n",
    "    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 1.0), tf.int32))\n",
    "    \n",
    "    # pad_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 2.0), tf.int32))\n",
    "\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n",
    "    # print(\"Pad 개수:\", pad_count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# # train_dataset에서 하나의 배치를 가져옵니다\n",
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "    \n",
    "#     # 첫 번째 이미지를 선택\n",
    "#     image = images[0]\n",
    "#     image_height, image_width, _ = image.shape\n",
    "\n",
    "#     # 첫 번째 이미지에 대한 레이블\n",
    "#     image_labels = labels[0]\n",
    "\n",
    "#     # 첫 번째 Positive 바운딩 박스 추출\n",
    "#     positive_boxes = image_labels[tf.equal(image_labels[:, 4], 1.0)]\n",
    "#     if tf.shape(positive_boxes)[0] > 0:  # Positive 바운딩 박스가 있는 경우에만\n",
    "#         first_box = positive_boxes[15]  # 첫 번째 바운딩 박스\n",
    "        \n",
    "#         boxes = tf.stack(\n",
    "#         [\n",
    "#             (first_box[0] - 0.5 * first_box[2])  ,  # xmin = x_center - width/2\n",
    "#             (first_box[1] - 0.5 * first_box[3])  ,  # ymin = y_center - height/2\n",
    "#             (first_box[0] + 0.5 * first_box[2])  ,  # xmax = x_center + width/2\n",
    "#             (first_box[1] + 0.5 * first_box[3])     # ymax = y_center + height/2\n",
    "#         ], axis=-1)\n",
    "\n",
    "\n",
    "#         x_center, y_center, width, height = boxes[:4]\n",
    "#         # 상대 좌표를 절대 픽셀 값으로 변환\n",
    "#         x_min = (x_center - width / 2) \n",
    "#         y_min = (y_center - height / 2) \n",
    "\n",
    "#         print(x_min)\n",
    "#         print(y_min)\n",
    "#         print(width)\n",
    "#         print(height)\n",
    "\n",
    "#         # 이미지 표시\n",
    "#         fig, ax = plt.subplots(1)\n",
    "#         ax.imshow(image.numpy())\n",
    "\n",
    "#         # 첫 번째 바운딩 박스 그리기\n",
    "#         rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(\"Positive 바운딩 박스가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "    \n",
    "#     # 첫 번째 이미지를 선택\n",
    "#     image = images[0]\n",
    "#     image_height, image_width, _ = image.shape\n",
    "\n",
    "#     # 첫 번째 이미지에 대한 레이블\n",
    "#     image_labels = labels[0]\n",
    "\n",
    "#     # Positive 바운딩 박스 추출\n",
    "#     positive_boxes = image_labels[tf.equal(image_labels[:, 4], 1.0)]\n",
    "\n",
    "#     # 이미지 표시\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image.numpy())\n",
    "\n",
    "#     # Positive 바운딩 박스를 이미지에 그림\n",
    "#     for box in positive_boxes:\n",
    "#         x_center, y_center, width, height = box[:4]\n",
    "#         # 상대 좌표를 절대 픽셀 값으로 변환\n",
    "#         x_min = (x_center - width / 2) * image_width\n",
    "#         y_min = (y_center - height / 2) * image_height\n",
    "#         width *= image_width\n",
    "#         height *= image_height\n",
    "\n",
    "#         # 바운딩 박스 그리기\n",
    "#         rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Conv2D, Conv2DTranspose, Flatten, Activation\n",
    "from keras.layers import Input, GlobalAveragePooling2D, Conv2D, ReLU, Reshape, Multiply\n",
    "from keras.layers import BatchNormalization, Dropout, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.layers import ZeroPadding2D, DepthwiseConv2D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Add\n",
    "\n",
    "class BackBone(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BackBone, self).__init__()\n",
    "        # self.l2_regularizer = l2(0.001)\n",
    "\n",
    "    def squeeze_excite_block(self, input_tensor, ratio=8):\n",
    "        \"\"\" Squeeze and Excitation block \"\"\"\n",
    "        # 채널 수 얻기\n",
    "        channels = input_tensor.shape[-1]\n",
    "\n",
    "        # Squeeze 단계: Global Average Pooling\n",
    "        x = GlobalAveragePooling2D()(input_tensor)\n",
    "        x = Reshape((1, 1, channels))(x)\n",
    "\n",
    "        # Excite 단계: 첫 번째 Conv2D 레이어 (채널 수 축소)\n",
    "        x = Conv2D(channels // ratio, (1, 1), padding='same')(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        # 두 번째 Conv2D 레이어 (채널 수 원래대로 확장)\n",
    "        x = Conv2D(channels, (1, 1), padding='same')(x)\n",
    "        # Sigmoid 활성화 함수 적용\n",
    "        x = tf.keras.activations.sigmoid(x)\n",
    "        # 원본 입력과 곱하기\n",
    "        out = Multiply()([input_tensor, x])\n",
    "        return out\n",
    "\n",
    "    def residual_layer(self, feature_map, latent, name:str):\n",
    "        add_layer = Add(name = name+'_output')([feature_map, latent])\n",
    "        return add_layer\n",
    "\n",
    "    def feature_extraction_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        feature_map = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_1')(feature_map)\n",
    "        feature_map = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        feature_map = Activation('relu')(feature_map)\n",
    "        feature_map = DepthwiseConv2D(kernel_size=3, padding = 'same')(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        # upsample_layer = Conv2DTranspose(filters = 6, kernel_size = 3, strides = (3, 3), padding = 'same')(inputs_image)\n",
    "        # feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        feature_map = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_3')(feature_map)\n",
    "        feature_map = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 2, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_4')(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        feature_map = Activation('relu')(feature_map)\n",
    "        feature_map = self.squeeze_excite_block(feature_map)\n",
    "        return feature_map\n",
    "\n",
    "    def convolutional_residual_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        latent = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_1')(feature_map)\n",
    "        latent = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(feature_map)\n",
    "        latent = BatchNormalization()(latent)\n",
    "        latent = Activation('relu')(latent)\n",
    "        latent = DepthwiseConv2D(kernel_size=3, padding = 'same')(latent)\n",
    "        # feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        latent = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_3')(latent)\n",
    "        latent = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        # kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_4')(latent)\n",
    "        latent = BatchNormalization()(latent)\n",
    "        latent = Activation('relu')(latent)\n",
    "        latent = DepthwiseConv2D(kernel_size=3, padding = 'same')(latent)\n",
    "        residual_block = self.residual_layer(feature_map, latent, name)\n",
    "        return residual_block\n",
    "    \n",
    "    def __call__(self, input_shape=(96, 128, 1)):\n",
    "        inputs_image = Input(shape=input_shape)\n",
    "        # inputs_image = ZeroPadding2D(((2, 2),(0, 0)))(inputs_image)\n",
    "        block_1 = self.feature_extraction_block(inputs_image, 16, 32, 'block_1')\n",
    "        block_1_output = self.convolutional_residual_block(block_1, 16, 32,'block_2')\n",
    "        block_2 = self.feature_extraction_block(block_1_output, 16, 32, 'block_3')\n",
    "        block_2_output = self.convolutional_residual_block(block_2, 16, 32, 'block_4')\n",
    "        block_3 = self.feature_extraction_block(block_2_output, 16, 32,'block_5')\n",
    "        block_3_output = self.convolutional_residual_block(block_3, 16, 32,'block_6')\n",
    "        block_4 = self.feature_extraction_block(block_3_output, 16, 32,'block_7')\n",
    "        block_4_output = self.convolutional_residual_block(block_4, 16, 32,'block_8')\n",
    "\n",
    "        model = Model(inputs_image, block_4_output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 96, 128, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " block_1_1 (Conv2D)          (None, 96, 128, 16)          160       ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " block_1_2 (Conv2D)          (None, 96, 128, 16)          2320      ['block_1_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 96, 128, 16)          64        ['block_1_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_32 (Activation)  (None, 96, 128, 16)          0         ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_24 (Depth  (None, 96, 128, 16)          160       ['activation_32[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 96, 128, 16)          64        ['depthwise_conv2d_24[0][0]'] \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " block_1_3 (Conv2D)          (None, 96, 128, 32)          4640      ['batch_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_1_4 (Conv2D)          (None, 48, 64, 32)           9248      ['block_1_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 48, 64, 32)           128       ['block_1_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_33 (Activation)  (None, 48, 64, 32)           0         ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_8  (None, 32)                   0         ['activation_33[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)         (None, 1, 1, 32)             0         ['global_average_pooling2d_8[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 1, 1, 4)              132       ['reshape_8[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 1, 1, 4)              0         ['conv2d_32[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 1, 1, 32)             160       ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_8 (TFOpLam  (None, 1, 1, 32)             0         ['conv2d_33[0][0]']           \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " multiply_8 (Multiply)       (None, 48, 64, 32)           0         ['activation_33[0][0]',       \n",
      "                                                                     'tf.math.sigmoid_8[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_2 (Conv2D)          (None, 48, 64, 16)           4624      ['multiply_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 48, 64, 16)           64        ['block_2_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_34 (Activation)  (None, 48, 64, 16)           0         ['batch_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_25 (Depth  (None, 48, 64, 16)           160       ['activation_34[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_3 (Conv2D)          (None, 48, 64, 32)           4640      ['depthwise_conv2d_25[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_4 (Conv2D)          (None, 48, 64, 32)           9248      ['block_2_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 48, 64, 32)           128       ['block_2_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_35 (Activation)  (None, 48, 64, 32)           0         ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_26 (Depth  (None, 48, 64, 32)           320       ['activation_35[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_output (Add)        (None, 48, 64, 32)           0         ['multiply_8[0][0]',          \n",
      "                                                                     'depthwise_conv2d_26[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_1 (Conv2D)          (None, 48, 64, 16)           4624      ['block_2_output[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_2 (Conv2D)          (None, 48, 64, 16)           2320      ['block_3_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 48, 64, 16)           64        ['block_3_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_36 (Activation)  (None, 48, 64, 16)           0         ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_27 (Depth  (None, 48, 64, 16)           160       ['activation_36[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 48, 64, 16)           64        ['depthwise_conv2d_27[0][0]'] \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " block_3_3 (Conv2D)          (None, 48, 64, 32)           4640      ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_3_4 (Conv2D)          (None, 24, 32, 32)           9248      ['block_3_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 24, 32, 32)           128       ['block_3_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_37 (Activation)  (None, 24, 32, 32)           0         ['batch_normalization_47[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_9  (None, 32)                   0         ['activation_37[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)         (None, 1, 1, 32)             0         ['global_average_pooling2d_9[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 1, 1, 4)              132       ['reshape_9[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 1, 1, 4)              0         ['conv2d_34[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 1, 1, 32)             160       ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_9 (TFOpLam  (None, 1, 1, 32)             0         ['conv2d_35[0][0]']           \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " multiply_9 (Multiply)       (None, 24, 32, 32)           0         ['activation_37[0][0]',       \n",
      "                                                                     'tf.math.sigmoid_9[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_2 (Conv2D)          (None, 24, 32, 16)           4624      ['multiply_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (Ba  (None, 24, 32, 16)           64        ['block_4_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_38 (Activation)  (None, 24, 32, 16)           0         ['batch_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_28 (Depth  (None, 24, 32, 16)           160       ['activation_38[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_3 (Conv2D)          (None, 24, 32, 32)           4640      ['depthwise_conv2d_28[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_4 (Conv2D)          (None, 24, 32, 32)           9248      ['block_4_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_49 (Ba  (None, 24, 32, 32)           128       ['block_4_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_39 (Activation)  (None, 24, 32, 32)           0         ['batch_normalization_49[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_29 (Depth  (None, 24, 32, 32)           320       ['activation_39[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_output (Add)        (None, 24, 32, 32)           0         ['multiply_9[0][0]',          \n",
      "                                                                     'depthwise_conv2d_29[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_1 (Conv2D)          (None, 24, 32, 16)           4624      ['block_4_output[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_2 (Conv2D)          (None, 24, 32, 16)           2320      ['block_5_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_50 (Ba  (None, 24, 32, 16)           64        ['block_5_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_40 (Activation)  (None, 24, 32, 16)           0         ['batch_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_30 (Depth  (None, 24, 32, 16)           160       ['activation_40[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization_51 (Ba  (None, 24, 32, 16)           64        ['depthwise_conv2d_30[0][0]'] \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " block_5_3 (Conv2D)          (None, 24, 32, 32)           4640      ['batch_normalization_51[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_5_4 (Conv2D)          (None, 12, 16, 32)           9248      ['block_5_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 12, 16, 32)           128       ['block_5_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_41 (Activation)  (None, 12, 16, 32)           0         ['batch_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1  (None, 32)                   0         ['activation_41[0][0]']       \n",
      " 0 (GlobalAveragePooling2D)                                                                       \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)        (None, 1, 1, 32)             0         ['global_average_pooling2d_10[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 1, 1, 4)              132       ['reshape_10[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 1, 1, 4)              0         ['conv2d_36[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 1, 1, 32)             160       ['re_lu_16[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_10 (TFOpLa  (None, 1, 1, 32)             0         ['conv2d_37[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " multiply_10 (Multiply)      (None, 12, 16, 32)           0         ['activation_41[0][0]',       \n",
      "                                                                     'tf.math.sigmoid_10[0][0]']  \n",
      "                                                                                                  \n",
      " block_6_2 (Conv2D)          (None, 12, 16, 16)           4624      ['multiply_10[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 12, 16, 16)           64        ['block_6_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_42 (Activation)  (None, 12, 16, 16)           0         ['batch_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_31 (Depth  (None, 12, 16, 16)           160       ['activation_42[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_3 (Conv2D)          (None, 12, 16, 32)           4640      ['depthwise_conv2d_31[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_4 (Conv2D)          (None, 12, 16, 32)           9248      ['block_6_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 12, 16, 32)           128       ['block_6_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_43 (Activation)  (None, 12, 16, 32)           0         ['batch_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_32 (Depth  (None, 12, 16, 32)           320       ['activation_43[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_output (Add)        (None, 12, 16, 32)           0         ['multiply_10[0][0]',         \n",
      "                                                                     'depthwise_conv2d_32[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_1 (Conv2D)          (None, 12, 16, 16)           4624      ['block_6_output[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_2 (Conv2D)          (None, 12, 16, 16)           2320      ['block_7_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_55 (Ba  (None, 12, 16, 16)           64        ['block_7_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_44 (Activation)  (None, 12, 16, 16)           0         ['batch_normalization_55[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_33 (Depth  (None, 12, 16, 16)           160       ['activation_44[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization_56 (Ba  (None, 12, 16, 16)           64        ['depthwise_conv2d_33[0][0]'] \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " block_7_3 (Conv2D)          (None, 12, 16, 32)           4640      ['batch_normalization_56[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_7_4 (Conv2D)          (None, 6, 8, 32)             9248      ['block_7_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_57 (Ba  (None, 6, 8, 32)             128       ['block_7_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_45 (Activation)  (None, 6, 8, 32)             0         ['batch_normalization_57[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1  (None, 32)                   0         ['activation_45[0][0]']       \n",
      " 1 (GlobalAveragePooling2D)                                                                       \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)        (None, 1, 1, 32)             0         ['global_average_pooling2d_11[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 1, 1, 4)              132       ['reshape_11[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)             (None, 1, 1, 4)              0         ['conv2d_38[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 1, 1, 32)             160       ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_11 (TFOpLa  (None, 1, 1, 32)             0         ['conv2d_39[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " multiply_11 (Multiply)      (None, 6, 8, 32)             0         ['activation_45[0][0]',       \n",
      "                                                                     'tf.math.sigmoid_11[0][0]']  \n",
      "                                                                                                  \n",
      " block_8_2 (Conv2D)          (None, 6, 8, 16)             4624      ['multiply_11[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_58 (Ba  (None, 6, 8, 16)             64        ['block_8_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_46 (Activation)  (None, 6, 8, 16)             0         ['batch_normalization_58[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_34 (Depth  (None, 6, 8, 16)             160       ['activation_46[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_3 (Conv2D)          (None, 6, 8, 32)             4640      ['depthwise_conv2d_34[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_4 (Conv2D)          (None, 6, 8, 32)             9248      ['block_8_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_59 (Ba  (None, 6, 8, 32)             128       ['block_8_4[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_47 (Activation)  (None, 6, 8, 32)             0         ['batch_normalization_59[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " depthwise_conv2d_35 (Depth  (None, 6, 8, 32)             320       ['activation_47[0][0]']       \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_output (Add)        (None, 6, 8, 32)             0         ['multiply_11[0][0]',         \n",
      "                                                                     'depthwise_conv2d_35[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 158432 (618.88 KB)\n",
      "Trainable params: 157536 (615.38 KB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "backbone = BackBone()\n",
    "model = backbone()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def get_backbone():\n",
    "    backbone = BackBone() \n",
    "    backbone = backbone(input_shape=[None, None, 1])\n",
    "\n",
    "    b2_output, b4_output, b6_output, b8_output= [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in ['block_2_output', 'block_4_output', 'block_6_output', 'block_8_output']\n",
    "    ]\n",
    "\n",
    "    return keras.Model(\n",
    "        inputs = [backbone.inputs], outputs=[b2_output, b4_output, b6_output, b8_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_2_output (Add)        (None, 48, 64, 64)\n",
    "\n",
    "# block_4_output (Add)        (None, 24, 32, 64)\n",
    "\n",
    "# block_6_output (Add)        (None, 12, 16, 64)\n",
    "\n",
    "class FeaturePyramid(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
    "        self.backbone = get_backbone()\n",
    "        self.conv_c2_1x1 = keras.layers.Conv2D(32, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = keras.layers.Conv2D(32, 1, 1, \"same\")\n",
    "        self.conv_c6_1x1 = keras.layers.Conv2D(32, 1, 1, \"same\")\n",
    "        self.conv_c8_1x1 = keras.layers.Conv2D(32, 1, 1, \"same\")\n",
    "\n",
    "        self.conv_c2_3x3 = keras.layers.Conv2D(32, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = keras.layers.Conv2D(32, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = keras.layers.Conv2D(32, 3, 1, \"same\")\n",
    "        self.conv_c8_3x3 = keras.layers.Conv2D(32, 3, 1, \"same\")\n",
    "        \n",
    "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
    "    def call(self, images, training=True):\n",
    "        b2_output, b4_output, b6_output, b8_output = self.backbone(images, training=training)\n",
    "        # p2_output = self.conv_c2_1x1(b2_output)\n",
    "        p4_output = self.conv_c4_1x1(b4_output)\n",
    "        p6_output = self.conv_c6_1x1(b6_output)\n",
    "        p8_output = self.conv_c8_1x1(b8_output)\n",
    "        \n",
    "        p6_output = p6_output + self.upsample_2x(p8_output)\n",
    "        p4_output = p4_output + self.upsample_2x(p6_output)\n",
    "        # p2_output = p2_output + self.upsample_2x(p4_output)\n",
    "        \n",
    "        # p2_output = self.conv_c2_3x3(tf.nn.relu(p2_output))\n",
    "        p4_output = self.conv_c4_3x3(tf.nn.relu(p4_output))\n",
    "        p6_output = self.conv_c6_3x3(tf.nn.relu(p6_output))\n",
    "        p8_output = self.conv_c8_3x3(tf.nn.relu(b8_output))\n",
    "        # print(\"b2_output shape:\", tf.shape(b2_output))\n",
    "        # print(\"b4_output shape:\", tf.shape(b4_output))\n",
    "        # print(\"b6_output shape:\", tf.shape(b6_output))\n",
    "        # print(\"p2_output shape:\", tf.shape(p2_output))\n",
    "        # print(\"p4_output shape:\", tf.shape(p4_output))\n",
    "        # print(\"p6_output shape:\", tf.shape(p6_output))\n",
    "        # print(\"pn_output shape:\", tf.shape(pn_output))\n",
    "\n",
    "             \n",
    "        return p4_output, p6_output, p8_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_head(output_filters, bias_init):\n",
    "    head = keras.Sequential([keras.Input(shape=[None, None, 32])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01, seed=1)\n",
    "\n",
    "    for _ in range(3):\n",
    "        head.add(\n",
    "            keras.layers.Conv2D(128, 3, padding = 'same', kernel_initializer=kernel_init)\n",
    "        )\n",
    "        head.add(keras.layers.ReLU())\n",
    "\n",
    "    \n",
    "    head.add(\n",
    "        keras.layers.Conv2D(\n",
    "            output_filters,\n",
    "            3,\n",
    "            1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            bias_initializer=bias_init,\n",
    "        )\n",
    "    )\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(keras.Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(CustomNet, self).__init__(name=\"CustomNet\", **kwargs)\n",
    "        self.fpn = FeaturePyramid()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(8 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(8 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=True):\n",
    "        # # # # # # print(f\"shape: {image.shape}\")\n",
    "        features = self.fpn(image, training=training)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "\n",
    "        for feature in features:\n",
    "            box_output = tf.reshape(self.box_head(feature), [N, -1, 4])\n",
    "            cls_output = tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
    "\n",
    "            # print(\"feature shape:\", tf.shape(feature))\n",
    "            # print(\"box_output shape:\", tf.shape(box_output))\n",
    "            # print(\"cls_output shape:\", tf.shape(cls_output))\n",
    "\n",
    "            box_outputs.append(box_output)\n",
    "            cls_outputs.append(cls_output)\n",
    "        \n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        # print(\"Final cls_outputs shape:\", tf.shape(cls_outputs))\n",
    "        # print(\"Final box_outputs shape:\", tf.shape(box_outputs))\n",
    "        final_output = tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        # print(\"Final output shape:\", tf.shape(final_output))\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetBoxLoss(tf.losses.Loss):  \n",
    "    def __init__(self, delta):\n",
    "        super(CustomNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"CustomNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5\n",
    "        )\n",
    "\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetClassificationLoss(tf.losses.Loss):   \n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(CustomNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"CustomNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred\n",
    "        )\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetLoss(tf.losses.Loss):    \n",
    "    def __init__(self, num_classes=1, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "        super(CustomNetLoss, self).__init__(reduction=\"auto\", name=\"CustomNetLoss\")\n",
    "        self._cls_loss = CustomNetClassificationLoss(alpha, gamma)\n",
    "        self._box_loss = CustomNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # print(\"y_true shape:\", tf.shape(y_true))\n",
    "        # print(\"y_pred shape:\", tf.shape(y_pred))\n",
    "\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        # print(\"box_labels shape:\", tf.shape(box_labels))\n",
    "        # print(\"box_predictions shape:\", tf.shape(box_predictions))\n",
    "        \n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        # print(\"cls_labels shape:\", tf.shape(cls_labels))\n",
    "        # print(\"cls_predictions shape:\", tf.shape(cls_predictions))\n",
    "\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        # print(\"positive_mask shape:\", tf.shape(positive_mask))\n",
    "        # print(\"ignore_mask shape:\", tf.shape(ignore_mask))\n",
    "\n",
    "        cls_loss = self._cls_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        # print(\"cls_loss shape:\", tf.shape(cls_loss))\n",
    "        # print(\"box_loss shape:\", tf.shape(box_loss))\n",
    "\n",
    "        cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, cls_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        # print(\"normalizer:\", normalizer)\n",
    "\n",
    "        cls_loss = tf.math.divide_no_nan(tf.reduce_sum(cls_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        \n",
    "        loss = cls_loss + box_loss\n",
    "        # print(\"Final loss shape:\", tf.shape(loss))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "# class CustomNetLoss(tf.losses.Loss):\n",
    "#     def __init__(self, num_classes=1):\n",
    "#         super(CustomNetLoss, self).__init__(reduction=\"auto\", name=\"CustomNetLoss\")\n",
    "#         self._num_classes = num_classes\n",
    "#         self.mse_loss = MeanSquaredError(reduction=\"none\")\n",
    "#         self.cce_loss = CategoricalCrossentropy(reduction=\"none\", from_logits=True)\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # 박스 레이블과 예측 분리\n",
    "#         box_labels = y_true[:, :, :4]\n",
    "#         box_predictions = y_pred[:, :, :4]\n",
    "\n",
    "#         # 클래스 레이블과 예측 분리 및 one-hot 인코딩\n",
    "#         cls_labels = tf.one_hot(tf.cast(y_true[:, :, 4], dtype=tf.int32), depth=self._num_classes)\n",
    "#         cls_predictions = y_pred[:, :, 4:]\n",
    "\n",
    "#         # 박스 손실(MSE) 계산\n",
    "#         box_loss = self.mse_loss(box_labels, box_predictions)\n",
    "\n",
    "#         # 클래스 손실(Categorical Cross-Entropy) 계산\n",
    "#         cls_loss = self.cce_loss(cls_labels, cls_predictions)\n",
    "\n",
    "#         # 손실 합산\n",
    "#         total_loss = tf.reduce_mean(cls_loss) + tf.reduce_mean(box_loss)\n",
    "#         return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")\n",
    "\n",
    "# LR = 0.0005\n",
    "# initial_learning_rate = LR\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,\n",
    "#     decay_steps=5000,\n",
    "#     decay_rate=0.90,\n",
    "#     staircase=True)\n",
    "\n",
    "\n",
    "model_dir = \"ObjectDetectionCheckpoint/customnet.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "# # # # 멀티 GPU 전략 설정\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # 모델, 손실 함수, 옵티마이저를 전략 범위 내에서 정의\n",
    "    model = CustomNet(num_classes)\n",
    "    loss_fn = CustomNetLoss()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "\n",
    "# # # # # 모델, 손실 함수, 옵티마이저를 전략 범위 내에서 정의\n",
    "# model = CustomNet(num_classes)\n",
    "# loss_fn = CustomNetLoss()\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "# # 모델 컴파일\n",
    "# model.compile(optimizer=optimizer, \n",
    "#               loss=loss_fn,\n",
    "#               metrics=['accuracy', Precision(), Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (7, 96, 128, 1)\n",
      "Images max: tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Images min: tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Labels shape: (7, 8064, 5)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "for batch in train_dataset.take(1):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    \n",
    "    # 이미지와 레이블의 형태를 출력합니다\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Images max:\", tf.reduce_max(images))\n",
    "    print(\"Images min:\", tf.reduce_min(images))\n",
    "    print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # TensorFlow Dataset 예시\n",
    "# # train_dataset = tf.data.Dataset.from_tensor_slices(...)\n",
    "\n",
    "# def check_nan_in_dataset(dataset):\n",
    "#     for batch in dataset:\n",
    "#         images, labels = batch\n",
    "#         if tf.reduce_any(tf.math.is_nan(images)) or tf.reduce_any(tf.math.is_nan(labels)):\n",
    "#             return True\n",
    "#     # print(images)\n",
    "#     # print(labels)\n",
    "#     return False\n",
    "\n",
    "\n",
    "# contains_nan = check_nan_in_dataset(train_dataset)\n",
    "# if contains_nan:\n",
    "#     print(\"Data contains NaN values\")\n",
    "# else:\n",
    "#     print(\"Data does not contain NaN values\")\n",
    "\n",
    "# check_nan_in_dataset(train_dataset)\n",
    "# check_nan_in_dataset(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 14:16:47.786171: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"UnbatchDataset/_28\"\n",
      "op: \"UnbatchDataset\"\n",
      "input: \"MapDataset/_27\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021UnbatchDataset:69\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 128\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 8200\n",
      "        }\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:Collective all_reduce tensors: 164 all_reduces, num_devices = 7, group_size = 7, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 164 all_reduces, num_devices = 7, group_size = 7, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 14:18:08.415236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-23 14:18:09.594885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-23 14:18:10.838538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-23 14:18:12.186020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-23 14:18:13.848318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-23 14:18:14.728375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    311/Unknown - 127s 77ms/step - loss: 1.2941 - accuracy: 0.1758 - precision_1: 0.1489 - recall_1: 9.9417e-04\n",
      "Epoch 1: loss improved from inf to 1.29408, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 14:18:56.372610: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3417217166738408402\n",
      "2024-01-23 14:18:56.372695: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15197218983853130882\n",
      "2024-01-23 14:18:56.372731: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 84200149573206411\n",
      "2024-01-23 14:18:56.372799: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14572776815660160788\n",
      "2024-01-23 14:18:56.372868: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18193135748813391130\n",
      "2024-01-23 14:18:56.372917: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16176776346855867109\n",
      "2024-01-23 14:18:56.372984: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16454798746452926006\n",
      "2024-01-23 14:18:56.373035: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1499858241190168523\n",
      "2024-01-23 14:18:56.373057: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11111030773341127260\n",
      "2024-01-23 14:18:56.373079: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1371623217688102450\n",
      "2024-01-23 14:18:56.373102: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14293954843631583703\n",
      "2024-01-23 14:18:56.373142: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4113224692896244462\n",
      "2024-01-23 14:18:56.373157: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5242577390931755927\n",
      "2024-01-23 14:18:56.373183: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6550962375221970731\n",
      "2024-01-23 14:18:56.373197: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4591644857036998391\n",
      "2024-01-23 14:18:56.373222: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14772182820102333236\n",
      "2024-01-23 14:18:56.373246: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12339651632607463266\n",
      "2024-01-23 14:18:56.373270: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 15414012011504640366\n",
      "2024-01-23 14:18:56.373283: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17252925559232744018\n",
      "2024-01-23 14:18:56.373307: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7172864543726226411\n",
      "2024-01-23 14:18:56.373330: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8486694270028614207\n",
      "2024-01-23 14:18:56.373353: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1779944698857154196\n",
      "2024-01-23 14:18:56.373382: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8699549943448840134\n",
      "2024-01-23 14:18:56.373400: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8691493830229642114\n",
      "2024-01-23 14:18:56.373414: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10671766034497939678\n",
      "2024-01-23 14:18:56.373427: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11852593435589301034\n",
      "2024-01-23 14:18:56.373444: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17114014274366508403\n",
      "2024-01-23 14:18:56.373467: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13909650123438864255\n",
      "2024-01-23 14:18:56.373488: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3751893066100377788\n",
      "2024-01-23 14:18:56.373508: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7856155977412593742\n",
      "2024-01-23 14:18:56.373518: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9005151927196305780\n",
      "2024-01-23 14:18:56.373532: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6463488972278877142\n",
      "2024-01-23 14:18:56.373549: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17635363136266720522\n",
      "2024-01-23 14:18:56.373580: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14870959634856299891\n",
      "2024-01-23 14:18:56.373601: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17720287379711085959\n",
      "2024-01-23 14:18:56.373620: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2888810623904868532\n",
      "2024-01-23 14:18:56.373641: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6691988856155499462\n",
      "2024-01-23 14:18:56.373660: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11652736820132353682\n",
      "2024-01-23 14:18:56.373681: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5770421002699034563\n",
      "2024-01-23 14:18:56.373708: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 718955369968209815\n",
      "2024-01-23 14:18:56.373727: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6332123594430123084\n",
      "2024-01-23 14:18:56.373748: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4144413115626106238\n",
      "2024-01-23 14:18:56.373767: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7443176193508793922\n",
      "2024-01-23 14:18:56.373789: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1733735576309436291\n",
      "2024-01-23 14:18:56.373810: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13256132473762107815\n",
      "2024-01-23 14:18:56.373828: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 15575405298473341823\n",
      "2024-01-23 14:18:56.373844: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10187376475932504444\n",
      "2024-01-23 14:18:56.373871: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10128668576692447382\n",
      "2024-01-23 14:18:56.373891: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16533841741093593338\n",
      "2024-01-23 14:18:56.373902: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6156017667135697422\n",
      "2024-01-23 14:18:56.373911: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17420726802986389795\n",
      "2024-01-23 14:18:56.373933: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11432550031247922743\n",
      "2024-01-23 14:18:56.373958: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14200158421364758364\n",
      "2024-01-23 14:18:56.373980: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15294386115662473402\n",
      "2024-01-23 14:18:56.373999: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10320549293296956166\n",
      "2024-01-23 14:18:56.374018: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17245838982094918347\n",
      "2024-01-23 14:18:56.374044: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3746031301635249855\n",
      "2024-01-23 14:18:56.374064: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13511981716475975468\n",
      "2024-01-23 14:18:56.374087: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8187054827410074010\n",
      "2024-01-23 14:18:56.374107: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14008581412502361398\n",
      "2024-01-23 14:18:56.374126: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 934968042755494475\n",
      "2024-01-23 14:18:56.374151: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6930959110385211455\n",
      "2024-01-23 14:18:56.374170: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10167063039978986396\n",
      "2024-01-23 14:18:56.374193: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6535624092352539586\n",
      "2024-01-23 14:18:56.374214: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13268201712430692558\n",
      "2024-01-23 14:18:56.374234: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12693457978199559811\n",
      "2024-01-23 14:18:56.374258: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2797593858924714247\n",
      "2024-01-23 14:18:56.374277: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 939345070119320684\n",
      "2024-01-23 14:18:56.374300: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9822302938376612898\n",
      "2024-01-23 14:18:56.374321: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13589634523909159366\n",
      "2024-01-23 14:18:56.374339: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 4104412846962949006\n",
      "2024-01-23 14:18:56.374355: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3074927879879022019\n",
      "2024-01-23 14:18:56.374380: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8079095350331611127\n",
      "2024-01-23 14:18:56.374398: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5319960704315047356\n",
      "2024-01-23 14:18:56.374421: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16617445577148786914\n",
      "2024-01-23 14:18:56.374443: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14639786189000020702\n",
      "2024-01-23 14:18:56.374462: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17085264785885173363\n",
      "2024-01-23 14:18:56.374488: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13215918067495739207\n",
      "2024-01-23 14:18:56.374507: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13512938612472445788\n",
      "2024-01-23 14:18:56.374530: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 224926363083960674\n",
      "2024-01-23 14:18:56.374551: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3347918466235784758\n",
      "2024-01-23 14:18:56.374571: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10252535900987792171\n",
      "2024-01-23 14:18:56.374596: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 421915286705546631\n",
      "2024-01-23 14:18:56.374615: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16764858758345023308\n",
      "2024-01-23 14:18:56.374638: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9380992058917712130\n",
      "2024-01-23 14:18:56.374659: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8361714064184180526\n",
      "2024-01-23 14:18:56.374679: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16183870501845786667\n",
      "2024-01-23 14:18:56.374703: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15965318789339849087\n",
      "2024-01-23 14:18:56.374721: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7003141101837776250\n",
      "2024-01-23 14:18:56.374743: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3656099586380947739\n",
      "2024-01-23 14:18:56.374765: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2088134749156386847\n",
      "2024-01-23 14:18:56.374785: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18013352634456520434\n",
      "2024-01-23 14:18:56.374807: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3656101113064926955\n",
      "2024-01-23 14:18:56.374831: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17811388573189007775\n",
      "2024-01-23 14:18:56.374850: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11830087230522161394\n",
      "2024-01-23 14:18:56.374875: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7243433652658056987\n",
      "2024-01-23 14:18:56.374897: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1620273852348738423\n",
      "2024-01-23 14:18:56.374916: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3785005212410854874\n",
      "2024-01-23 14:18:56.374938: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13194163150517433147\n",
      "2024-01-23 14:18:56.374964: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8760004338697628671\n",
      "2024-01-23 14:18:56.374983: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15079677569269513994\n",
      "2024-01-23 14:18:56.375006: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13839801914586887283\n",
      "2024-01-23 14:18:56.375027: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17713793231222713655\n",
      "2024-01-23 14:18:56.375047: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13352116588856344769\n",
      "2024-01-23 14:18:56.375069: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4480926576286070699\n",
      "2024-01-23 14:18:56.375093: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9660543736190398303\n",
      "2024-01-23 14:18:56.375112: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11377200598437244001\n",
      "2024-01-23 14:18:56.375134: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7838554349560855227\n",
      "2024-01-23 14:18:56.375154: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18117966601230758239\n",
      "2024-01-23 14:18:56.375172: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3596378572865979081\n",
      "2024-01-23 14:18:56.375193: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10092606522996603531\n",
      "2024-01-23 14:18:56.375218: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12982331268559302271\n",
      "2024-01-23 14:18:56.375237: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14080684730397115145\n",
      "2024-01-23 14:18:56.375260: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7099459547187362867\n",
      "2024-01-23 14:18:56.375282: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13204267255221483015\n",
      "2024-01-23 14:18:56.375302: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11462821379766994633\n",
      "2024-01-23 14:18:56.375323: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11934939836433985175\n",
      "2024-01-23 14:18:56.375341: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15449194497818176545\n",
      "2024-01-23 14:18:56.375359: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10246386780200155567\n",
      "2024-01-23 14:18:56.375378: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3559008454960400537\n",
      "2024-01-23 14:18:56.375397: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5420001835524323159\n",
      "2024-01-23 14:18:56.375415: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16952447947858938577\n",
      "2024-01-23 14:18:56.375432: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15642471204501683399\n",
      "2024-01-23 14:18:56.375451: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10728253235409800025\n",
      "2024-01-23 14:18:56.375469: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8494966255482813767\n",
      "2024-01-23 14:18:56.375488: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4681692313042488385\n",
      "2024-01-23 14:18:56.375507: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13257154751754682111\n",
      "2024-01-23 14:18:56.375525: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8029399031033532641\n",
      "2024-01-23 14:18:56.375544: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11174317930212212495\n",
      "2024-01-23 14:18:56.375561: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3002120339627365065\n",
      "2024-01-23 14:18:56.375580: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2844668325475353141\n",
      "2024-01-23 14:18:56.375600: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15582430011734299537\n",
      "2024-01-23 14:18:56.375619: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17209969174427657365\n",
      "2024-01-23 14:18:56.375639: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8581073881515546257\n",
      "2024-01-23 14:18:56.375658: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18102462268200330965\n",
      "2024-01-23 14:18:56.375677: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5835868124335463617\n",
      "2024-01-23 14:18:56.375696: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9650211707020093253\n",
      "2024-01-23 14:18:56.375715: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5095635009975950689\n",
      "2024-01-23 14:18:56.375732: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12733076541720897413\n",
      "2024-01-23 14:18:56.375752: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5774308730731535897\n",
      "2024-01-23 14:18:56.375771: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6872980018431167733\n",
      "2024-01-23 14:18:56.375790: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7422503976507563985\n",
      "2024-01-23 14:18:56.375809: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1020126218931785005\n",
      "2024-01-23 14:18:56.375828: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5844647503646481585\n",
      "2024-01-23 14:18:56.375846: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4296245054819995221\n",
      "2024-01-23 14:18:56.375865: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 885500414564246377\n",
      "2024-01-23 14:18:56.375884: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4143405765636222653\n",
      "2024-01-23 14:18:56.375903: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18330283506127529489\n",
      "2024-01-23 14:18:56.375920: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7303351836640236269\n",
      "2024-01-23 14:18:56.375940: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8536042035412265649\n",
      "2024-01-23 14:18:56.375957: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9233944391516451589\n",
      "2024-01-23 14:18:56.375976: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13474184137739578785\n",
      "2024-01-23 14:18:56.375995: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15536103044249433285\n",
      "2024-01-23 14:18:56.376014: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16971605924498190033\n",
      "2024-01-23 14:18:56.376036: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10659402669743117917\n",
      "2024-01-23 14:18:56.376056: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1358133892270157689\n",
      "2024-01-23 14:18:56.376074: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2860651809688319693\n",
      "2024-01-23 14:18:56.376094: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11516004630730959457\n",
      "2024-01-23 14:18:56.376112: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13199660640789231893\n",
      "2024-01-23 14:18:56.376132: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6318536649176133353\n",
      "2024-01-23 14:18:56.376150: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7858307346504096885\n",
      "2024-01-23 14:18:56.376169: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2409000689902170641\n",
      "2024-01-23 14:18:56.376188: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 787745027832454181\n",
      "2024-01-23 14:18:56.376208: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1300641186580657025\n",
      "2024-01-23 14:18:56.376227: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 582941533605559853\n",
      "2024-01-23 14:18:56.376247: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9672557131055805241\n",
      "2024-01-23 14:18:56.376266: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16780081766811000813\n",
      "2024-01-23 14:18:56.376285: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12325001630370412649\n",
      "2024-01-23 14:18:56.376305: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13707543491003939369\n",
      "2024-01-23 14:18:56.376323: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7930722838149579152\n",
      "2024-01-23 14:18:56.376478: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13642190656723502344\n",
      "2024-01-23 14:18:56.376495: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18307512816018851088\n",
      "2024-01-23 14:18:56.376513: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7881197966336116520\n",
      "2024-01-23 14:18:56.376533: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4433530070811921528\n",
      "2024-01-23 14:18:56.376548: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10979582489444797552\n",
      "2024-01-23 14:18:56.376564: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4118783017905868384\n",
      "2024-01-23 14:18:56.376579: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16841253234884481504\n",
      "2024-01-23 14:18:56.376592: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12109493085767405888\n",
      "2024-01-23 14:18:56.376612: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5087289518202623376\n",
      "2024-01-23 14:18:56.376627: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10125688176994653880\n",
      "2024-01-23 14:18:56.376648: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1652860247412580248\n",
      "2024-01-23 14:18:56.376667: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 82866359558365368\n",
      "2024-01-23 14:18:56.376682: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9926116024652767496\n",
      "2024-01-23 14:18:56.376700: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1959262194380808760\n",
      "2024-01-23 14:18:56.376867: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10398828850569569080\n",
      "2024-01-23 14:18:56.376895: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13601419037152535592\n",
      "2024-01-23 14:18:56.376909: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4550585204276667560\n",
      "2024-01-23 14:18:56.377082: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16703820441414510624\n",
      "2024-01-23 14:18:56.377100: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1737975261543197392\n",
      "2024-01-23 14:18:56.377246: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14349791494609595432\n",
      "2024-01-23 14:18:56.377280: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1735809554546602256\n",
      "2024-01-23 14:18:56.377297: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15239359816460530112\n",
      "2024-01-23 14:18:56.377328: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5720840345496419136\n",
      "2024-01-23 14:18:56.377499: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4952913098501078368\n",
      "2024-01-23 14:18:56.377669: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11982965617385144064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 [==============================] - 128s 80ms/step - loss: 1.2941 - accuracy: 0.1758 - precision_1: 0.1489 - recall_1: 9.9417e-04\n",
      "Epoch 2/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 1.0550 - accuracy: 0.1905 - precision_1: 0.1442 - recall_1: 0.0031\n",
      "Epoch 2: loss improved from 1.29408 to 1.05498, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_2\n",
      "311/311 [==============================] - 25s 79ms/step - loss: 1.0550 - accuracy: 0.1905 - precision_1: 0.1442 - recall_1: 0.0031\n",
      "Epoch 3/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.9082 - accuracy: 0.4075 - precision_1: 0.1390 - recall_1: 0.0057\n",
      "Epoch 3: loss improved from 1.05498 to 0.90815, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_3\n",
      "311/311 [==============================] - 25s 80ms/step - loss: 0.9082 - accuracy: 0.4075 - precision_1: 0.1390 - recall_1: 0.0057\n",
      "Epoch 4/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8846 - accuracy: 0.2146 - precision_1: 0.1433 - recall_1: 0.0062\n",
      "Epoch 4: loss improved from 0.90815 to 0.88461, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_4\n",
      "311/311 [==============================] - 25s 81ms/step - loss: 0.8846 - accuracy: 0.2146 - precision_1: 0.1433 - recall_1: 0.0062\n",
      "Epoch 5/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8764 - accuracy: 0.1986 - precision_1: 0.1450 - recall_1: 0.0071\n",
      "Epoch 5: loss improved from 0.88461 to 0.87638, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_5\n",
      "311/311 [==============================] - 25s 79ms/step - loss: 0.8764 - accuracy: 0.1986 - precision_1: 0.1450 - recall_1: 0.0071\n",
      "Epoch 6/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8802 - accuracy: 0.1835 - precision_1: 0.1442 - recall_1: 0.0074\n",
      "Epoch 6: loss did not improve from 0.87638\n",
      "311/311 [==============================] - 24s 78ms/step - loss: 0.8802 - accuracy: 0.1835 - precision_1: 0.1442 - recall_1: 0.0074\n",
      "Epoch 7/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8756 - accuracy: 0.2145 - precision_1: 0.1457 - recall_1: 0.0078\n",
      "Epoch 7: loss improved from 0.87638 to 0.87563, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_7\n",
      "311/311 [==============================] - 25s 81ms/step - loss: 0.8756 - accuracy: 0.2145 - precision_1: 0.1457 - recall_1: 0.0078\n",
      "Epoch 8/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8871 - accuracy: 0.5992 - precision_1: 0.1449 - recall_1: 0.0080\n",
      "Epoch 8: loss did not improve from 0.87563\n",
      "311/311 [==============================] - 24s 78ms/step - loss: 0.8871 - accuracy: 0.5992 - precision_1: 0.1449 - recall_1: 0.0080\n",
      "Epoch 9/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8834 - accuracy: 0.6048 - precision_1: 0.1466 - recall_1: 0.0079\n",
      "Epoch 9: loss did not improve from 0.87563\n",
      "311/311 [==============================] - 24s 78ms/step - loss: 0.8834 - accuracy: 0.6048 - precision_1: 0.1466 - recall_1: 0.0079\n",
      "Epoch 10/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8811 - accuracy: 0.6162 - precision_1: 0.1463 - recall_1: 0.0080\n",
      "Epoch 10: loss did not improve from 0.87563\n",
      "311/311 [==============================] - 24s 78ms/step - loss: 0.8811 - accuracy: 0.6162 - precision_1: 0.1463 - recall_1: 0.0080\n",
      "Epoch 11/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8789 - accuracy: 0.6630 - precision_1: 0.1458 - recall_1: 0.0082\n",
      "Epoch 11: loss did not improve from 0.87563\n",
      "311/311 [==============================] - 24s 79ms/step - loss: 0.8789 - accuracy: 0.6630 - precision_1: 0.1458 - recall_1: 0.0082\n",
      "Epoch 12/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8761 - accuracy: 0.6069 - precision_1: 0.1494 - recall_1: 0.0087\n",
      "Epoch 12: loss did not improve from 0.87563\n",
      "311/311 [==============================] - 24s 78ms/step - loss: 0.8761 - accuracy: 0.6069 - precision_1: 0.1494 - recall_1: 0.0087\n",
      "Epoch 13/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8756 - accuracy: 0.5951 - precision_1: 0.1486 - recall_1: 0.0086\n",
      "Epoch 13: loss improved from 0.87563 to 0.87558, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_13\n",
      "311/311 [==============================] - 25s 80ms/step - loss: 0.8756 - accuracy: 0.5951 - precision_1: 0.1486 - recall_1: 0.0086\n",
      "Epoch 14/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.6301 - precision_1: 0.1494 - recall_1: 0.0082\n",
      "Epoch 14: loss did not improve from 0.87558\n",
      "311/311 [==============================] - 24s 78ms/step - loss: 0.8809 - accuracy: 0.6301 - precision_1: 0.1494 - recall_1: 0.0082\n",
      "Epoch 15/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8825 - accuracy: 0.6080 - precision_1: 0.1469 - recall_1: 0.0078\n",
      "Epoch 15: loss did not improve from 0.87558\n",
      "311/311 [==============================] - 24s 78ms/step - loss: 0.8825 - accuracy: 0.6080 - precision_1: 0.1469 - recall_1: 0.0078\n",
      "Epoch 16/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8711 - accuracy: 0.3879 - precision_1: 0.1456 - recall_1: 0.0083\n",
      "Epoch 16: loss improved from 0.87558 to 0.87107, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_16\n",
      "311/311 [==============================] - 25s 80ms/step - loss: 0.8711 - accuracy: 0.3879 - precision_1: 0.1456 - recall_1: 0.0083\n",
      "Epoch 17/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8678 - accuracy: 0.3954 - precision_1: 0.1471 - recall_1: 0.0089\n",
      "Epoch 17: loss improved from 0.87107 to 0.86779, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_17\n",
      "311/311 [==============================] - 25s 80ms/step - loss: 0.8678 - accuracy: 0.3954 - precision_1: 0.1471 - recall_1: 0.0089\n",
      "Epoch 18/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8634 - accuracy: 0.3805 - precision_1: 0.1479 - recall_1: 0.0089\n",
      "Epoch 18: loss improved from 0.86779 to 0.86343, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_18\n",
      "311/311 [==============================] - 25s 81ms/step - loss: 0.8634 - accuracy: 0.3805 - precision_1: 0.1479 - recall_1: 0.0089\n",
      "Epoch 19/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8654 - accuracy: 0.3267 - precision_1: 0.1497 - recall_1: 0.0090\n",
      "Epoch 19: loss did not improve from 0.86343\n",
      "311/311 [==============================] - 24s 79ms/step - loss: 0.8654 - accuracy: 0.3267 - precision_1: 0.1497 - recall_1: 0.0090\n",
      "Epoch 20/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8601 - accuracy: 0.4460 - precision_1: 0.1534 - recall_1: 0.0096\n",
      "Epoch 20: loss improved from 0.86343 to 0.86009, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_20\n",
      "311/311 [==============================] - 25s 81ms/step - loss: 0.8601 - accuracy: 0.4460 - precision_1: 0.1534 - recall_1: 0.0096\n",
      "Epoch 21/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8599 - accuracy: 0.3899 - precision_1: 0.1509 - recall_1: 0.0094\n",
      "Epoch 21: loss improved from 0.86009 to 0.85991, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_21\n",
      "311/311 [==============================] - 25s 81ms/step - loss: 0.8599 - accuracy: 0.3899 - precision_1: 0.1509 - recall_1: 0.0094\n",
      "Epoch 22/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8585 - accuracy: 0.4269 - precision_1: 0.1527 - recall_1: 0.0094\n",
      "Epoch 22: loss improved from 0.85991 to 0.85853, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_22\n",
      "311/311 [==============================] - 25s 81ms/step - loss: 0.8585 - accuracy: 0.4269 - precision_1: 0.1527 - recall_1: 0.0094\n",
      "Epoch 23/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8641 - accuracy: 0.4204 - precision_1: 0.1557 - recall_1: 0.0093\n",
      "Epoch 23: loss did not improve from 0.85853\n",
      "311/311 [==============================] - 25s 79ms/step - loss: 0.8641 - accuracy: 0.4204 - precision_1: 0.1557 - recall_1: 0.0093\n",
      "Epoch 24/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8813 - accuracy: 0.3688 - precision_1: 0.1419 - recall_1: 0.0062\n",
      "Epoch 24: loss did not improve from 0.85853\n",
      "311/311 [==============================] - 25s 79ms/step - loss: 0.8813 - accuracy: 0.3688 - precision_1: 0.1419 - recall_1: 0.0062\n",
      "Epoch 25/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8727 - accuracy: 0.4302 - precision_1: 0.1449 - recall_1: 0.0072\n",
      "Epoch 25: loss did not improve from 0.85853\n",
      "311/311 [==============================] - 24s 79ms/step - loss: 0.8727 - accuracy: 0.4302 - precision_1: 0.1449 - recall_1: 0.0072\n",
      "Epoch 26/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8736 - accuracy: 0.5706 - precision_1: 0.1482 - recall_1: 0.0077\n",
      "Epoch 26: loss did not improve from 0.85853\n",
      "311/311 [==============================] - 25s 79ms/step - loss: 0.8736 - accuracy: 0.5706 - precision_1: 0.1482 - recall_1: 0.0077\n",
      "Epoch 27/100\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.8756 - accuracy: 0.6481 - precision_1: 0.1475 - recall_1: 0.0083\n",
      "Epoch 27: loss did not improve from 0.85853\n",
      "311/311 [==============================] - 25s 79ms/step - loss: 0.8756 - accuracy: 0.6481 - precision_1: 0.1475 - recall_1: 0.0083\n",
      "Epoch 28/100\n",
      "283/311 [==========================>...] - ETA: 2s - loss: 0.8745 - accuracy: 0.6470 - precision_1: 0.1486 - recall_1: 0.0083"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecodePredictions(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_classes=1,\n",
    "#         confidence_threshold=0.05,\n",
    "#         nms_iou_threshold=0.5,\n",
    "#         max_detections_per_class=1,\n",
    "#         max_detections=4,\n",
    "#         box_variance=[0.1, 0.1, 0.2, 0.2],\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super(DecodePredictions, self).__init__(**kwargs)\n",
    "#         self.num_classes = num_classes\n",
    "#         self.confidence_threshold = confidence_threshold\n",
    "#         self.nms_iou_threshold = nms_iou_threshold\n",
    "#         self.max_detections_per_class = max_detections_per_class\n",
    "#         self.max_detections = max_detections\n",
    "\n",
    "#         self._anchor_box = AnchorBox()\n",
    "#         self._box_variance = tf.convert_to_tensor(\n",
    "#             [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "#         )\n",
    "\n",
    "#     def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
    "#         boxes = box_predictions * self._box_variance\n",
    "#         boxes = tf.concat(\n",
    "#             [\n",
    "#                 boxes[:,:,:2] * anchor_boxes[:,:,2:] + anchor_boxes[:,:,:2],\n",
    "#                 tf.math.exp(boxes[:,:,2:]) * anchor_boxes[:,:,2:]\n",
    "#             ],\n",
    "#             axis=-1,\n",
    "#         )\n",
    "#         boxes_transformed = convert_to_corners(boxes)\n",
    "#         return boxes_transformed\n",
    "\n",
    "#     def call(self, images, predictions):\n",
    "#         image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "#         anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "#         box_predictions = predictions[:,:,:4]\n",
    "#         cls_predictions = predictions[:,:,4:]\n",
    "#         boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "#         cls_predictions = tf.nn.sigmoid(cls_predictions)\n",
    "\n",
    "#         return tf.image.combined_non_max_suppression(\n",
    "#             tf.expand_dims(boxes, axis=2),\n",
    "#             cls_predictions,\n",
    "#             self.max_detections_per_class,\n",
    "#             self.max_detections,\n",
    "#             self.nms_iou_threshold,\n",
    "#             self.confidence_threshold,\n",
    "#             clip_boxes=False,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = tf.keras.Input(shape=[None, None, 1], name=\"image\")\n",
    "# predictions = model(image, training=False)\n",
    "# # detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
    "# inference_model = tf.keras.Model(inputs=image, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_detections(\n",
    "#     image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
    "# ):\n",
    "#     image = np.array(image, dtype=np.uint8)\n",
    "#     plt.figure(figsize=figsize)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.imshow(image)\n",
    "#     ax = plt.gca()\n",
    "#     for box, _cls, score in zip(boxes, classes, scores):\n",
    "#         text = \"{}: {:.2f}\".format(_cls, score)\n",
    "#         x1, y1, x2, y2 = box\n",
    "#         w, h = x2 - x1, y2 - y1\n",
    "#         patch = plt.Rectangle(\n",
    "#             [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "#         )\n",
    "#         ax.add_patch(patch)\n",
    "#         ax.text(\n",
    "#             x1,\n",
    "#             y1,\n",
    "#             text,\n",
    "#             bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
    "#             clip_box=ax.clipbox,\n",
    "#             clip_on=True,\n",
    "#         )\n",
    "#     plt.show()\n",
    "#     return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = []\n",
    "# for img, _ in train_dataset.take(1):\n",
    "#     image.append(img[0])\n",
    "# image = np.array(image)\n",
    "# plt.imshow(image[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset['images']\n",
    "plt.imshow(a[3000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_image(image):\n",
    "#     image, _, ratio = resize_and_pad_image(image)\n",
    "#     # image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "#     return tf.expand_dims(image, axis=0), ratio\n",
    "\n",
    "\n",
    "# image = dataset['images']\n",
    "# image = tf.cast(image[3000], dtype=tf.float32)\n",
    "# input_image, ratio = prepare_image(image)\n",
    "# print(input_image.shape)\n",
    "# print(ratio)\n",
    "# detections = inference_model.predict(input_image)\n",
    "# print(detections.shape)\n",
    "# # num_detections = detections.valid_detections[0]\n",
    "# # visualize_detections(\n",
    "#     # image,\n",
    "#     # detections.nmsed_boxes[0][:num_detections] / ratio,\n",
    "#     # detections.nmsed_scores[0][:num_detections],\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
