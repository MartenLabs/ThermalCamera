{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# class Logger(object):\n",
    "#     def __init__(self, filename=\"tensorflow_logs.log\"):\n",
    "#         self.terminal = sys.stdout\n",
    "#         self.log = open(filename, \"a\")\n",
    "\n",
    "#     def write(self, message):\n",
    "#         self.terminal.write(message)\n",
    "#         self.log.write(message)\n",
    "\n",
    "#     def flush(self):\n",
    "#         # 이 메소드는 flush() 인터페이스를 유지하기 위한 것입니다.\n",
    "#         pass\n",
    "\n",
    "# sys.stdout = Logger(\"tensorflow_logs.log\")\n",
    "\n",
    "# # 테스트 출력\n",
    "# print(\"This will be written to both console and file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "datasets = np.load('npz/ObjectDetection.npz', allow_pickle=True)\n",
    "images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']\n",
    "\n",
    "max_label_length = 4\n",
    "labels = []\n",
    "for num in numbers:\n",
    "    cls = [1] * num if num != 0 else [0]\n",
    "    cls += [0] * (max_label_length - len(cls))\n",
    "    labels.append(cls)\n",
    "\n",
    "# labels = np.array(labels)\n",
    "expected_image_shape = images[9000].shape\n",
    "expected_bbox_shape = bboxes[9000].shape\n",
    "expected_label_length = bboxes[9000].shape\n",
    "\n",
    "non_zero_indices = np.where(numbers != 0)[0]\n",
    "\n",
    "images_filtered = images[non_zero_indices]\n",
    "bboxes_filtered = bboxes[non_zero_indices]\n",
    "labels_filtered = np.array(labels)[non_zero_indices]\n",
    "# labels = np.array(labels)\n",
    "\n",
    "\n",
    "for i in range(len(images_filtered)):\n",
    "    image = images_filtered[i]\n",
    "    bbox = bboxes_filtered[i]\n",
    "    label = labels_filtered[i]\n",
    "\n",
    "    # 이미지 형태와 데이터 타입 검증\n",
    "    if image.shape != expected_image_shape or image.dtype != np.uint8:\n",
    "        print(f\"이미지 {i}의 형태 또는 데이터 타입이 잘못되었습니다: 형태={image.shape}, 데이터 타입={image.dtype}\")\n",
    "\n",
    "    # 바운딩 박스 형태와 데이터 타입 검증\n",
    "    if bbox.shape != expected_bbox_shape or bbox.dtype != np.float64:\n",
    "        print(f\"바운딩 박스 {i}의 형태 또는 데이터 타입이 잘못되었습니다: 형태={bbox.shape}, 데이터 타입={bbox.dtype}\")\n",
    "\n",
    "    # # 레이블 길이와 데이터 타입 검증\n",
    "    # if len(label) != expected_label_length or not all(isinstance(x, int) for x in label):\n",
    "    #     print(f\"레이블 {i}의 길이 또는 데이터 타입이 잘못되었습니다: 길이={len(label)}, 레이블={label}\")\n",
    "\n",
    "print(images.shape, numbers.shape, bboxes.shape, len(labels))\n",
    "\n",
    "print(images.max(), images.min())\n",
    "print(bboxes[9000:9010])\n",
    "print(labels[9000:9010])\n",
    "\n",
    "\n",
    "dataset = {\n",
    "    'images' : images_filtered,\n",
    "    'bboxes' : bboxes_filtered,\n",
    "    'cls' : labels_filtered\n",
    "}\n",
    "\n",
    "print(dataset['images'].shape)\n",
    "print(dataset['bboxes'].shape)\n",
    "print(dataset['cls'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 12:23:16.593468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n",
      "24\n",
      "24\n",
      "tf.Tensor(\n",
      "[[11.  0. 20.  4.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(4, 4), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 12:23:26.594325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22292 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1d:00.0, compute capability: 8.6\n",
      "2024-01-22 12:23:26.598171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22292 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1f:00.0, compute capability: 8.6\n",
      "2024-01-22 12:23:26.600151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22292 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:20:00.0, compute capability: 8.6\n",
      "2024-01-22 12:23:26.603294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22292 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n",
      "2024-01-22 12:23:26.605244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 22292 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:22:00.0, compute capability: 8.6\n",
      "2024-01-22 12:23:26.608576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 22292 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:23:00.0, compute capability: 8.6\n",
      "2024-01-22 12:23:26.612428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 22292 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:24:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "images = dataset['images']\n",
    "# numbers =dataset['numbers']\n",
    "bboxes = dataset['bboxes']\n",
    "cls = dataset['cls']\n",
    "\n",
    "boxes = bboxes[0]\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.axis('off')\n",
    "image = images[0]\n",
    "print(image.shape)\n",
    "print(image.shape[0])\n",
    "print(image.shape[0])\n",
    "plt.imshow(images[0])\n",
    "ax = plt.gca()\n",
    "boxes = tf.stack([\n",
    "\tboxes[:, 0] * images.shape[2],\n",
    "\tboxes[:, 1] * images.shape[1],\n",
    "\tboxes[:, 2] * images.shape[2],\n",
    "\tboxes[:, 3] * images.shape[1]], axis = -1\n",
    ")\n",
    "\n",
    "print(boxes)\n",
    "\n",
    "for box in boxes:\n",
    "\txmin, ymin = box[:2]\n",
    "\tw, h = box[2:] - box[:2]\n",
    "\tpatch = plt.Rectangle(\n",
    "\t\t[xmin, ymin], w, h, fill = False, edgecolor = [1, 0, 0], linewidth = 2\n",
    "\t)\n",
    "\tax.add_patch(patch)\n",
    "plt.show()\n",
    "print(cls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_SIZE_WIDTH:   32\n",
      "IMG_SIZE_HEIGHT:  24\n",
      "N_DATA:           12880\n",
      "N_TRAIN:          12880\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE_WIDTH = images.shape[2]\n",
    "IMG_SIZE_HEIGHT = images.shape[1]\n",
    "N_DATA = images.shape[0]\n",
    "N_TRAIN = images.shape[0]\n",
    "# N_VAL = images.shape[0] - N_TRAIN\n",
    "LOG_DIR = 'ObjectDetectionLog'\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'tfrecord/ObjectDetection')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "print(\"IMG_SIZE_WIDTH:  \", IMG_SIZE_WIDTH)\n",
    "print(\"IMG_SIZE_HEIGHT: \", IMG_SIZE_HEIGHT)\n",
    "print(\"N_DATA:          \", N_DATA)\n",
    "print(\"N_TRAIN:         \", N_TRAIN)\n",
    "\n",
    "\n",
    "\n",
    "shuffle_list = list(range(N_DATA))\n",
    "random.shuffle(shuffle_list)\n",
    "\n",
    "train_idx_list = shuffle_list[:N_TRAIN]\n",
    "# val_idx_list = shuffle_list[N_TRAIN:]\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'tfrecord/ObjectDetection')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')\n",
    "# tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')\n",
    "\n",
    "writer_train = tf.io.TFRecordWriter(tfr_train_dir)\n",
    "# writer_val = tf.io.TFRecordWriter(tfr_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.max(), images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list = tf.train.FloatList(value = value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list = tf.train.Int64List(value = [value]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in train_idx_list:\n",
    "    bbox = bboxes[idx]\n",
    "    xmin, ymin, xmax, ymax = bbox[:, 0], bbox[:, 1], bbox[:, 2], bbox[:, 3]\n",
    "    bbox = np.stack([xmin, ymin, xmax, ymax], axis=-1).flatten()\n",
    "\n",
    "    image = images[idx]\n",
    "    bimage = image.tobytes()\n",
    "\n",
    "    number = numbers[idx]\n",
    "    class_id = cls[idx]\n",
    "    # print(len(cls))\n",
    "    serialized_cls = tf.io.serialize_tensor(tf.constant(class_id, dtype=tf.int32)).numpy()\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image': _bytes_feature(bimage),\n",
    "        'bbox': _float_feature(bbox),\n",
    "        'label': _bytes_feature(serialized_cls),\n",
    "        # 'number': _int64_feature(number)\n",
    "    }))\n",
    "    \n",
    "    writer_train.write(example.SerializeToString())\n",
    "writer_train.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "RES_HEIGHT =24\n",
    "RES_WIDTH = 32\n",
    "# N_EPOCHS = 100\n",
    "# N_BATCH = 8\n",
    "# LR = 0.0005\n",
    "\n",
    "\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    features = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'bbox': tf.io.VarLenFeature(tf.float32),  \n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'number': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "\n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, [RES_HEIGHT, RES_WIDTH, 1])\n",
    "    image = image / tf.reduce_max(image)\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    # image = image / tf.reduce_max(image)\n",
    "\n",
    "    bbox = tf.sparse.to_dense(parsed_features['bbox']) \n",
    "    bbox = tf.cast(bbox, tf.float32)\n",
    "    # num_boxes = tf.shape(bbox)[0] // 4\n",
    "    bbox = tf.reshape(bbox, [-1, 4])\n",
    "\n",
    "    serialized_cls = parsed_features['label']\n",
    "    label = tf.io.parse_tensor(serialized_cls, out_type=tf.int32)\n",
    "    \n",
    "    # number = tf.cast(parsed_features['number'], tf.int64)\n",
    "    return image, bbox, label\n",
    "\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(AUTOTUNE).batch(N_BATCH, drop_remainder=True)\n",
    "\n",
    "# val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
    "# val_dataset = val_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    image = image.numpy()\n",
    "    print(image.shape)\n",
    "    # print(label)\n",
    "    # # plt.axis('off')\n",
    "    # plt.imshow(image)\n",
    "    # ax = plt.gca()  \n",
    "    # print(bbox)\n",
    "\n",
    "    # boxes = tf.stack(\n",
    "    # \t[\n",
    "    # \t bbox[:,0] * image.shape[1],\n",
    "    # \t bbox[:,1] * image.shape[0],\n",
    "    # \t bbox[:,2] * image.shape[1],\n",
    "    # \t bbox[:,3] * image.shape[0]\n",
    "    # \t], axis = -1\n",
    "    # )\n",
    "    # print(image.shape)\n",
    "    # print(boxes)\n",
    "    # for box in boxes:\n",
    "    #     xmin, ymin = box[:2]\n",
    "    #     w, h = box[2:] - box[:2]\n",
    "    #     patch = plt.Rectangle(\n",
    "    #         [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "    #     )\n",
    "    #     ax.add_patch(patch)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):    \n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_corners(boxes):\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.09375    0.20833333 0.4375     0.5833333 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.265625  0.3958333 0.34375   0.375    ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    print(bbox)\n",
    "    print(convert_to_xywh(bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_image(image, min_side=96, max_side=128):\n",
    "    \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype = tf.float32)\n",
    "    # print(f\"image_shape: {image_shape}\")\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    # print(f\"ratio: {ratio}\")\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "      ratio = max_side / tf.reduce_max(image_shape)\n",
    "    # print(f\"ratio: {ratio}\")\n",
    "\n",
    "    new_image_shape = ratio * image_shape\n",
    "    # print(f\"new_image_shape: {new_image_shape}\")\n",
    "\n",
    "    image = tf.image.resize(image, \n",
    "                            tf.cast(new_image_shape, dtype=tf.int32), \n",
    "                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # print(f\"image: {image.shape}\")\n",
    "\n",
    "    return image, new_image_shape, ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image, gt_boxes, cls_ids):\n",
    "    # image = sample[\"images\"]\n",
    "    # bbox = sample[\"bboxes\"]\n",
    "    cls_ids = tf.cast(cls_ids, dtype = tf.int32)\n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "    \n",
    "    bbox = tf.stack([\n",
    "        gt_boxes[:, 0] * image_shape[1],\n",
    "        gt_boxes[:, 1] * image_shape[0],\n",
    "        gt_boxes[:, 2] * image_shape[1],\n",
    "        gt_boxes[:, 3] * image_shape[0]],\n",
    "        axis = -1\n",
    "    )\n",
    "    \n",
    "    bbox = convert_to_xywh(bbox)\n",
    "    print(image.shape)\n",
    "    print(bbox.shape)\n",
    "    print(cls_ids.shape)\n",
    "    return image, bbox, cls_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    resize_and_pad_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 0 0], shape=(4,), dtype=int32)\n",
      "(96, 128, 1)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(96, 128, 1)\n",
      "(96, 128, 3)\n",
      "tf.Tensor(\n",
      "[[34. 38. 44. 36.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(4, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    print(label)\n",
    "    img, bbox, class_id = preprocess_data(image, bbox, label)\n",
    "    print(img.shape) # (72, 96, 1)\n",
    "\n",
    "    anchor_img = np.zeros((*img.shape[:2], 3), dtype=np.uint8)\n",
    "    print(anchor_img.shape)\n",
    "\n",
    "    print(bbox)\n",
    "\n",
    "    strides = [4, 8, 16, 32]\n",
    "    colors = {\n",
    "        4: [0, 255, 0],  # 초록색\n",
    "        8: [0, 0, 255],  # 파란색\n",
    "        16: [255, 0, 0],   # 빨간색\n",
    "        32:[255, 255, 255],  # 노란색\n",
    "        # 64:[255, 255, 0],  # 노란색\n",
    "    }\n",
    "\n",
    "    for stride in strides:\n",
    "        color = colors[stride]\n",
    "        for y in range(0, anchor_img.shape[0], stride):\n",
    "            for x in range(0, anchor_img.shape[1], stride):\n",
    "                anchor_img[y, x, :] = color\n",
    "\n",
    "    # 이미지 표시\n",
    "    plt.imshow(img, alpha=1)  \n",
    "    plt.imshow(anchor_img, alpha=0.5) \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 128, 1)\n",
      "(4, 4)\n",
      "(4,)\n",
      "tf.Tensor([1 0 0 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[34. 38. 44. 36.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "width:  128\n",
      "height:  96\n",
      "bbox:  tf.Tensor(\n",
      "[[12. 20. 56. 56.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 0 0], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for image, bbox, label in train_dataset.take(1):\n",
    "    img, box, label = preprocess_data(image, bbox, label)\n",
    "    print(label)\n",
    "    print(box)\n",
    "    print(tf.reduce_max(image), tf.reduce_min(image))\n",
    "    # 이미지 시각화\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    print(\"width: \", width)\n",
    "    print(\"height: \", height)\n",
    "    boxes = tf.stack(\n",
    "        [\n",
    "            (box[:, 0] - 0.5 * box[:, 2])  ,  # xmin = x_center - width/2\n",
    "            (box[:, 1] - 0.5 * box[:, 3])  ,  # ymin = y_center - height/2\n",
    "            (box[:, 0] + 0.5 * box[:, 2])  ,  # xmax = x_center + width/2\n",
    "            (box[:, 1] + 0.5 * box[:, 3])     # ymax = y_center + height/2\n",
    "        ], axis=-1\n",
    "    )\n",
    "    print(\"bbox: \", boxes)\n",
    "    # 각 바운딩 박스에 대해 반복하여 그리기\n",
    "    for box in boxes[2:3]:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        # print(\"xmin, ymin: \", xmin, ymin)\n",
    "        print(box)\n",
    "        w, h = xmax - xmin, ymax - ymin\n",
    "        patch = plt.Rectangle(\n",
    "            [xmin, ymin], w, h, fill=False, edgecolor=[1, 0, 0], linewidth=2\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    plt.show()\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBox:\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]        \n",
    "        self.scales = [2** x for x in [0, 1/3, 2/3]]\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(2, 10)]\n",
    "        self._areas = [x ** 2 for x in [4.0, 8.0, 16.0, 32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        anchor_dims_all = []\n",
    "\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios: \n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis = -1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales: \n",
    "                    anchor_dims.append(scale * dims) \n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis = -2))\n",
    "        return anchor_dims_all \n",
    "    \n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        rx = tf.range(feature_width, dtype = tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype = tf.float32) + 0.5\n",
    "\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis = -1) * self._strides[level - 2]\n",
    "        # print(f\"centers: {centers}\")\n",
    "        centers = tf.expand_dims(centers, axis = -2)\n",
    "        # print(f\"centers: {centers}\")\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        # print(f\"centers: {centers}\")\n",
    "\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 2], [feature_height, feature_width, 1, 1] \n",
    "        )\n",
    "        # print(f\"dims: {dims}\")\n",
    "\n",
    "        \n",
    "        anchors = tf.concat([centers, dims], axis=-1) \n",
    "        # print(f\"anchors: {anchors}\")\n",
    "\n",
    "        # print(f\"{tf.reshape(anchors, [feature_height * feature_width * self._num_anchors, 4]).shape}\")\n",
    "\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i), # 올림\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i\n",
    "            )\n",
    "            for i in range(2, 10)\n",
    "        ]\n",
    "\n",
    "        anchors = tf.concat(anchors, axis=0)\n",
    "\n",
    "        # 앵커 박스의 좌표를 이미지 크기 내로 제한\n",
    "        anchors = tf.clip_by_value(anchors, 0, [image_height, image_width, image_height, image_width])\n",
    "        return tf.concat(anchors, axis=0)\n",
    "        # return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor 음수 값: False\n",
      "(24648, 4)\n",
      "[[22.        94.         5.4989185  7.331891 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anchors = AnchorBox()\n",
    "anchor = anchors.get_anchors(96, 128)\n",
    "\n",
    "has_negative_values = tf.reduce_any(tf.less(anchor, 0))\n",
    "print(\"Anchor 음수 값:\", has_negative_values.numpy())\n",
    "\n",
    "# print(anchor)\n",
    "print(anchor.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bounding_boxes(data, num_samples):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    # print(img.shape)\n",
    "    data_np = data.numpy()\n",
    "\n",
    "    if len(data) > num_samples:\n",
    "        sampled_indices = np.random.choice(len(data), num_samples, replace=False)\n",
    "        sample_data = data_np[sampled_indices]\n",
    "    else : \n",
    "        sample_data = data_np\n",
    "    print(sample_data)\n",
    "    for center_x, center_y, width, height in sample_data:\n",
    "        top_left_x = center_x - width / 2\n",
    "        top_left_y = center_y - height / 2\n",
    "\n",
    "        rect = patches.Rectangle((top_left_x, top_left_y), width, height, linewidth=0.8, edgecolor='white', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "draw_bounding_boxes(anchor, 1)\n",
    "\n",
    "\n",
    "# 24192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    print(\"boxes1: \", boxes1)\n",
    "    print(\"boxes1_corners: \", boxes1_corners)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    print(\"boxes2: \", boxes2)\n",
    "    print(\"boxes2_corners: \", boxes2_corners)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    print(\"lu: \", lu)\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])  \n",
    "    print(\"rd: \", rd)\n",
    "\n",
    "    \n",
    "\n",
    "    intersection = tf.maximum(rd - lu, 0.0)\n",
    "    print(\"intersection: \", intersection)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    print(\"intersection_area: \", intersection_area)\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8)\n",
    "    print(\"union_area: \", union_area)\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[62. 54. 78. 70.]], shape=(1, 4), dtype=float64)\n",
      "tf.Tensor([[60. 52. 92. 92.]], shape=(1, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "b1 = np.array([[70, 62, 16, 16]]) \n",
    "b1c = convert_to_corners(b1)\n",
    "print(b1c)\n",
    "\n",
    "b2 = np.array([[76, 72, 32, 40]])\n",
    "b2c = convert_to_corners(b2)\n",
    "print(b2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# 주어진 바운딩 박스 데이터\n",
    "box1 = [62, 54, 78, 70]  # [x_min, y_min, x_max, y_max]\n",
    "box2 = [60, 52, 82, 82]\n",
    "\n",
    "# 그림 생성\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 첫 번째 바운딩 박스 추가\n",
    "rect1 = patches.Rectangle((box1[0], box1[1]), box1[2] - box1[0], box1[3] - box1[1], \n",
    "                          linewidth=2, edgecolor='blue', facecolor='none')\n",
    "ax.add_patch(rect1)\n",
    "\n",
    "# 두 번째 바운딩 박스 추가\n",
    "rect2 = patches.Rectangle((box2[0], box2[1]), box2[2] - box2[0], box2[3] - box2[1], \n",
    "                          linewidth=2, edgecolor='red', facecolor='none')\n",
    "ax.add_patch(rect2)\n",
    "\n",
    "# 축 범위 설정\n",
    "ax.set_xlim(0, 90)\n",
    "ax.set_ylim(0, 90)\n",
    "\n",
    "# 그림 표시\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes1:  [[70 62 16 16]]\n",
      "boxes1_corners:  tf.Tensor([[62. 54. 78. 70.]], shape=(1, 4), dtype=float64)\n",
      "boxes2:  [[76 72 32 40]]\n",
      "boxes2_corners:  tf.Tensor([[60. 52. 92. 92.]], shape=(1, 4), dtype=float64)\n",
      "lu:  tf.Tensor([[[62. 54.]]], shape=(1, 1, 2), dtype=float64)\n",
      "rd:  tf.Tensor([[[78. 70.]]], shape=(1, 1, 2), dtype=float64)\n",
      "intersection:  tf.Tensor([[[16. 16.]]], shape=(1, 1, 2), dtype=float64)\n",
      "intersection_area:  tf.Tensor([[256.]], shape=(1, 1), dtype=float64)\n",
      "union_area:  tf.Tensor([[1280.]], shape=(1, 1), dtype=float64)\n",
      "tf.Tensor([[0.2]], shape=(1, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "GA = np.array([[70, 62, 16, 16]])\n",
    "GT = np.array([[76, 72, 32, 40]])\n",
    "\n",
    "print(compute_iou(GA, GT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    \n",
    "    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou = 0.5, ignore_iou = 0.4):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        print(\"iou_matrix:  \", iou_matrix)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        print(\"max_iou:  \", max_iou)\n",
    "\n",
    "\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis = 1)\n",
    "        print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "\n",
    "    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        print(\"positive_mask:  \", positive_mask)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        print(\"negative_mask:  \", negative_mask)\n",
    "\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        print(\"ignore_mask:  \", ignore_mask)\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype = tf.float32),\n",
    "            tf.cast(ignore_mask, dtype = tf.float32),\n",
    "        )\n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:])\n",
    "            ],\n",
    "            axis = -1,\n",
    "        )\n",
    "        print(\"box_target:  \", box_target)\n",
    "        box_target = box_target / self._box_variance\n",
    "        print(\"box_target:  \", box_target)\n",
    "        return box_target\n",
    "    \n",
    "\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        print(\"anchor_boxes  : \", anchor_boxes)\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        print(\"cls_ids\", cls_ids)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        print(\"matched_gt_idx:  \", matched_gt_idx)\n",
    "        print(\"positive_mask:  \", positive_mask)\n",
    "        print(\"ignore_mask:  \", ignore_mask)\n",
    "\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "\n",
    "        print(\"matched_gt_boxes:  \", matched_gt_boxes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        print(\"box_target:  \", box_target)\n",
    "\n",
    "\n",
    "\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        print(\"matched_gt_cls_ids:  \", matched_gt_cls_ids)\n",
    "        \n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
    "        )\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        print(\"cls_target:  \", cls_target)\n",
    "\n",
    "\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        print(\"label:  \", label)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):       \n",
    "        images_shape = tf.shape(batch_images)\n",
    "        print(\"images_shape:  \", images_shape)\n",
    "        batch_size = images_shape[0]\n",
    "        print(\"batch_size:  \", batch_size)\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        print(\"labels:  \", labels)\n",
    "        # batch_size_val = batch_size.numpy()\n",
    "        for i in range(1):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            print(\"label:  \", label)\n",
    "            labels = labels.write(i, label)\n",
    "        return batch_images, labels.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Eager execution: \", tf.executing_eagerly())\n",
    "if not tf.executing_eagerly():\n",
    "    tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 128, 1)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(1, 96, 128, 1) (1, 4, 4) (1, 4)\n",
      "images_shape:   tf.Tensor([  1  96 128   1], shape=(4,), dtype=int32)\n",
      "batch_size:   tf.Tensor(1, shape=(), dtype=int32)\n",
      "labels:   <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd8b87ce190>\n",
      "anchor_boxes  :  tf.Tensor(\n",
      "[[  2.          2.          2.          8.       ]\n",
      " [  2.          2.          2.5198421  10.079369 ]\n",
      " [  2.          2.          3.174802   12.699208 ]\n",
      " ...\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]], shape=(24648, 4), dtype=float32)\n",
      "cls_ids tf.Tensor([1. 0. 0. 0.], shape=(4,), dtype=float32)\n",
      "boxes1:  tf.Tensor(\n",
      "[[  2.          2.          2.          8.       ]\n",
      " [  2.          2.          2.5198421  10.079369 ]\n",
      " [  2.          2.          3.174802   12.699208 ]\n",
      " ...\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]\n",
      " [ 96.        128.         96.        128.       ]], shape=(24648, 4), dtype=float32)\n",
      "boxes1_corners:  tf.Tensor(\n",
      "[[  1.          -2.           3.           6.        ]\n",
      " [  0.7400789   -3.0396843    3.259921     7.0396843 ]\n",
      " [  0.41259897  -4.349604     3.587401     8.349604  ]\n",
      " ...\n",
      " [ 48.          64.         144.         192.        ]\n",
      " [ 48.          64.         144.         192.        ]\n",
      " [ 48.          64.         144.         192.        ]], shape=(24648, 4), dtype=float32)\n",
      "boxes2:  [[34. 38. 44. 36.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "boxes2_corners:  tf.Tensor(\n",
      "[[12. 20. 56. 56.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(4, 4), dtype=float32)\n",
      "lu:  tf.Tensor(\n",
      "[[[12.         20.        ]\n",
      "  [ 1.          0.        ]\n",
      "  [ 1.          0.        ]\n",
      "  [ 1.          0.        ]]\n",
      "\n",
      " [[12.         20.        ]\n",
      "  [ 0.7400789   0.        ]\n",
      "  [ 0.7400789   0.        ]\n",
      "  [ 0.7400789   0.        ]]\n",
      "\n",
      " [[12.         20.        ]\n",
      "  [ 0.41259897  0.        ]\n",
      "  [ 0.41259897  0.        ]\n",
      "  [ 0.41259897  0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[48.         64.        ]\n",
      "  [48.         64.        ]\n",
      "  [48.         64.        ]\n",
      "  [48.         64.        ]]\n",
      "\n",
      " [[48.         64.        ]\n",
      "  [48.         64.        ]\n",
      "  [48.         64.        ]\n",
      "  [48.         64.        ]]\n",
      "\n",
      " [[48.         64.        ]\n",
      "  [48.         64.        ]\n",
      "  [48.         64.        ]\n",
      "  [48.         64.        ]]], shape=(24648, 4, 2), dtype=float32)\n",
      "rd:  tf.Tensor(\n",
      "[[[ 3.         6.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[ 3.259921   7.0396843]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[ 3.587401   8.349604 ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[56.        56.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[56.        56.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]]\n",
      "\n",
      " [[56.        56.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]\n",
      "  [ 0.         0.       ]]], shape=(24648, 4, 2), dtype=float32)\n",
      "intersection:  tf.Tensor(\n",
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[8. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[8. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[8. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]], shape=(24648, 4, 2), dtype=float32)\n",
      "intersection_area:  tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(24648, 4), dtype=float32)\n",
      "union_area:  tf.Tensor(\n",
      "[[ 1600.          16.          16.          16.      ]\n",
      " [ 1609.3984      25.398418    25.398418    25.398418]\n",
      " [ 1624.3175      40.317474    40.317474    40.317474]\n",
      " ...\n",
      " [13872.       12288.       12288.       12288.      ]\n",
      " [13872.       12288.       12288.       12288.      ]\n",
      " [13872.       12288.       12288.       12288.      ]], shape=(24648, 4), dtype=float32)\n",
      "iou_matrix:   tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(24648, 4), dtype=float32)\n",
      "max_iou:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(24648,), dtype=float32)\n",
      "matched_gt_idx:   tf.Tensor([0 0 0 ... 0 0 0], shape=(24648,), dtype=int64)\n",
      "positive_mask:   tf.Tensor([False False False ... False False False], shape=(24648,), dtype=bool)\n",
      "negative_mask:   tf.Tensor([ True  True  True ...  True  True  True], shape=(24648,), dtype=bool)\n",
      "ignore_mask:   tf.Tensor([False False False ... False False False], shape=(24648,), dtype=bool)\n",
      "matched_gt_idx:   tf.Tensor([0 0 0 ... 0 0 0], shape=(24648,), dtype=int64)\n",
      "positive_mask:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(24648,), dtype=float32)\n",
      "ignore_mask:   tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(24648,), dtype=float32)\n",
      "matched_gt_boxes:   tf.Tensor(\n",
      "[[34. 38. 44. 36.]\n",
      " [34. 38. 44. 36.]\n",
      " [34. 38. 44. 36.]\n",
      " ...\n",
      " [34. 38. 44. 36.]\n",
      " [34. 38. 44. 36.]\n",
      " [34. 38. 44. 36.]], shape=(24648, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[16.         4.5        3.0910425  1.5040774]\n",
      " [12.699208   3.5716524  2.8599935  1.2730284]\n",
      " [10.079369   2.8348224  2.6289444  1.0419793]\n",
      " ...\n",
      " [-0.6458333 -0.703125  -0.7801585 -1.2685113]\n",
      " [-0.6458333 -0.703125  -0.7801585 -1.2685113]\n",
      " [-0.6458333 -0.703125  -0.7801585 -1.2685113]], shape=(24648, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[160.         45.         15.455213    7.520387 ]\n",
      " [126.99208    35.716522   14.299967    6.365142 ]\n",
      " [100.793686   28.348225   13.144722    5.2098966]\n",
      " ...\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565]], shape=(24648, 4), dtype=float32)\n",
      "box_target:   tf.Tensor(\n",
      "[[160.         45.         15.455213    7.520387 ]\n",
      " [126.99208    35.716522   14.299967    6.365142 ]\n",
      " [100.793686   28.348225   13.144722    5.2098966]\n",
      " ...\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565]], shape=(24648, 4), dtype=float32)\n",
      "matched_gt_cls_ids:   tf.Tensor([1. 1. 1. ... 1. 1. 1.], shape=(24648,), dtype=float32)\n",
      "cls_target:   tf.Tensor([-1. -1. -1. ... -1. -1. -1.], shape=(24648,), dtype=float32)\n",
      "cls_target:   tf.Tensor([-1. -1. -1. ... -1. -1. -1.], shape=(24648,), dtype=float32)\n",
      "cls_target:   tf.Tensor(\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]], shape=(24648, 1), dtype=float32)\n",
      "label:   tf.Tensor(\n",
      "[[160.         45.         15.455213    7.520387   -1.       ]\n",
      " [126.99208    35.716522   14.299967    6.365142   -1.       ]\n",
      " [100.793686   28.348225   13.144722    5.2098966  -1.       ]\n",
      " ...\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565  -1.       ]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565  -1.       ]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565  -1.       ]], shape=(24648, 5), dtype=float32)\n",
      "label:   tf.Tensor(\n",
      "[[160.         45.         15.455213    7.520387   -1.       ]\n",
      " [126.99208    35.716522   14.299967    6.365142   -1.       ]\n",
      " [100.793686   28.348225   13.144722    5.2098966  -1.       ]\n",
      " ...\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565  -1.       ]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565  -1.       ]\n",
      " [ -6.458333   -7.03125    -3.9007926  -6.3425565  -1.       ]], shape=(24648, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for image, bbox, label in train_dataset.take(1):\n",
    "    img, box, label = preprocess_data(image, bbox, label)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    box = np.expand_dims(box, axis = 0)\n",
    "    label = np.expand_dims(label, axis = 0)\n",
    "\n",
    "    print(img.shape, box.shape, label.shape)\n",
    "    label_encoder.encode_batch(img, box, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_axis(img, box, label):\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    box = np.expand_dims(box, axis = 0)\n",
    "    label = np.expand_dims(label, axis = 0)\n",
    "    return img, box, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 1)\n",
      "(None, 4)\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "autotune = tf.data.AUTOTUNE\n",
    "num_classes = 1\n",
    "batch_size = 5\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# train_dataset = train_dataset.shuffle(batch_size * 8)\n",
    "\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "    batch_size=batch_size, \n",
    "    padded_shapes = ([96, 128, 1], [4, 4], [4]),\n",
    "    padding_values=(0.0, 1e-8, -1), \n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "\n",
    "# val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "# val_dataset = val_dataset.padded_batch(\n",
    "#     batch_size=batch_size, \n",
    "#     padded_shapes = ([96, 128, 1], [4, 4], [4]),\n",
    "#     padding_values=(0.0, 1e-8, 0), \n",
    "#     drop_remainder=True\n",
    "# )\n",
    "# val_dataset = val_dataset.map(\n",
    "#     label_encoder.encode_batch, num_parallel_calls=autotune\n",
    "# )\n",
    "\n",
    "# val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_shape:   Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "batch_size:   Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "labels:   <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd8b868bee0>\n",
      "anchor_boxes  :  Tensor(\"concat_9/concat:0\", shape=(24648, 4), dtype=float32)\n",
      "cls_ids Tensor(\"Cast:0\", shape=(4,), dtype=float32)\n",
      "boxes1:  Tensor(\"concat_9/concat:0\", shape=(24648, 4), dtype=float32)\n",
      "boxes1_corners:  Tensor(\"concat_10:0\", shape=(24648, 4), dtype=float32)\n",
      "boxes2:  Tensor(\"strided_slice_1:0\", shape=(4, 4), dtype=float32)\n",
      "boxes2_corners:  Tensor(\"concat_11:0\", shape=(4, 4), dtype=float32)\n",
      "lu:  Tensor(\"Maximum:0\", shape=(24648, 4, 2), dtype=float32)\n",
      "rd:  Tensor(\"Minimum:0\", shape=(24648, 4, 2), dtype=float32)\n",
      "intersection:  Tensor(\"Maximum_1:0\", shape=(24648, 4, 2), dtype=float32)\n",
      "intersection_area:  Tensor(\"mul_24:0\", shape=(24648, 4), dtype=float32)\n",
      "union_area:  Tensor(\"Maximum_2:0\", shape=(24648, 4), dtype=float32)\n",
      "iou_matrix:   Tensor(\"clip_by_value_1:0\", shape=(24648, 4), dtype=float32)\n",
      "max_iou:   Tensor(\"Max:0\", shape=(24648,), dtype=float32)\n",
      "matched_gt_idx:   Tensor(\"ArgMax:0\", shape=(24648,), dtype=int64)\n",
      "positive_mask:   Tensor(\"GreaterEqual:0\", shape=(24648,), dtype=bool)\n",
      "negative_mask:   Tensor(\"Less:0\", shape=(24648,), dtype=bool)\n",
      "ignore_mask:   Tensor(\"LogicalNot:0\", shape=(24648,), dtype=bool)\n",
      "matched_gt_idx:   Tensor(\"ArgMax:0\", shape=(24648,), dtype=int64)\n",
      "positive_mask:   Tensor(\"Cast_1:0\", shape=(24648,), dtype=float32)\n",
      "ignore_mask:   Tensor(\"Cast_2:0\", shape=(24648,), dtype=float32)\n",
      "matched_gt_boxes:   Tensor(\"GatherV2:0\", shape=(24648, 4), dtype=float32)\n",
      "box_target:   Tensor(\"concat_12:0\", shape=(24648, 4), dtype=float32)\n",
      "box_target:   Tensor(\"truediv_23:0\", shape=(24648, 4), dtype=float32)\n",
      "box_target:   Tensor(\"truediv_23:0\", shape=(24648, 4), dtype=float32)\n",
      "matched_gt_cls_ids:   Tensor(\"GatherV2_1:0\", shape=(24648,), dtype=float32)\n",
      "cls_target:   Tensor(\"SelectV2:0\", shape=(24648,), dtype=float32)\n",
      "cls_target:   Tensor(\"SelectV2_1:0\", shape=(24648,), dtype=float32)\n",
      "cls_target:   Tensor(\"ExpandDims_8:0\", shape=(24648, 1), dtype=float32)\n",
      "label:   Tensor(\"concat_13:0\", shape=(24648, 5), dtype=float32)\n",
      "label:   Tensor(\"concat_13:0\", shape=(24648, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
    ")\n",
    "# train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 24648, 5)\n",
      "Positive 개수: 0\n",
      "Negative 개수: 24643\n",
      "Ignore 개수: 5\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "ignore_count = []\n",
    "for batch in train_dataset.take(1):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    print(labels.shape)\n",
    "\n",
    "    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 1.0), tf.int32))\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "#     print(\"원본 레이블 shape:\", labels.shape)\n",
    "\n",
    "#     # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "#     mask = tf.not_equal(labels[:, :, 4], -2.0)\n",
    "#     filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "#     # 필터링된 레이블을 원하는 크기로 조절\n",
    "#     # 예: 36288개로 조절\n",
    "#     desired_count = 36288\n",
    "#     filtered_labels = filtered_labels[:desired_count, :]\n",
    "#     print(\"조정된 레이블 shape:\", filtered_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cut_labels(images, labels):\n",
    "#     # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "#     mask = tf.not_equal(labels[:, 4], -2.0)\n",
    "#     filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "#     # 필터링된 레이블을 원하는 크기로 조절\n",
    "#     desired_count = 36288\n",
    "#     # 주의: 여기서 filtered_labels의 크기가 desired_count보다 작을 수 있으므로, 적절한 처리가 필요합니다.\n",
    "#     if tf.shape(filtered_labels)[0] > desired_count:\n",
    "#         filtered_labels = filtered_labels[:desired_count, :]\n",
    "\n",
    "#     return images, filtered_labels\n",
    "\n",
    "\n",
    "def cut_labels(images, labels):\n",
    "    # ignore 상태가 아닌 앵커 박스만 필터링\n",
    "    mask = tf.not_equal(labels[:, 4], -2.0)\n",
    "    filtered_labels = tf.boolean_mask(labels, mask)\n",
    "\n",
    "    # 필터링된 레이블을 원하는 크기로 조절\n",
    "    desired_count = 24192\n",
    "    current_count = tf.shape(filtered_labels)[0]\n",
    "\n",
    "    # 필요한 경우 레이블을 패딩\n",
    "    if current_count < desired_count:\n",
    "        # 누락된 개수만큼 ignore 상태(-2.0)로 패딩\n",
    "        padding_count = desired_count - current_count\n",
    "        padding = tf.fill([padding_count, 5], -2.0)  # -2.0으로 채워진 텐서 생성\n",
    "        filtered_labels = tf.concat([filtered_labels, padding], axis=0)\n",
    "    else:\n",
    "        filtered_labels = filtered_labels[:desired_count, :]\n",
    "\n",
    "    return images, filtered_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 원본 train_dataset에 process_labels 함수 적용\n",
    "train_dataset = train_dataset.unbatch().map(cut_labels).batch(batch_size)\n",
    "# val_dataset = val_dataset.unbatch().map(cut_labels).batch(batch_size)\n",
    "\n",
    "# buffer_size = 1000\n",
    "# 필요한 경우 배치 크기, 셔플, 반복 등을 적용\n",
    "# train_dataset = new_train_dataset.batch(batch_size).shuffle(buffer_size).repeat()\n",
    "# val_dataset = new_val_dataset.batch(batch_size).shuffle(buffer_size).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(160.0, shape=(), dtype=float32) tf.Tensor(-310.0, shape=(), dtype=float32)\n",
      "Positive 개수: 0\n",
      "Negative 개수: 24192\n",
      "Ignore 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "positive_count = []\n",
    "negative_count = []\n",
    "ignore_count = []\n",
    "for batch in train_dataset.take(1):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    labels[:, :, 4:]\n",
    "    print(tf.reduce_max(images), tf.reduce_min(images))\n",
    "    print(tf.reduce_max(labels), tf.reduce_min(labels))\n",
    "\n",
    "    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산\n",
    "    positive_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 1.0), tf.int32))\n",
    "    \n",
    "    # pad_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], 2.0), tf.int32))\n",
    "\n",
    "    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -1.0), tf.int32))\n",
    "    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, :, 4], -2.0), tf.int32))\n",
    "\n",
    "    print(\"Positive 개수:\", positive_count.numpy())\n",
    "    print(\"Negative 개수:\", negative_count.numpy())\n",
    "    print(\"Ignore 개수:\", ignore_count.numpy())\n",
    "    # print(\"Pad 개수:\", pad_count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# # train_dataset에서 하나의 배치를 가져옵니다\n",
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "    \n",
    "#     # 첫 번째 이미지를 선택\n",
    "#     image = images[0]\n",
    "#     image_height, image_width, _ = image.shape\n",
    "\n",
    "#     # 첫 번째 이미지에 대한 레이블\n",
    "#     image_labels = labels[0]\n",
    "\n",
    "#     # 첫 번째 Positive 바운딩 박스 추출\n",
    "#     positive_boxes = image_labels[tf.equal(image_labels[:, 4], 1.0)]\n",
    "#     if tf.shape(positive_boxes)[0] > 0:  # Positive 바운딩 박스가 있는 경우에만\n",
    "#         first_box = positive_boxes[15]  # 첫 번째 바운딩 박스\n",
    "        \n",
    "#         boxes = tf.stack(\n",
    "#         [\n",
    "#             (first_box[0] - 0.5 * first_box[2])  ,  # xmin = x_center - width/2\n",
    "#             (first_box[1] - 0.5 * first_box[3])  ,  # ymin = y_center - height/2\n",
    "#             (first_box[0] + 0.5 * first_box[2])  ,  # xmax = x_center + width/2\n",
    "#             (first_box[1] + 0.5 * first_box[3])     # ymax = y_center + height/2\n",
    "#         ], axis=-1)\n",
    "\n",
    "\n",
    "#         x_center, y_center, width, height = boxes[:4]\n",
    "#         # 상대 좌표를 절대 픽셀 값으로 변환\n",
    "#         x_min = (x_center - width / 2) \n",
    "#         y_min = (y_center - height / 2) \n",
    "\n",
    "#         print(x_min)\n",
    "#         print(y_min)\n",
    "#         print(width)\n",
    "#         print(height)\n",
    "\n",
    "#         # 이미지 표시\n",
    "#         fig, ax = plt.subplots(1)\n",
    "#         ax.imshow(image.numpy())\n",
    "\n",
    "#         # 첫 번째 바운딩 박스 그리기\n",
    "#         rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(\"Positive 바운딩 박스가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# for batch in train_dataset.take(1):\n",
    "#     images, labels = batch\n",
    "    \n",
    "#     # 첫 번째 이미지를 선택\n",
    "#     image = images[0]\n",
    "#     image_height, image_width, _ = image.shape\n",
    "\n",
    "#     # 첫 번째 이미지에 대한 레이블\n",
    "#     image_labels = labels[0]\n",
    "\n",
    "#     # Positive 바운딩 박스 추출\n",
    "#     positive_boxes = image_labels[tf.equal(image_labels[:, 4], 1.0)]\n",
    "\n",
    "#     # 이미지 표시\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image.numpy())\n",
    "\n",
    "#     # Positive 바운딩 박스를 이미지에 그림\n",
    "#     for box in positive_boxes:\n",
    "#         x_center, y_center, width, height = box[:4]\n",
    "#         # 상대 좌표를 절대 픽셀 값으로 변환\n",
    "#         x_min = (x_center - width / 2) * image_width\n",
    "#         y_min = (y_center - height / 2) * image_height\n",
    "#         width *= image_width\n",
    "#         height *= image_height\n",
    "\n",
    "#         # 바운딩 박스 그리기\n",
    "#         rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Conv2D, Conv2DTranspose, Flatten, Activation\n",
    "from keras.layers import BatchNormalization, Dropout, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Add\n",
    "\n",
    "class BackBone:\n",
    "    def __init__(self):\n",
    "        self.l2_regularizer = l2(0.001)\n",
    "\n",
    "    def residual_layer(self, feature_map, latent, name:str):\n",
    "        add_layer = Add(name = name+'_output')([feature_map, latent])\n",
    "        return add_layer\n",
    "\n",
    "    def feature_extraction_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        feature_map = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name)(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        feature_map = Activation('relu')(feature_map)\n",
    "        feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        feature_map = ZeroPadding2D(padding=((0, 1), (0, 1)), name=name+'_pad')(feature_map)\n",
    "        feature_map = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 2, padding = 'valid', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(feature_map)\n",
    "        feature_map = BatchNormalization()(feature_map)\n",
    "        return feature_map\n",
    "\n",
    "    def convolutional_residual_block(self, feature_map, filters_conv1:int, filters_conv2:int, name:str):\n",
    "        latent = Conv2D(filters=filters_conv1, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name)(feature_map)\n",
    "        latent =  BatchNormalization()(latent)\n",
    "        latent = Activation('relu')(latent)\n",
    "        feature_map = Dropout(0.3)(feature_map)\n",
    "\n",
    "        latent = Conv2D(filters=filters_conv2, kernel_size = 3, strides = 1, padding = 'same', \n",
    "                        kernel_regularizer=self.l2_regularizer,\n",
    "                        name = name+'_2')(latent)\n",
    "        latent = BatchNormalization()(latent)\n",
    "        residual_block = self.residual_layer(feature_map, latent, name)\n",
    "        return residual_block\n",
    "    \n",
    "    def __call__(self, input_shape=(96, 128, 1)):\n",
    "        inputs_image = Input(shape=input_shape)\n",
    "        upsample_layer = Conv2DTranspose(filters = 16, kernel_size = 3, strides = (1, 1), padding = 'same')(inputs_image)\n",
    "        block_1 = self.feature_extraction_block(upsample_layer, 32, 64,'block_1')\n",
    "        block_1_output = self.convolutional_residual_block(block_1, 32, 64,'block_2')\n",
    "        block_2 = self.feature_extraction_block(block_1_output, 32, 64, 'block_3')\n",
    "        block_2_output = self.convolutional_residual_block(block_2, 32, 64, 'block_4')\n",
    "        block_3 = self.feature_extraction_block(block_2_output, 32, 64,'block_5')\n",
    "        block_3_output = self.convolutional_residual_block(block_3, 32, 64,'block_6')\n",
    "\n",
    "        model = Model(inputs_image, block_3_output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 128, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 96, 128, 16)          160       ['input_1[0][0]']             \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " block_1 (Conv2D)            (None, 96, 128, 32)          4640      ['conv2d_transpose[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 96, 128, 32)          128       ['block_1[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 96, 128, 32)          0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 96, 128, 32)          0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D  (None, 97, 129, 32)          0         ['dropout[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_2 (Conv2D)          (None, 48, 64, 64)           18496     ['block_1_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 48, 64, 64)           256       ['block_1_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_2 (Conv2D)            (None, 48, 64, 32)           18464     ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 48, 64, 32)           128       ['block_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 48, 64, 32)           0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_2_2 (Conv2D)          (None, 48, 64, 64)           18496     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 48, 64, 64)           0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 48, 64, 64)           256       ['block_2_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_2_output (Add)        (None, 48, 64, 64)           0         ['dropout_1[0][0]',           \n",
      "                                                                     'batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_3 (Conv2D)            (None, 48, 64, 32)           18464     ['block_2_output[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 48, 64, 32)           128       ['block_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 48, 64, 32)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 48, 64, 32)           0         ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D  (None, 49, 65, 32)           0         ['dropout_2[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_3_2 (Conv2D)          (None, 24, 32, 64)           18496     ['block_3_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 24, 32, 64)           256       ['block_3_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_4 (Conv2D)            (None, 24, 32, 32)           18464     ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 24, 32, 32)           128       ['block_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 24, 32, 32)           0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_4_2 (Conv2D)          (None, 24, 32, 64)           18496     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 24, 32, 64)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 24, 32, 64)           256       ['block_4_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_4_output (Add)        (None, 24, 32, 64)           0         ['dropout_3[0][0]',           \n",
      "                                                                     'batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " block_5 (Conv2D)            (None, 24, 32, 32)           18464     ['block_4_output[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 24, 32, 32)           128       ['block_5[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 24, 32, 32)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 24, 32, 32)           0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " block_5_pad (ZeroPadding2D  (None, 25, 33, 32)           0         ['dropout_4[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_5_2 (Conv2D)          (None, 12, 16, 64)           18496     ['block_5_pad[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 12, 16, 64)           256       ['block_5_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_6 (Conv2D)            (None, 12, 16, 32)           18464     ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 12, 16, 32)           128       ['block_6[0][0]']             \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 12, 16, 32)           0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_6_2 (Conv2D)          (None, 12, 16, 64)           18496     ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 12, 16, 64)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 12, 16, 64)           256       ['block_6_2[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " block_6_output (Add)        (None, 12, 16, 64)           0         ['dropout_5[0][0]',           \n",
      "                                                                     'batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 210400 (821.88 KB)\n",
      "Trainable params: 209248 (817.38 KB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "backbone = BackBone()\n",
    "model = backbone()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def get_backbone():\n",
    "    backbone = BackBone() \n",
    "    backbone = backbone(input_shape=[None, None, 1])\n",
    "\n",
    "    b2_output, b4_output, b6_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in ['block_2_output', 'block_4_output', 'block_6_output']\n",
    "    ]\n",
    "\n",
    "    return keras.Model(\n",
    "        inputs = [backbone.inputs], outputs=[b2_output, b4_output, b6_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_2_output (Add)        (None, 48, 64, 64)\n",
    "\n",
    "# block_4_output (Add)        (None, 24, 32, 64)\n",
    "\n",
    "# block_6_output (Add)        (None, 12, 16, 64)\n",
    "\n",
    "class FeaturePyramid(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
    "        self.backbone = get_backbone()\n",
    "        self.conv_c2_1x1 = keras.layers.Conv2D(32, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = keras.layers.Conv2D(32, 1, 1, \"same\")\n",
    "        self.conv_c6_1x1 = keras.layers.Conv2D(32, 1, 1, \"same\")\n",
    "        \n",
    "        self.conv_c2_3x3 = keras.layers.Conv2D(32, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = keras.layers.Conv2D(32, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = keras.layers.Conv2D(32, 3, 1, \"same\")\n",
    "        # self.conv_c6_3x3 = keras.layers.Conv2D(32, 3, 2, \"same\")\n",
    "        # self.conv_c7_3x3 = keras.layers.Conv2D(32, 3, 2, \"same\")\n",
    "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        b2_output, b4_output, b6_output = self.backbone(images, training=training)\n",
    "        p2_output = self.conv_c2_1x1(b2_output)\n",
    "        p4_output = self.conv_c4_1x1(b4_output)\n",
    "        p6_output = self.conv_c6_1x1(b6_output)\n",
    "        \n",
    "        p4_output = p4_output + self.upsample_2x(p6_output)\n",
    "        p2_output = p2_output + self.upsample_2x(p4_output)\n",
    "        \n",
    "        # p1_output = self.conv_c3_3x3(p1_output)\n",
    "        p2_output = self.conv_c2_3x3(p2_output)\n",
    "        p4_output = self.conv_c4_3x3(p4_output)\n",
    "        p6_output = self.conv_c6_3x3(p6_output)\n",
    "        # pn_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "\n",
    "        # print(\"b2_output shape:\", tf.shape(b2_output))\n",
    "        # print(\"b4_output shape:\", tf.shape(b4_output))\n",
    "        # print(\"b6_output shape:\", tf.shape(b6_output))\n",
    "        # print(\"p2_output shape:\", tf.shape(p2_output))\n",
    "        # print(\"p4_output shape:\", tf.shape(p4_output))\n",
    "        # print(\"p6_output shape:\", tf.shape(p6_output))\n",
    "        # print(\"pn_output shape:\", tf.shape(pn_output))\n",
    "\n",
    "             \n",
    "        return p2_output, p4_output, p6_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_head(output_filters, bias_init):\n",
    "    head = keras.Sequential([keras.Input(shape=[None, None, 32])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01, seed=1)\n",
    "\n",
    "    for _ in range(4):\n",
    "        head.add(\n",
    "            keras.layers.Conv2D(32, 3, padding = 'same', kernel_initializer=kernel_init)\n",
    "        )\n",
    "        head.add(keras.layers.ReLU())\n",
    "\n",
    "    \n",
    "    head.add(\n",
    "        keras.layers.Conv2D(\n",
    "            output_filters,\n",
    "            3,\n",
    "            1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            bias_initializer=bias_init,\n",
    "        )\n",
    "    )\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(keras.Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(CustomNet, self).__init__(name=\"CustomNet\", **kwargs)\n",
    "        self.fpn = FeaturePyramid()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(6 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(6 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=True):\n",
    "        # # # # # # print(f\"shape: {image.shape}\")\n",
    "        features = self.fpn(image, training=training)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "\n",
    "        for feature in features:\n",
    "            box_output = tf.reshape(self.box_head(feature), [N, -1, 4])\n",
    "            cls_output = tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
    "\n",
    "            # print(\"feature shape:\", tf.shape(feature))\n",
    "            # print(\"box_output shape:\", tf.shape(box_output))\n",
    "            # print(\"cls_output shape:\", tf.shape(cls_output))\n",
    "\n",
    "            box_outputs.append(box_output)\n",
    "            cls_outputs.append(cls_output)\n",
    "        \n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        # print(\"Final cls_outputs shape:\", tf.shape(cls_outputs))\n",
    "        # print(\"Final box_outputs shape:\", tf.shape(box_outputs))\n",
    "        final_output = tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        # print(\"Final output shape:\", tf.shape(final_output))\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetBoxLoss(tf.losses.Loss):  \n",
    "    def __init__(self, delta):\n",
    "        super(CustomNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"CustomNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5\n",
    "        )\n",
    "\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetClassificationLoss(tf.losses.Loss):   \n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(CustomNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"CustomNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred\n",
    "        )\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetLoss(tf.losses.Loss):    \n",
    "    def __init__(self, num_classes=1, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "        super(CustomNetLoss, self).__init__(reduction=\"auto\", name=\"CustomNetLoss\")\n",
    "        self._cls_loss = CustomNetClassificationLoss(alpha, gamma)\n",
    "        self._box_loss = CustomNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # print(\"y_true shape:\", tf.shape(y_true))\n",
    "        # print(\"y_pred shape:\", tf.shape(y_pred))\n",
    "\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        # print(\"box_labels shape:\", tf.shape(box_labels))\n",
    "        # print(\"box_predictions shape:\", tf.shape(box_predictions))\n",
    "        \n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        # print(\"cls_labels shape:\", tf.shape(cls_labels))\n",
    "        # print(\"cls_predictions shape:\", tf.shape(cls_predictions))\n",
    "\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        # print(\"positive_mask shape:\", tf.shape(positive_mask))\n",
    "        # print(\"ignore_mask shape:\", tf.shape(ignore_mask))\n",
    "\n",
    "        cls_loss = self._cls_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        # print(\"cls_loss shape:\", tf.shape(cls_loss))\n",
    "        # print(\"box_loss shape:\", tf.shape(box_loss))\n",
    "\n",
    "        cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, cls_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        # print(\"normalizer:\", normalizer)\n",
    "\n",
    "        cls_loss = tf.math.divide_no_nan(tf.reduce_sum(cls_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        \n",
    "        loss = cls_loss + box_loss\n",
    "        # print(\"Final loss shape:\", tf.shape(loss))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "# class CustomNetLoss(tf.losses.Loss):\n",
    "#     def __init__(self, num_classes=1):\n",
    "#         super(CustomNetLoss, self).__init__(reduction=\"auto\", name=\"CustomNetLoss\")\n",
    "#         self._num_classes = num_classes\n",
    "#         self.mse_loss = MeanSquaredError(reduction=\"none\")\n",
    "#         self.cce_loss = CategoricalCrossentropy(reduction=\"none\", from_logits=True)\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # 박스 레이블과 예측 분리\n",
    "#         box_labels = y_true[:, :, :4]\n",
    "#         box_predictions = y_pred[:, :, :4]\n",
    "\n",
    "#         # 클래스 레이블과 예측 분리 및 one-hot 인코딩\n",
    "#         cls_labels = tf.one_hot(tf.cast(y_true[:, :, 4], dtype=tf.int32), depth=self._num_classes)\n",
    "#         cls_predictions = y_pred[:, :, 4:]\n",
    "\n",
    "#         # 박스 손실(MSE) 계산\n",
    "#         box_loss = self.mse_loss(box_labels, box_predictions)\n",
    "\n",
    "#         # 클래스 손실(Categorical Cross-Entropy) 계산\n",
    "#         cls_loss = self.cce_loss(cls_labels, cls_predictions)\n",
    "\n",
    "#         # 손실 합산\n",
    "#         total_loss = tf.reduce_mean(cls_loss) + tf.reduce_mean(box_loss)\n",
    "#         return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "\n",
    "# learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "# learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "# learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     boundaries=learning_rate_boundaries, values=learning_rates\n",
    "# )\n",
    "\n",
    "LR = 0.0005\n",
    "initial_learning_rate = LR\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.90,\n",
    "    staircase=True)\n",
    "\n",
    "\n",
    "model_dir = \"ObjectDetectionCheckpoint/customnet.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "# 멀티 GPU 전략 설정\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # 모델, 손실 함수, 옵티마이저를 전략 범위 내에서 정의\n",
    "    model = CustomNet(num_classes)\n",
    "    loss_fn = CustomNetLoss()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# 이후 모델 훈련 및 평가 코드는 변경 없이 사용 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (5, 96, 128, 1)\n",
      "Images max: tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Images min: tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Labels shape: (5, 24192, 5)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에서 하나의 배치를 가져옵니다\n",
    "for batch in train_dataset.take(1):\n",
    "    # 배치에서 이미지와 레이블을 추출합니다\n",
    "    images, labels = batch\n",
    "    \n",
    "    # 이미지와 레이블의 형태를 출력합니다\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Images max:\", tf.reduce_max(images))\n",
    "    print(\"Images min:\", tf.reduce_min(images))\n",
    "    print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # TensorFlow Dataset 예시\n",
    "# # train_dataset = tf.data.Dataset.from_tensor_slices(...)\n",
    "\n",
    "# def check_nan_in_dataset(dataset):\n",
    "#     for batch in dataset:\n",
    "#         images, labels = batch\n",
    "#         if tf.reduce_any(tf.math.is_nan(images)) or tf.reduce_any(tf.math.is_nan(labels)):\n",
    "#             return True\n",
    "#     # print(images)\n",
    "#     # print(labels)\n",
    "#     return False\n",
    "\n",
    "\n",
    "# contains_nan = check_nan_in_dataset(train_dataset)\n",
    "# if contains_nan:\n",
    "#     print(\"Data contains NaN values\")\n",
    "# else:\n",
    "#     print(\"Data does not contain NaN values\")\n",
    "\n",
    "# check_nan_in_dataset(train_dataset)\n",
    "# check_nan_in_dataset(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 12:23:35.980073: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"UnbatchDataset/_30\"\n",
      "op: \"UnbatchDataset\"\n",
      "input: \"MapDataset/_29\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021UnbatchDataset:29\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 128\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 24648\n",
      "        }\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:Collective all_reduce tensors: 82 all_reduces, num_devices = 7, group_size = 7, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 82 all_reduces, num_devices = 7, group_size = 7, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 12:24:12.703736: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inCustomNet/FeaturePyramid/model_2/dropout_6/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-01-22 12:24:22.264168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 12:24:23.330509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 12:24:24.485545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 12:24:25.851916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 12:24:26.161027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-01-22 12:24:26.555168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 12:24:31.050084: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd5c0952010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-22 12:24:31.050138: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-22 12:24:31.050152: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-22 12:24:31.050162: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-22 12:24:31.050171: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-22 12:24:31.050181: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-22 12:24:31.050190: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-22 12:24:31.050200: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-01-22 12:24:31.062289: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-22 12:24:31.203419: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-01-22 12:24:32.653572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-01-22 12:24:33.060272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2576/Unknown - 203s 55ms/step - loss: 1.4396 - accuracy: 0.2069 - precision: 0.1988 - recall: 0.0036"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 12:27:00.399739: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2747469563446689080\n",
      "2024-01-22 12:27:00.399836: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2945880018600264241\n",
      "2024-01-22 12:27:00.399903: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17351640747388281418\n",
      "2024-01-22 12:27:00.399976: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8513498933865136315\n",
      "2024-01-22 12:27:00.400031: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 998488130902900940\n",
      "2024-01-22 12:27:00.400052: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12387672491566482898\n",
      "2024-01-22 12:27:00.400074: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 752173846908732003\n",
      "2024-01-22 12:27:00.400097: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5015782914463876036\n",
      "2024-01-22 12:27:00.400109: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8425866675446851155\n",
      "2024-01-22 12:27:00.400124: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6165079125031749116\n",
      "2024-01-22 12:27:00.400141: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3002409190499444258\n",
      "2024-01-22 12:27:00.400163: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16308448459367696355\n",
      "2024-01-22 12:27:00.400182: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9511976648397424604\n",
      "2024-01-22 12:27:00.400193: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4265682943338055307\n",
      "2024-01-22 12:27:00.400204: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7152047163522533276\n",
      "2024-01-22 12:27:00.400212: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2420930198321621339\n",
      "2024-01-22 12:27:00.400221: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4408811604248717130\n",
      "2024-01-22 12:27:00.400234: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7318199505813793411\n",
      "2024-01-22 12:27:00.400247: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15949863911479306666\n",
      "2024-01-22 12:27:00.400263: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9433873915275165790\n",
      "2024-01-22 12:27:00.400283: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2402026848927072973\n",
      "2024-01-22 12:27:00.400308: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15393347686341397274\n",
      "2024-01-22 12:27:00.400331: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12638263918305231446\n",
      "2024-01-22 12:27:00.400351: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17550195739801372005\n",
      "2024-01-22 12:27:00.400370: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 261417856855408863\n",
      "2024-01-22 12:27:00.400397: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10433156502563275590\n",
      "2024-01-22 12:27:00.400421: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14186043838187554397\n",
      "2024-01-22 12:27:00.400439: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11894805821238018407\n",
      "2024-01-22 12:27:00.400460: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17671432151930422382\n",
      "2024-01-22 12:27:00.400479: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12985089514803595157\n",
      "2024-01-22 12:27:00.400498: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9882899040525688583\n",
      "2024-01-22 12:27:00.400519: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10438724246726542758\n",
      "2024-01-22 12:27:00.400535: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13454675909676124189\n",
      "2024-01-22 12:27:00.400553: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6325023036096292535\n",
      "2024-01-22 12:27:00.400574: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8203186747889040510\n",
      "2024-01-22 12:27:00.400591: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1509855991212542013\n",
      "2024-01-22 12:27:00.400610: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4439192148489619551\n",
      "2024-01-22 12:27:00.400631: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16273372625549427294\n",
      "2024-01-22 12:27:00.400650: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3366257656744888829\n",
      "2024-01-22 12:27:00.400672: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10241703435648945151\n",
      "2024-01-22 12:27:00.400698: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6685538221984123382\n",
      "2024-01-22 12:27:00.400715: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11809756701551758621\n",
      "2024-01-22 12:27:00.400734: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8930900270556327327\n",
      "2024-01-22 12:27:00.400753: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6275226062814694374\n",
      "2024-01-22 12:27:00.400769: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10576732404302531901\n",
      "2024-01-22 12:27:00.400788: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3839423295724720119\n",
      "2024-01-22 12:27:00.400809: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12831862091015031902\n",
      "2024-01-22 12:27:00.400827: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4580712868895313813\n",
      "2024-01-22 12:27:00.400845: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11298229046119328895\n",
      "2024-01-22 12:27:00.400869: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9641419407420689638\n",
      "2024-01-22 12:27:00.400889: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 956219424275353781\n",
      "2024-01-22 12:27:00.400904: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6744650205307833278\n",
      "2024-01-22 12:27:00.400927: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16550580014338716111\n",
      "2024-01-22 12:27:00.400949: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10836009013979878005\n",
      "2024-01-22 12:27:00.400969: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11460823757155290190\n",
      "2024-01-22 12:27:00.400991: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3425037424941257391\n",
      "2024-01-22 12:27:00.401017: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17828318237919531957\n",
      "2024-01-22 12:27:00.401034: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18354677266688365190\n",
      "2024-01-22 12:27:00.401045: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11406208700547453997\n",
      "2024-01-22 12:27:00.401054: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10872445481104742254\n",
      "2024-01-22 12:27:00.401064: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11117610838610531853\n",
      "2024-01-22 12:27:00.401072: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12190670740122776495\n",
      "2024-01-22 12:27:00.401095: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4135122501816786310\n",
      "2024-01-22 12:27:00.401115: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5379142949501525261\n",
      "2024-01-22 12:27:00.401124: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13754368787588931814\n",
      "2024-01-22 12:27:00.401133: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2804720993582503319\n",
      "2024-01-22 12:27:00.401153: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9853417709381885381\n",
      "2024-01-22 12:27:00.401173: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 454518287353012526\n",
      "2024-01-22 12:27:00.401191: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2353326728338134831\n",
      "2024-01-22 12:27:00.401212: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12574500181113036749\n",
      "2024-01-22 12:27:00.401230: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4248660544869033694\n",
      "2024-01-22 12:27:00.401249: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2285868423029184751\n",
      "2024-01-22 12:27:00.401274: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3836561097209716933\n",
      "2024-01-22 12:27:00.401291: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11388547253948525022\n",
      "2024-01-22 12:27:00.401309: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 244786893287127335\n",
      "2024-01-22 12:27:00.401329: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12574779656576526533\n",
      "2024-01-22 12:27:00.401350: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2514377769611946590\n",
      "2024-01-22 12:27:00.401368: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5810049962715699439\n",
      "2024-01-22 12:27:00.401388: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14682303884093893085\n",
      "2024-01-22 12:27:00.401404: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14689472507619470990\n",
      "2024-01-22 12:27:00.401424: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7712222311211146271\n",
      "2024-01-22 12:27:00.401445: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12754646162816986653\n",
      "2024-01-22 12:27:00.401462: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8607724983060918206\n",
      "2024-01-22 12:27:00.401482: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2606087201324939423\n",
      "2024-01-22 12:27:00.401507: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10151903129987641253\n",
      "2024-01-22 12:27:00.401524: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7400015368884138014\n",
      "2024-01-22 12:27:00.401543: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13784080246158636463\n",
      "2024-01-22 12:27:00.401564: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5585591879625568677\n",
      "2024-01-22 12:27:00.401584: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9794112444102422934\n",
      "2024-01-22 12:27:00.401599: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2965084778689934157\n",
      "2024-01-22 12:27:00.401614: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10309635341680882894\n",
      "2024-01-22 12:27:00.401636: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2016409680266410390\n",
      "2024-01-22 12:27:00.401655: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4412368923525360802\n",
      "2024-01-22 12:27:00.401679: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6712473833910483045\n",
      "2024-01-22 12:27:00.401697: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15511026773928966750\n",
      "2024-01-22 12:27:00.401715: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6234959546183757442\n",
      "2024-01-22 12:27:00.401737: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14501210071878089261\n",
      "2024-01-22 12:27:00.401757: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10538515382139829462\n",
      "2024-01-22 12:27:00.401775: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8671562350139878970\n",
      "2024-01-22 12:27:00.401797: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 206354450606505365\n",
      "2024-01-22 12:27:00.401814: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10974890012747152558\n",
      "2024-01-22 12:27:00.401832: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7033201640315392514\n",
      "2024-01-22 12:27:00.401853: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11249009538936602157\n",
      "2024-01-22 12:27:00.401872: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3793986910048140294\n",
      "2024-01-22 12:27:00.401891: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15589238535285331050\n",
      "2024-01-22 12:27:00.401911: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11645268365824736877\n",
      "2024-01-22 12:27:00.401929: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13620427043466537939\n",
      "2024-01-22 12:27:00.401947: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13665251687798592858\n",
      "2024-01-22 12:27:00.401971: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2980138421109517443\n",
      "2024-01-22 12:27:00.401990: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15876656221517763810\n",
      "2024-01-22 12:27:00.402009: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8210829021920988555\n",
      "2024-01-22 12:27:00.402031: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13836381740068457316\n",
      "2024-01-22 12:27:00.402050: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5642679344438065242\n",
      "2024-01-22 12:27:00.402070: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8172890303138189379\n",
      "2024-01-22 12:27:00.402088: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4184064843858251092\n",
      "2024-01-22 12:27:00.402107: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12353934617499031970\n",
      "2024-01-22 12:27:00.402132: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17887200616283288267\n",
      "2024-01-22 12:27:00.402149: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3131909247730634644\n",
      "2024-01-22 12:27:00.402170: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 157685068095036050\n",
      "2024-01-22 12:27:00.402191: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10459501703630631515\n",
      "2024-01-22 12:27:00.402212: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6377171537773072572\n",
      "2024-01-22 12:27:00.402232: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12862087753235020650\n",
      "2024-01-22 12:27:00.402252: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17731833671795292171\n",
      "2024-01-22 12:27:00.402270: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3410766311955779804\n",
      "2024-01-22 12:27:00.402290: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5556041387377049322\n",
      "2024-01-22 12:27:00.402315: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15362934902854572731\n",
      "2024-01-22 12:27:00.402332: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2315911659197222788\n",
      "2024-01-22 12:27:00.402350: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12253800137946709794\n",
      "2024-01-22 12:27:00.402371: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2872240957156104795\n",
      "2024-01-22 12:27:00.402391: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9698217525830096268\n",
      "2024-01-22 12:27:00.402408: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15038509323686059363\n",
      "2024-01-22 12:27:00.402424: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15708101895136368252\n",
      "2024-01-22 12:27:00.402443: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16764640731654542924\n",
      "2024-01-22 12:27:00.402461: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15982050550752581076\n",
      "2024-01-22 12:27:00.402476: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13932393177433512260\n",
      "2024-01-22 12:27:00.402492: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14789654524408898084\n",
      "2024-01-22 12:27:00.402508: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8700465249627220212\n",
      "2024-01-22 12:27:00.402529: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8798880724741204228\n",
      "2024-01-22 12:27:00.402545: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9427985806349382604\n",
      "2024-01-22 12:27:00.402561: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10245416016012720860\n",
      "2024-01-22 12:27:00.402686: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12350940624491382153\n",
      "2024-01-22 12:27:00.402702: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7279671645568583729\n",
      "2024-01-22 12:27:00.402718: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 764438957298510153\n",
      "2024-01-22 12:27:00.402735: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15861515400795761825\n",
      "2024-01-22 12:27:00.402751: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1938258650211585105\n",
      "2024-01-22 12:27:00.402772: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5161474328974683665\n",
      "2024-01-22 12:27:00.402867: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9159992477741797289\n",
      "2024-01-22 12:27:00.402888: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13305438134101832449\n",
      "2024-01-22 12:27:00.402905: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3407230727968168969\n",
      "2024-01-22 12:27:00.402920: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8107401656242167401\n",
      "2024-01-22 12:27:00.402936: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10379859635143090568\n",
      "2024-01-22 12:27:00.402951: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 640937009249223488\n",
      "2024-01-22 12:27:00.402965: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5771890173211214920\n",
      "2024-01-22 12:27:00.402984: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2582441586082415744\n",
      "2024-01-22 12:27:00.403004: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6893223978217400584\n",
      "2024-01-22 12:27:00.403019: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7302699155485371600\n",
      "2024-01-22 12:27:00.403139: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5903344223169842576\n",
      "2024-01-22 12:27:00.403157: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12036252658842750592\n",
      "2024-01-22 12:27:00.403171: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17543699290868166608\n",
      "2024-01-22 12:27:00.403189: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15851541983189283440\n",
      "2024-01-22 12:27:00.403290: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7687642036255060600\n",
      "2024-01-22 12:27:00.403419: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3595834761066140784\n",
      "2024-01-22 12:27:00.403440: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2755758385369061440\n",
      "2024-01-22 12:27:00.403460: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11406123627111970872\n",
      "2024-01-22 12:27:00.403475: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10234597314081372864\n",
      "2024-01-22 12:27:00.403497: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 798836968734768784\n",
      "2024-01-22 12:27:00.403512: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14665402889272475336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: loss improved from inf to 1.43956, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_1\n",
      "2576/2576 [==============================] - 204s 55ms/step - loss: 1.4396 - accuracy: 0.2069 - precision: 0.1988 - recall: 0.0036\n",
      "Epoch 2/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 1.1650 - accuracy: 0.4466 - precision: 0.2008 - recall: 0.0051\n",
      "Epoch 2: loss improved from 1.43956 to 1.16505, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_2\n",
      "2576/2576 [==============================] - 140s 54ms/step - loss: 1.1650 - accuracy: 0.4466 - precision: 0.2008 - recall: 0.0051\n",
      "Epoch 3/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 1.2276 - accuracy: 0.3317 - precision: 0.1989 - recall: 0.0059\n",
      "Epoch 3: loss did not improve from 1.16505\n",
      "2576/2576 [==============================] - 140s 54ms/step - loss: 1.2276 - accuracy: 0.3317 - precision: 0.1989 - recall: 0.0059\n",
      "Epoch 4/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 1.0283 - accuracy: 0.3625 - precision: 0.1991 - recall: 0.0058\n",
      "Epoch 4: loss improved from 1.16505 to 1.02825, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_4\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 1.0283 - accuracy: 0.3625 - precision: 0.1991 - recall: 0.0058\n",
      "Epoch 5/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.3907 - precision: 0.1996 - recall: 0.0057\n",
      "Epoch 5: loss improved from 1.02825 to 0.95667, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_5\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.9567 - accuracy: 0.3907 - precision: 0.1996 - recall: 0.0057\n",
      "Epoch 6/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.9152 - accuracy: 0.3282 - precision: 0.1995 - recall: 0.0056\n",
      "Epoch 6: loss improved from 0.95667 to 0.91522, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_6\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.9152 - accuracy: 0.3282 - precision: 0.1995 - recall: 0.0056\n",
      "Epoch 7/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8814 - accuracy: 0.3045 - precision: 0.1996 - recall: 0.0057\n",
      "Epoch 7: loss improved from 0.91522 to 0.88145, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_7\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8814 - accuracy: 0.3045 - precision: 0.1996 - recall: 0.0057\n",
      "Epoch 8/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8736 - accuracy: 0.2920 - precision: 0.1992 - recall: 0.0056\n",
      "Epoch 8: loss improved from 0.88145 to 0.87364, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_8\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8736 - accuracy: 0.2920 - precision: 0.1992 - recall: 0.0056\n",
      "Epoch 9/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8672 - accuracy: 0.2892 - precision: 0.1995 - recall: 0.0058\n",
      "Epoch 9: loss improved from 0.87364 to 0.86717, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_9\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8672 - accuracy: 0.2892 - precision: 0.1995 - recall: 0.0058\n",
      "Epoch 10/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8617 - accuracy: 0.3104 - precision: 0.1997 - recall: 0.0057\n",
      "Epoch 10: loss improved from 0.86717 to 0.86168, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_10\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8617 - accuracy: 0.3104 - precision: 0.1997 - recall: 0.0057\n",
      "Epoch 11/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8580 - accuracy: 0.3134 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 11: loss improved from 0.86168 to 0.85801, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_11\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8580 - accuracy: 0.3134 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 12/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8519 - accuracy: 0.2948 - precision: 0.1998 - recall: 0.0056\n",
      "Epoch 12: loss improved from 0.85801 to 0.85193, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_12\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8519 - accuracy: 0.2948 - precision: 0.1998 - recall: 0.0056\n",
      "Epoch 13/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8475 - accuracy: 0.3280 - precision: 0.1997 - recall: 0.0056\n",
      "Epoch 13: loss improved from 0.85193 to 0.84746, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_13\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8475 - accuracy: 0.3280 - precision: 0.1997 - recall: 0.0056\n",
      "Epoch 14/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8455 - accuracy: 0.3093 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 14: loss improved from 0.84746 to 0.84552, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_14\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8455 - accuracy: 0.3093 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 15/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8409 - accuracy: 0.4198 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 15: loss improved from 0.84552 to 0.84088, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_15\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8409 - accuracy: 0.4198 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 16/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8367 - accuracy: 0.4154 - precision: 0.1995 - recall: 0.0056\n",
      "Epoch 16: loss improved from 0.84088 to 0.83673, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_16\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8367 - accuracy: 0.4154 - precision: 0.1995 - recall: 0.0056\n",
      "Epoch 17/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8306 - accuracy: 0.4250 - precision: 0.1995 - recall: 0.0054\n",
      "Epoch 17: loss improved from 0.83673 to 0.83063, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_17\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8306 - accuracy: 0.4250 - precision: 0.1995 - recall: 0.0054\n",
      "Epoch 18/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8263 - accuracy: 0.4980 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 18: loss improved from 0.83063 to 0.82633, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_18\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8263 - accuracy: 0.4980 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 19/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8234 - accuracy: 0.4758 - precision: 0.1997 - recall: 0.0055\n",
      "Epoch 19: loss improved from 0.82633 to 0.82339, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_19\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8234 - accuracy: 0.4758 - precision: 0.1997 - recall: 0.0055\n",
      "Epoch 20/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8194 - accuracy: 0.4841 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 20: loss improved from 0.82339 to 0.81936, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_20\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8194 - accuracy: 0.4841 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 21/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8146 - accuracy: 0.4290 - precision: 0.1995 - recall: 0.0055\n",
      "Epoch 21: loss improved from 0.81936 to 0.81458, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_21\n",
      "2576/2576 [==============================] - 142s 55ms/step - loss: 0.8146 - accuracy: 0.4290 - precision: 0.1995 - recall: 0.0055\n",
      "Epoch 22/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8108 - accuracy: 0.4756 - precision: 0.2000 - recall: 0.0054\n",
      "Epoch 22: loss improved from 0.81458 to 0.81077, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_22\n",
      "2576/2576 [==============================] - 140s 54ms/step - loss: 0.8108 - accuracy: 0.4756 - precision: 0.2000 - recall: 0.0054\n",
      "Epoch 23/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8067 - accuracy: 0.4327 - precision: 0.2001 - recall: 0.0056\n",
      "Epoch 23: loss improved from 0.81077 to 0.80666, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_23\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8067 - accuracy: 0.4327 - precision: 0.2001 - recall: 0.0056\n",
      "Epoch 24/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.8025 - accuracy: 0.4157 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 24: loss improved from 0.80666 to 0.80254, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_24\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.8025 - accuracy: 0.4157 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 25/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.3441 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 25: loss improved from 0.80254 to 0.79944, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_25\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.7994 - accuracy: 0.3441 - precision: 0.1996 - recall: 0.0055\n",
      "Epoch 26/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.7963 - accuracy: 0.4222 - precision: 0.2001 - recall: 0.0055\n",
      "Epoch 26: loss improved from 0.79944 to 0.79626, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_26\n",
      "2576/2576 [==============================] - 141s 55ms/step - loss: 0.7963 - accuracy: 0.4222 - precision: 0.2001 - recall: 0.0055\n",
      "Epoch 27/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.7929 - accuracy: 0.4306 - precision: 0.1997 - recall: 0.0055\n",
      "Epoch 27: loss improved from 0.79626 to 0.79294, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_27\n",
      "2576/2576 [==============================] - 140s 54ms/step - loss: 0.7929 - accuracy: 0.4306 - precision: 0.1997 - recall: 0.0055\n",
      "Epoch 28/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.7885 - accuracy: 0.3516 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 28: loss improved from 0.79294 to 0.78849, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_28\n",
      "2576/2576 [==============================] - 140s 54ms/step - loss: 0.7885 - accuracy: 0.3516 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 29/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.7879 - accuracy: 0.4087 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 29: loss improved from 0.78849 to 0.78790, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_29\n",
      "2576/2576 [==============================] - 140s 54ms/step - loss: 0.7879 - accuracy: 0.4087 - precision: 0.1999 - recall: 0.0055\n",
      "Epoch 30/30\n",
      "2576/2576 [==============================] - ETA: 0s - loss: 0.7840 - accuracy: 0.2906 - precision: 0.2003 - recall: 0.0055\n",
      "Epoch 30: loss improved from 0.78790 to 0.78403, saving model to ObjectDetectionCheckpoint/customnet.ckpt/weights_epoch_30\n",
      "2576/2576 [==============================] - 140s 54ms/step - loss: 0.7840 - accuracy: 0.2906 - precision: 0.2003 - recall: 0.0055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda333d4ee0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CustomNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " FeaturePyramid (FeaturePyr  multiple                  244384    \n",
      " amid)                                                           \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, None, None, 6)     38726     \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, None, None, 24)    43928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 327038 (1.25 MB)\n",
      "Trainable params: 325886 (1.24 MB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
