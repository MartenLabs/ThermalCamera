{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.data_decoders import tf_example_decoder\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Reshape, Concatenate\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data paths\n",
    "train_tfrecord_dir = 'ThermalCamera_MLX90640.v3i.tfrecord/train/'\n",
    "valid_tfrecord_dir = 'ThermalCamera_MLX90640.v3i.tfrecord/valid/'\n",
    "train_label_map_path = os.path.join(train_tfrecord_dir, 'person_label_map.pbtxt')\n",
    "valid_label_map_path = os.path.join(valid_tfrecord_dir, 'person_label_map.pbtxt')\n",
    "\n",
    "# Load the label map\n",
    "label_map = label_map_util.load_labelmap(train_label_map_path)\n",
    "label_map_dict = label_map_util.get_label_map_dict(label_map)\n",
    "\n",
    "# Get the list of TFRecord files\n",
    "train_tfrecord_files = tf.io.gfile.glob(os.path.join(train_tfrecord_dir, '*.tfrecord'))\n",
    "valid_tfrecord_files = tf.io.gfile.glob(os.path.join(valid_tfrecord_dir, '*.tfrecord'))\n",
    "\n",
    "def load_dataset(tfrecord_files, batch_size=32, shuffle_buffer_size=1000):\n",
    "    \"\"\"Loads a TFRecord dataset.\"\"\"\n",
    "    feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'depth': tf.io.FixedLenFeature([], tf.int64, default_value=1),\n",
    "    'bbox_xmin': tf.io.VarLenFeature(tf.float32),\n",
    "    'bbox_ymin': tf.io.VarLenFeature(tf.float32),\n",
    "    'bbox_xmax': tf.io.VarLenFeature(tf.float32),\n",
    "    'bbox_ymax': tf.io.VarLenFeature(tf.float32),\n",
    "    'class_labels': tf.io.VarLenFeature(tf.int64),\n",
    "    # Add other features as needed for your dataset\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "    def _parse_tfrecord_fn(example_proto):\n",
    "        # Parse the TFRecord example\n",
    "        features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        image = tf.image.decode_image(features['image'], channels=3)  # Adjust channels if needed\n",
    "    \n",
    "        # Process and preprocess your data here...\n",
    "        bbox_xmin = tf.sparse.to_dense(features['bbox_xmin'])\n",
    "        bbox_ymin = tf.sparse.to_dense(features['bbox_ymin'])\n",
    "        bbox_xmax = tf.sparse.to_dense(features['bbox_xmax'])\n",
    "        bbox_ymax = tf.sparse.to_dense(features['bbox_ymax'])\n",
    "    \n",
    "        # Stack bounding box coordinates into one tensor\n",
    "        bbox = tf.stack([bbox_ymin, bbox_xmin, \n",
    "                         bbox_ymax, bbox_xmax], axis=-1)\n",
    "\n",
    "        # Convert class labels to one-hot encoding\n",
    "        num_classes = len(label_map_dict) + 1  # Add one for background class\n",
    "        class_labels_one_hot = tf.one_hot(tf.sparse.to_dense(features['class_labels']), depth=num_classes)\n",
    "     \n",
    "        # Reshape as needed...\n",
    "        bbox = tf.reshape(bbox, (-1, 4))\n",
    "        class_labels_one_hot = tf.reshape(class_labels_one_hot , (-1,num_classes))\n",
    "\n",
    "        ground_truth_annotations={\n",
    "             'predictions':tf.concat([bbox,class_labels_one_hot],axis=-1)\n",
    "        }\n",
    "\n",
    "        return image ,ground_truth_annotations\n",
    "    return tf.data.TFRecordDataset(tfrecord_files).map(_parse_tfrecord_fn).shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = load_dataset(train_tfrecord_files)\n",
    "valid_dataset = load_dataset(valid_tfrecord_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'image/encoded': bytes_list {\n",
      "  value: \"\\377\\330\\377\\340\\000\\020JFIF\\000\\001\\001\\000\\000\\001\\000\\001\\000\\000\\377\\333\\000C\\000\\010\\006\\006\\007\\006\\005\\010\\007\\007\\007\\t\\t\\010\\n\\014\\024\\r\\014\\013\\013\\014\\031\\022\\023\\017\\024\\035\\032\\037\\036\\035\\032\\034\\034 $.\\' \\\",#\\034\\034(7),01444\\037\\'9=82<.342\\377\\333\\000C\\001\\t\\t\\t\\014\\013\\014\\030\\r\\r\\0302!\\034!22222222222222222222222222222222222222222222222222\\377\\300\\000\\021\\010\\000\\010\\000\\004\\003\\001\\\"\\000\\002\\021\\001\\003\\021\\001\\377\\304\\000\\037\\000\\000\\001\\005\\001\\001\\001\\001\\001\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\002\\003\\004\\005\\006\\007\\010\\t\\n\\013\\377\\304\\000\\265\\020\\000\\002\\001\\003\\003\\002\\004\\003\\005\\005\\004\\004\\000\\000\\001}\\001\\002\\003\\000\\004\\021\\005\\022!1A\\006\\023Qa\\007\\\"q\\0242\\201\\221\\241\\010#B\\261\\301\\025R\\321\\360$3br\\202\\t\\n\\026\\027\\030\\031\\032%&\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\203\\204\\205\\206\\207\\210\\211\\212\\222\\223\\224\\225\\226\\227\\230\\231\\232\\242\\243\\244\\245\\246\\247\\250\\251\\252\\262\\263\\264\\265\\266\\267\\270\\271\\272\\302\\303\\304\\305\\306\\307\\310\\311\\312\\322\\323\\324\\325\\326\\327\\330\\331\\332\\341\\342\\343\\344\\345\\346\\347\\350\\351\\352\\361\\362\\363\\364\\365\\366\\367\\370\\371\\372\\377\\304\\000\\037\\001\\000\\003\\001\\001\\001\\001\\001\\001\\001\\001\\001\\000\\000\\000\\000\\000\\000\\001\\002\\003\\004\\005\\006\\007\\010\\t\\n\\013\\377\\304\\000\\265\\021\\000\\002\\001\\002\\004\\004\\003\\004\\007\\005\\004\\004\\000\\001\\002w\\000\\001\\002\\003\\021\\004\\005!1\\006\\022AQ\\007aq\\023\\\"2\\201\\010\\024B\\221\\241\\261\\301\\t#3R\\360\\025br\\321\\n\\026$4\\341%\\361\\027\\030\\031\\032&\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\202\\203\\204\\205\\206\\207\\210\\211\\212\\222\\223\\224\\225\\226\\227\\230\\231\\232\\242\\243\\244\\245\\246\\247\\250\\251\\252\\262\\263\\264\\265\\266\\267\\270\\271\\272\\302\\303\\304\\305\\306\\307\\310\\311\\312\\322\\323\\324\\325\\326\\327\\330\\331\\332\\342\\343\\344\\345\\346\\347\\350\\351\\352\\362\\363\\364\\365\\366\\367\\370\\371\\372\\377\\332\\000\\014\\003\\001\\000\\002\\021\\003\\021\\000?\\000\\347\\364\\357\\212\\326P\\3053\\\\\\332]\\te\\235\\345a\\020VPX\\347\\000\\2223\\371QE\\024\\204\\177\\377\\331\"\n",
      "}\n",
      ", 'image/filename': bytes_list {\n",
      "  value: \"2023-09-14-03-49-26-mp4_121_png.rf.00054ae4f92c2b01506e2f66d383d0f8.jpg\"\n",
      "}\n",
      ", 'image/format': bytes_list {\n",
      "  value: \"jpeg\"\n",
      "}\n",
      ", 'image/height': int64_list {\n",
      "  value: 8\n",
      "}\n",
      ", 'image/object/bbox/xmax': float_list {\n",
      "  value: 0.25\n",
      "}\n",
      ", 'image/object/bbox/xmin': float_list {\n",
      "  value: 0.0\n",
      "}\n",
      ", 'image/object/bbox/ymax': float_list {\n",
      "  value: 0.625\n",
      "}\n",
      ", 'image/object/bbox/ymin': float_list {\n",
      "  value: 0.0\n",
      "}\n",
      ", 'image/object/class/label': int64_list {\n",
      "  value: 1\n",
      "}\n",
      ", 'image/object/class/text': bytes_list {\n",
      "  value: \"person\"\n",
      "}\n",
      ", 'image/width': int64_list {\n",
      "  value: 4\n",
      "}\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the first record file.\n",
    "record_file = train_tfrecord_files[0]\n",
    "\n",
    "# Create a dataset from the record file.\n",
    "dataset = tf.data.TFRecordDataset(record_file)\n",
    "\n",
    "# Decode the first example.\n",
    "for raw_record in dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    \n",
    "    # Print the keys in this example.\n",
    "    print(example.features.feature.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 22:02:07.717604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-09-14 22:02:08.186184: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186209: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186222: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186231: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186243: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186253: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186281: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186294: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186302: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186308: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186325: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n",
      "2023-09-14 22:02:08.186347: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4573538418100377000\n",
      "2023-09-14 22:02:08.186355: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10435876619965792128\n",
      "2023-09-14 22:02:08.186361: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4265757422512485983\n",
      "2023-09-14 22:02:08.186395: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Feature: image (data type: string) is required but could not be found.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Feature: image (data type: string) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n\t [[IteratorGetNext]]\n\t [[model_17/Cast/_8]]\n  (1) INVALID_ARGUMENT:  Feature: image (data type: string) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_105859]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb 셀 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39mloss_fn, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m], loss_weights\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1.0\u001b[39m})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m  \u001b[39m# Adjust as needed\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     train_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalid_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback, early_stopping_callback]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Feature: image (data type: string) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n\t [[IteratorGetNext]]\n\t [[model_17/Cast/_8]]\n  (1) INVALID_ARGUMENT:  Feature: image (data type: string) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_105859]"
     ]
    }
   ],
   "source": [
    "# Define the SSD MobileNetV2 model\n",
    "base_model = MobileNetV2(input_shape=(None, None, 3), include_top=False)\n",
    "# Add additional layers for object detection\n",
    "x = base_model.output\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='additional_conv1')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='additional_conv2')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='additional_conv3')(x)\n",
    "\n",
    "# Define the prediction head\n",
    "num_boxes = 1\n",
    "num_classes = len(label_map_dict) + 1  # Add one for the background class\n",
    "bbox_predictions = Conv2D(4 * num_boxes, (3, 3), padding='same', name='bbox_predictions')(x)\n",
    "bbox_predictions = Reshape((-1, 4))(bbox_predictions)\n",
    "class_predictions = Conv2D(num_classes * num_boxes, (3, 3), padding='same', name='class_predictions')(x)\n",
    "class_predictions = Reshape((-1, num_classes))(class_predictions)\n",
    "predictions = Concatenate(axis=-1, name='predictions')([bbox_predictions, class_predictions])\n",
    "\n",
    "\n",
    "# Create the SSD model|\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Define the loss function and compile the model\n",
    "loss_fn = Huber(delta=1.0)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\"ssd_mobilenetv2_checkpoint.h5\", save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'], loss_weights={'predictions': 1.0})\n",
    "\n",
    "\n",
    "\n",
    "epochs = 50  # Adjust as needed\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ThermalCamera_MLX90640.v3i.tfrecord/train/person.tfrecord']\n",
      "['ThermalCamera_MLX90640.v3i.tfrecord/valid/person.tfrecord']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb 셀 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(train_tfrecord_files)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(valid_tfrecord_files)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_dataset\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(images\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(labels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'take'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb 셀 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m  \u001b[39m# Adjust as needed\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalid_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback, early_stopping_callback]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mac/Dev/Project/ThermalCamera/Model/ssdMobileNetV2/CustomNet.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/src/engine/data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1102\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1104\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[1;32m   1108\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m     )\n\u001b[1;32m   1110\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1113\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 50  # Adjust as needed\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
